{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Przeprowadzone testy</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from p2_model_training import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_Data():\n",
    "    # Create dataset\n",
    "    X, y, X_test, y_test = create_data_mnist(\"../fashion_mnist_images\")\n",
    "    keys = np.array(range(X.shape[0]))\n",
    "    np.random.shuffle(keys)\n",
    "    X = X[keys]\n",
    "    y = y[keys]\n",
    "    # Scale and reshape samples\n",
    "    X = (X.reshape(X.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
    "    X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
    "    return X, y, X_test, y_test\n",
    "\n",
    "def Train_Model(X, y, X_test, y_test, model_number=None, batch_size=128, epochs=10, number_of_neurons=128, learning_rate=0.001, decay=1e-4, dropout_rate=0.9, number_of_layers=1):\n",
    "    model = Model()\n",
    "    # Add layers\n",
    "    model.add(Layer_Dense(X.shape[1], number_of_neurons))\n",
    "\n",
    "    for i in range(number_of_layers):\n",
    "        model.add(Layer_Dense(number_of_neurons, number_of_neurons))\n",
    "        model.add(Activation_ReLU())\n",
    "        \n",
    "    model.add(Layer_Dropout(dropout_rate))\n",
    "    model.add(Layer_Dense(number_of_neurons, 10))\n",
    "    model.add(Activation_Softmax())\n",
    "\n",
    "    model.set(\n",
    "        loss=Loss_CategoricalCrossentropy(),\n",
    "        optimizer=Optimizer_Adam(decay=decay, learning_rate=learning_rate),\n",
    "        accuracy=Accuracy_Categorical(),\n",
    "    )\n",
    "\n",
    "    model.finalize()\n",
    "\n",
    "    model.train(\n",
    "        X, y, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size, print_every=100\n",
    "    )\n",
    "\n",
    "    model.save(\"fashion_mnist\"+ model_number +\".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_test, y_test = Read_Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0, acc: 0.078, loss: 2.312 (data_loss: 2.312, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.617, loss: 0.989 (data_loss: 0.989, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 200, acc: 0.742, loss: 0.751 (data_loss: 0.751, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 300, acc: 0.727, loss: 0.687 (data_loss: 0.687, reg_loss: 0.000), lr: 0.0009708737864077671\n",
      "step: 400, acc: 0.719, loss: 0.907 (data_loss: 0.907, reg_loss: 0.000), lr: 0.0009615384615384615\n",
      "step: 468, acc: 0.719, loss: 0.761 (data_loss: 0.761, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "training, acc: 0.659, loss: 0.931 (data_loss: 0.931, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "validation, acc: 0.791, loss: 0.571\n",
      "epoch: 2\n",
      "step: 0, acc: 0.750, loss: 0.611 (data_loss: 0.611, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.758, loss: 0.771 (data_loss: 0.771, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 200, acc: 0.797, loss: 0.602 (data_loss: 0.602, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 300, acc: 0.797, loss: 0.590 (data_loss: 0.590, reg_loss: 0.000), lr: 0.0009285913269570063\n",
      "step: 400, acc: 0.805, loss: 0.724 (data_loss: 0.724, reg_loss: 0.000), lr: 0.0009200478424878093\n",
      "step: 468, acc: 0.750, loss: 0.710 (data_loss: 0.710, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "training, acc: 0.735, loss: 0.741 (data_loss: 0.741, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "validation, acc: 0.811, loss: 0.527\n",
      "epoch: 3\n",
      "step: 0, acc: 0.750, loss: 0.626 (data_loss: 0.626, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.742, loss: 0.853 (data_loss: 0.853, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 200, acc: 0.789, loss: 0.587 (data_loss: 0.587, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 300, acc: 0.766, loss: 0.694 (data_loss: 0.694, reg_loss: 0.000), lr: 0.0008898380494749957\n",
      "step: 400, acc: 0.789, loss: 0.759 (data_loss: 0.759, reg_loss: 0.000), lr: 0.0008819897689186807\n",
      "step: 468, acc: 0.677, loss: 0.901 (data_loss: 0.901, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "training, acc: 0.752, loss: 0.707 (data_loss: 0.707, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "validation, acc: 0.817, loss: 0.513\n",
      "epoch: 4\n",
      "step: 0, acc: 0.797, loss: 0.577 (data_loss: 0.577, reg_loss: 0.000), lr: 0.0008766546857192952\n",
      "step: 100, acc: 0.758, loss: 0.780 (data_loss: 0.780, reg_loss: 0.000), lr: 0.0008690362388111585\n",
      "step: 200, acc: 0.789, loss: 0.612 (data_loss: 0.612, reg_loss: 0.000), lr: 0.0008615490652192642\n",
      "step: 300, acc: 0.789, loss: 0.600 (data_loss: 0.600, reg_loss: 0.000), lr: 0.0008541898009737763\n",
      "step: 400, acc: 0.781, loss: 0.680 (data_loss: 0.680, reg_loss: 0.000), lr: 0.0008469551960701278\n",
      "step: 468, acc: 0.792, loss: 0.616 (data_loss: 0.616, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "training, acc: 0.763, loss: 0.686 (data_loss: 0.686, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "validation, acc: 0.822, loss: 0.509\n",
      "epoch: 5\n",
      "step: 0, acc: 0.773, loss: 0.640 (data_loss: 0.640, reg_loss: 0.000), lr: 0.0008420343550016842\n",
      "step: 100, acc: 0.758, loss: 0.855 (data_loss: 0.855, reg_loss: 0.000), lr: 0.0008350033400133601\n",
      "step: 200, acc: 0.781, loss: 0.669 (data_loss: 0.669, reg_loss: 0.000), lr: 0.0008280887711162637\n",
      "step: 300, acc: 0.812, loss: 0.580 (data_loss: 0.580, reg_loss: 0.000), lr: 0.0008212877792378449\n",
      "step: 400, acc: 0.828, loss: 0.655 (data_loss: 0.655, reg_loss: 0.000), lr: 0.0008145975887911372\n",
      "step: 468, acc: 0.781, loss: 0.777 (data_loss: 0.777, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "training, acc: 0.766, loss: 0.672 (data_loss: 0.672, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "validation, acc: 0.825, loss: 0.496\n",
      "epoch: 6\n",
      "step: 0, acc: 0.828, loss: 0.572 (data_loss: 0.572, reg_loss: 0.000), lr: 0.0008100445524503849\n",
      "step: 100, acc: 0.758, loss: 0.697 (data_loss: 0.697, reg_loss: 0.000), lr: 0.0008035355564483729\n",
      "step: 200, acc: 0.820, loss: 0.551 (data_loss: 0.551, reg_loss: 0.000), lr: 0.0007971303308090873\n",
      "step: 300, acc: 0.773, loss: 0.537 (data_loss: 0.537, reg_loss: 0.000), lr: 0.0007908264136022144\n",
      "step: 400, acc: 0.828, loss: 0.768 (data_loss: 0.768, reg_loss: 0.000), lr: 0.0007846214201647706\n",
      "step: 468, acc: 0.760, loss: 0.676 (data_loss: 0.676, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "training, acc: 0.773, loss: 0.658 (data_loss: 0.658, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "validation, acc: 0.824, loss: 0.491\n",
      "epoch: 7\n",
      "step: 0, acc: 0.758, loss: 0.587 (data_loss: 0.587, reg_loss: 0.000), lr: 0.0007803964413922272\n",
      "step: 100, acc: 0.750, loss: 0.813 (data_loss: 0.813, reg_loss: 0.000), lr: 0.0007743534148985598\n",
      "step: 200, acc: 0.828, loss: 0.523 (data_loss: 0.523, reg_loss: 0.000), lr: 0.0007684032580298141\n",
      "step: 300, acc: 0.844, loss: 0.492 (data_loss: 0.492, reg_loss: 0.000), lr: 0.0007625438462711606\n",
      "step: 400, acc: 0.836, loss: 0.616 (data_loss: 0.616, reg_loss: 0.000), lr: 0.0007567731194187983\n",
      "step: 468, acc: 0.792, loss: 0.711 (data_loss: 0.711, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "training, acc: 0.779, loss: 0.647 (data_loss: 0.647, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "validation, acc: 0.829, loss: 0.489\n",
      "epoch: 8\n",
      "step: 0, acc: 0.805, loss: 0.629 (data_loss: 0.629, reg_loss: 0.000), lr: 0.0007528419784687194\n",
      "step: 100, acc: 0.797, loss: 0.712 (data_loss: 0.712, reg_loss: 0.000), lr: 0.0007472166180975864\n",
      "step: 200, acc: 0.797, loss: 0.600 (data_loss: 0.600, reg_loss: 0.000), lr: 0.0007416747014759327\n",
      "step: 300, acc: 0.797, loss: 0.492 (data_loss: 0.492, reg_loss: 0.000), lr: 0.0007362143856290952\n",
      "step: 400, acc: 0.797, loss: 0.636 (data_loss: 0.636, reg_loss: 0.000), lr: 0.0007308338814587444\n",
      "step: 468, acc: 0.771, loss: 0.746 (data_loss: 0.746, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "training, acc: 0.779, loss: 0.638 (data_loss: 0.638, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "validation, acc: 0.829, loss: 0.486\n",
      "epoch: 9\n",
      "step: 0, acc: 0.758, loss: 0.553 (data_loss: 0.553, reg_loss: 0.000), lr: 0.0007271669575334498\n",
      "step: 100, acc: 0.773, loss: 0.738 (data_loss: 0.738, reg_loss: 0.000), lr: 0.000721917412647993\n",
      "step: 200, acc: 0.852, loss: 0.529 (data_loss: 0.529, reg_loss: 0.000), lr: 0.0007167431192660551\n",
      "step: 300, acc: 0.781, loss: 0.536 (data_loss: 0.536, reg_loss: 0.000), lr: 0.0007116424708226587\n",
      "step: 400, acc: 0.820, loss: 0.667 (data_loss: 0.667, reg_loss: 0.000), lr: 0.0007066139061616733\n",
      "step: 468, acc: 0.865, loss: 0.609 (data_loss: 0.609, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "training, acc: 0.783, loss: 0.635 (data_loss: 0.635, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "validation, acc: 0.829, loss: 0.482\n",
      "epoch: 10\n",
      "step: 0, acc: 0.773, loss: 0.557 (data_loss: 0.557, reg_loss: 0.000), lr: 0.0007031854299978904\n",
      "step: 100, acc: 0.750, loss: 0.836 (data_loss: 0.836, reg_loss: 0.000), lr: 0.0006982752601075343\n",
      "step: 200, acc: 0.789, loss: 0.625 (data_loss: 0.625, reg_loss: 0.000), lr: 0.000693433187712364\n",
      "step: 300, acc: 0.797, loss: 0.559 (data_loss: 0.559, reg_loss: 0.000), lr: 0.0006886578059362303\n",
      "step: 400, acc: 0.797, loss: 0.592 (data_loss: 0.592, reg_loss: 0.000), lr: 0.0006839477463921757\n",
      "step: 468, acc: 0.833, loss: 0.641 (data_loss: 0.641, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "training, acc: 0.785, loss: 0.628 (data_loss: 0.628, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "validation, acc: 0.830, loss: 0.486\n",
      "epoch: 1\n",
      "step: 0, acc: 0.148, loss: 2.302 (data_loss: 2.302, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.516, loss: 1.163 (data_loss: 1.163, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 200, acc: 0.625, loss: 1.067 (data_loss: 1.067, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 300, acc: 0.641, loss: 0.806 (data_loss: 0.806, reg_loss: 0.000), lr: 0.0009708737864077671\n",
      "step: 400, acc: 0.688, loss: 0.831 (data_loss: 0.831, reg_loss: 0.000), lr: 0.0009615384615384615\n",
      "step: 468, acc: 0.615, loss: 1.162 (data_loss: 1.162, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "training, acc: 0.572, loss: 1.076 (data_loss: 1.076, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "validation, acc: 0.775, loss: 0.595\n",
      "epoch: 2\n",
      "step: 0, acc: 0.703, loss: 0.807 (data_loss: 0.807, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.656, loss: 0.934 (data_loss: 0.934, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 200, acc: 0.695, loss: 0.746 (data_loss: 0.746, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 300, acc: 0.703, loss: 0.735 (data_loss: 0.735, reg_loss: 0.000), lr: 0.0009285913269570063\n",
      "step: 400, acc: 0.680, loss: 0.842 (data_loss: 0.842, reg_loss: 0.000), lr: 0.0009200478424878093\n",
      "step: 468, acc: 0.688, loss: 0.925 (data_loss: 0.925, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "training, acc: 0.666, loss: 0.858 (data_loss: 0.858, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "validation, acc: 0.782, loss: 0.565\n",
      "epoch: 3\n",
      "step: 0, acc: 0.680, loss: 0.803 (data_loss: 0.803, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.703, loss: 0.763 (data_loss: 0.763, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 200, acc: 0.727, loss: 0.783 (data_loss: 0.783, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 300, acc: 0.711, loss: 0.791 (data_loss: 0.791, reg_loss: 0.000), lr: 0.0008898380494749957\n",
      "step: 400, acc: 0.688, loss: 0.776 (data_loss: 0.776, reg_loss: 0.000), lr: 0.0008819897689186807\n",
      "step: 468, acc: 0.677, loss: 0.807 (data_loss: 0.807, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "training, acc: 0.679, loss: 0.832 (data_loss: 0.832, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "validation, acc: 0.796, loss: 0.549\n",
      "epoch: 4\n",
      "step: 0, acc: 0.656, loss: 0.816 (data_loss: 0.816, reg_loss: 0.000), lr: 0.0008766546857192952\n",
      "step: 100, acc: 0.672, loss: 0.778 (data_loss: 0.778, reg_loss: 0.000), lr: 0.0008690362388111585\n",
      "step: 200, acc: 0.750, loss: 0.727 (data_loss: 0.727, reg_loss: 0.000), lr: 0.0008615490652192642\n",
      "step: 300, acc: 0.727, loss: 0.720 (data_loss: 0.720, reg_loss: 0.000), lr: 0.0008541898009737763\n",
      "step: 400, acc: 0.688, loss: 0.883 (data_loss: 0.883, reg_loss: 0.000), lr: 0.0008469551960701278\n",
      "step: 468, acc: 0.708, loss: 0.854 (data_loss: 0.854, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "training, acc: 0.689, loss: 0.806 (data_loss: 0.806, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "validation, acc: 0.805, loss: 0.534\n",
      "epoch: 5\n",
      "step: 0, acc: 0.648, loss: 0.789 (data_loss: 0.789, reg_loss: 0.000), lr: 0.0008420343550016842\n",
      "step: 100, acc: 0.688, loss: 0.909 (data_loss: 0.909, reg_loss: 0.000), lr: 0.0008350033400133601\n",
      "step: 200, acc: 0.711, loss: 0.871 (data_loss: 0.871, reg_loss: 0.000), lr: 0.0008280887711162637\n",
      "step: 300, acc: 0.672, loss: 0.772 (data_loss: 0.772, reg_loss: 0.000), lr: 0.0008212877792378449\n",
      "step: 400, acc: 0.766, loss: 0.682 (data_loss: 0.682, reg_loss: 0.000), lr: 0.0008145975887911372\n",
      "step: 468, acc: 0.667, loss: 0.900 (data_loss: 0.900, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "training, acc: 0.697, loss: 0.786 (data_loss: 0.786, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "validation, acc: 0.815, loss: 0.504\n",
      "epoch: 6\n",
      "step: 0, acc: 0.672, loss: 0.809 (data_loss: 0.809, reg_loss: 0.000), lr: 0.0008100445524503849\n",
      "step: 100, acc: 0.688, loss: 0.912 (data_loss: 0.912, reg_loss: 0.000), lr: 0.0008035355564483729\n",
      "step: 200, acc: 0.711, loss: 0.804 (data_loss: 0.804, reg_loss: 0.000), lr: 0.0007971303308090873\n",
      "step: 300, acc: 0.766, loss: 0.548 (data_loss: 0.548, reg_loss: 0.000), lr: 0.0007908264136022144\n",
      "step: 400, acc: 0.750, loss: 0.750 (data_loss: 0.750, reg_loss: 0.000), lr: 0.0007846214201647706\n",
      "step: 468, acc: 0.719, loss: 0.901 (data_loss: 0.901, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "training, acc: 0.703, loss: 0.779 (data_loss: 0.779, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "validation, acc: 0.820, loss: 0.504\n",
      "epoch: 7\n",
      "step: 0, acc: 0.758, loss: 0.558 (data_loss: 0.558, reg_loss: 0.000), lr: 0.0007803964413922272\n",
      "step: 100, acc: 0.758, loss: 0.734 (data_loss: 0.734, reg_loss: 0.000), lr: 0.0007743534148985598\n",
      "step: 200, acc: 0.703, loss: 0.844 (data_loss: 0.844, reg_loss: 0.000), lr: 0.0007684032580298141\n",
      "step: 300, acc: 0.703, loss: 0.733 (data_loss: 0.733, reg_loss: 0.000), lr: 0.0007625438462711606\n",
      "step: 400, acc: 0.727, loss: 0.762 (data_loss: 0.762, reg_loss: 0.000), lr: 0.0007567731194187983\n",
      "step: 468, acc: 0.615, loss: 0.978 (data_loss: 0.978, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "training, acc: 0.709, loss: 0.765 (data_loss: 0.765, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "validation, acc: 0.817, loss: 0.500\n",
      "epoch: 8\n",
      "step: 0, acc: 0.727, loss: 0.736 (data_loss: 0.736, reg_loss: 0.000), lr: 0.0007528419784687194\n",
      "step: 100, acc: 0.727, loss: 0.738 (data_loss: 0.738, reg_loss: 0.000), lr: 0.0007472166180975864\n",
      "step: 200, acc: 0.734, loss: 0.753 (data_loss: 0.753, reg_loss: 0.000), lr: 0.0007416747014759327\n",
      "step: 300, acc: 0.711, loss: 0.716 (data_loss: 0.716, reg_loss: 0.000), lr: 0.0007362143856290952\n",
      "step: 400, acc: 0.711, loss: 0.904 (data_loss: 0.904, reg_loss: 0.000), lr: 0.0007308338814587444\n",
      "step: 468, acc: 0.646, loss: 0.911 (data_loss: 0.911, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "training, acc: 0.715, loss: 0.756 (data_loss: 0.756, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "validation, acc: 0.818, loss: 0.497\n",
      "epoch: 9\n",
      "step: 0, acc: 0.719, loss: 0.692 (data_loss: 0.692, reg_loss: 0.000), lr: 0.0007271669575334498\n",
      "step: 100, acc: 0.758, loss: 0.720 (data_loss: 0.720, reg_loss: 0.000), lr: 0.000721917412647993\n",
      "step: 200, acc: 0.750, loss: 0.803 (data_loss: 0.803, reg_loss: 0.000), lr: 0.0007167431192660551\n",
      "step: 300, acc: 0.711, loss: 0.634 (data_loss: 0.634, reg_loss: 0.000), lr: 0.0007116424708226587\n",
      "step: 400, acc: 0.797, loss: 0.646 (data_loss: 0.646, reg_loss: 0.000), lr: 0.0007066139061616733\n",
      "step: 468, acc: 0.719, loss: 0.768 (data_loss: 0.768, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "training, acc: 0.715, loss: 0.752 (data_loss: 0.752, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "validation, acc: 0.815, loss: 0.496\n",
      "epoch: 10\n",
      "step: 0, acc: 0.688, loss: 0.728 (data_loss: 0.728, reg_loss: 0.000), lr: 0.0007031854299978904\n",
      "step: 100, acc: 0.742, loss: 0.763 (data_loss: 0.763, reg_loss: 0.000), lr: 0.0006982752601075343\n",
      "step: 200, acc: 0.719, loss: 0.666 (data_loss: 0.666, reg_loss: 0.000), lr: 0.000693433187712364\n",
      "step: 300, acc: 0.758, loss: 0.579 (data_loss: 0.579, reg_loss: 0.000), lr: 0.0006886578059362303\n",
      "step: 400, acc: 0.711, loss: 0.826 (data_loss: 0.826, reg_loss: 0.000), lr: 0.0006839477463921757\n",
      "step: 468, acc: 0.656, loss: 0.925 (data_loss: 0.925, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "training, acc: 0.720, loss: 0.746 (data_loss: 0.746, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "validation, acc: 0.820, loss: 0.506\n",
      "epoch: 1\n",
      "step: 0, acc: 0.094, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.461, loss: 1.248 (data_loss: 1.248, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 200, acc: 0.562, loss: 1.080 (data_loss: 1.080, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 300, acc: 0.547, loss: 1.045 (data_loss: 1.045, reg_loss: 0.000), lr: 0.0009708737864077671\n",
      "step: 400, acc: 0.500, loss: 1.146 (data_loss: 1.146, reg_loss: 0.000), lr: 0.0009615384615384615\n",
      "step: 468, acc: 0.531, loss: 1.125 (data_loss: 1.125, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "training, acc: 0.474, loss: 1.238 (data_loss: 1.238, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "validation, acc: 0.721, loss: 0.755\n",
      "epoch: 2\n",
      "step: 0, acc: 0.562, loss: 0.996 (data_loss: 0.996, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.617, loss: 1.119 (data_loss: 1.119, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 200, acc: 0.594, loss: 0.983 (data_loss: 0.983, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 300, acc: 0.656, loss: 0.815 (data_loss: 0.815, reg_loss: 0.000), lr: 0.0009285913269570063\n",
      "step: 400, acc: 0.641, loss: 0.977 (data_loss: 0.977, reg_loss: 0.000), lr: 0.0009200478424878093\n",
      "step: 468, acc: 0.542, loss: 1.086 (data_loss: 1.086, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "training, acc: 0.602, loss: 0.979 (data_loss: 0.979, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "validation, acc: 0.772, loss: 0.633\n",
      "epoch: 3\n",
      "step: 0, acc: 0.672, loss: 0.823 (data_loss: 0.823, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.680, loss: 1.016 (data_loss: 1.016, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 200, acc: 0.656, loss: 0.803 (data_loss: 0.803, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 300, acc: 0.680, loss: 0.759 (data_loss: 0.759, reg_loss: 0.000), lr: 0.0008898380494749957\n",
      "step: 400, acc: 0.688, loss: 0.802 (data_loss: 0.802, reg_loss: 0.000), lr: 0.0008819897689186807\n",
      "step: 468, acc: 0.625, loss: 0.975 (data_loss: 0.975, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "training, acc: 0.643, loss: 0.890 (data_loss: 0.890, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "validation, acc: 0.786, loss: 0.565\n",
      "epoch: 4\n",
      "step: 0, acc: 0.617, loss: 0.883 (data_loss: 0.883, reg_loss: 0.000), lr: 0.0008766546857192952\n",
      "step: 100, acc: 0.641, loss: 0.816 (data_loss: 0.816, reg_loss: 0.000), lr: 0.0008690362388111585\n",
      "step: 200, acc: 0.648, loss: 0.913 (data_loss: 0.913, reg_loss: 0.000), lr: 0.0008615490652192642\n",
      "step: 300, acc: 0.727, loss: 0.737 (data_loss: 0.737, reg_loss: 0.000), lr: 0.0008541898009737763\n",
      "step: 400, acc: 0.711, loss: 0.787 (data_loss: 0.787, reg_loss: 0.000), lr: 0.0008469551960701278\n",
      "step: 468, acc: 0.698, loss: 0.982 (data_loss: 0.982, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "training, acc: 0.671, loss: 0.829 (data_loss: 0.829, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "validation, acc: 0.793, loss: 0.544\n",
      "epoch: 5\n",
      "step: 0, acc: 0.750, loss: 0.740 (data_loss: 0.740, reg_loss: 0.000), lr: 0.0008420343550016842\n",
      "step: 100, acc: 0.719, loss: 0.933 (data_loss: 0.933, reg_loss: 0.000), lr: 0.0008350033400133601\n",
      "step: 200, acc: 0.742, loss: 0.816 (data_loss: 0.816, reg_loss: 0.000), lr: 0.0008280887711162637\n",
      "step: 300, acc: 0.711, loss: 0.756 (data_loss: 0.756, reg_loss: 0.000), lr: 0.0008212877792378449\n",
      "step: 400, acc: 0.680, loss: 0.813 (data_loss: 0.813, reg_loss: 0.000), lr: 0.0008145975887911372\n",
      "step: 468, acc: 0.677, loss: 0.931 (data_loss: 0.931, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "training, acc: 0.697, loss: 0.777 (data_loss: 0.777, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "validation, acc: 0.797, loss: 0.545\n",
      "epoch: 6\n",
      "step: 0, acc: 0.680, loss: 0.767 (data_loss: 0.767, reg_loss: 0.000), lr: 0.0008100445524503849\n",
      "step: 100, acc: 0.758, loss: 0.693 (data_loss: 0.693, reg_loss: 0.000), lr: 0.0008035355564483729\n",
      "step: 200, acc: 0.711, loss: 0.774 (data_loss: 0.774, reg_loss: 0.000), lr: 0.0007971303308090873\n",
      "step: 300, acc: 0.734, loss: 0.729 (data_loss: 0.729, reg_loss: 0.000), lr: 0.0007908264136022144\n",
      "step: 400, acc: 0.758, loss: 0.728 (data_loss: 0.728, reg_loss: 0.000), lr: 0.0007846214201647706\n",
      "step: 468, acc: 0.719, loss: 0.771 (data_loss: 0.771, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "training, acc: 0.717, loss: 0.742 (data_loss: 0.742, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "validation, acc: 0.824, loss: 0.498\n",
      "epoch: 7\n",
      "step: 0, acc: 0.766, loss: 0.647 (data_loss: 0.647, reg_loss: 0.000), lr: 0.0007803964413922272\n",
      "step: 100, acc: 0.664, loss: 0.893 (data_loss: 0.893, reg_loss: 0.000), lr: 0.0007743534148985598\n",
      "step: 200, acc: 0.773, loss: 0.584 (data_loss: 0.584, reg_loss: 0.000), lr: 0.0007684032580298141\n",
      "step: 300, acc: 0.758, loss: 0.666 (data_loss: 0.666, reg_loss: 0.000), lr: 0.0007625438462711606\n",
      "step: 400, acc: 0.711, loss: 0.692 (data_loss: 0.692, reg_loss: 0.000), lr: 0.0007567731194187983\n",
      "step: 468, acc: 0.740, loss: 0.754 (data_loss: 0.754, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "training, acc: 0.729, loss: 0.719 (data_loss: 0.719, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "validation, acc: 0.815, loss: 0.500\n",
      "epoch: 8\n",
      "step: 0, acc: 0.703, loss: 0.703 (data_loss: 0.703, reg_loss: 0.000), lr: 0.0007528419784687194\n",
      "step: 100, acc: 0.719, loss: 0.841 (data_loss: 0.841, reg_loss: 0.000), lr: 0.0007472166180975864\n",
      "step: 200, acc: 0.742, loss: 0.631 (data_loss: 0.631, reg_loss: 0.000), lr: 0.0007416747014759327\n",
      "step: 300, acc: 0.750, loss: 0.597 (data_loss: 0.597, reg_loss: 0.000), lr: 0.0007362143856290952\n",
      "step: 400, acc: 0.766, loss: 0.591 (data_loss: 0.591, reg_loss: 0.000), lr: 0.0007308338814587444\n",
      "step: 468, acc: 0.667, loss: 0.863 (data_loss: 0.863, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "training, acc: 0.742, loss: 0.689 (data_loss: 0.689, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "validation, acc: 0.836, loss: 0.459\n",
      "epoch: 9\n",
      "step: 0, acc: 0.758, loss: 0.611 (data_loss: 0.611, reg_loss: 0.000), lr: 0.0007271669575334498\n",
      "step: 100, acc: 0.781, loss: 0.794 (data_loss: 0.794, reg_loss: 0.000), lr: 0.000721917412647993\n",
      "step: 200, acc: 0.750, loss: 0.599 (data_loss: 0.599, reg_loss: 0.000), lr: 0.0007167431192660551\n",
      "step: 300, acc: 0.758, loss: 0.598 (data_loss: 0.598, reg_loss: 0.000), lr: 0.0007116424708226587\n",
      "step: 400, acc: 0.719, loss: 0.673 (data_loss: 0.673, reg_loss: 0.000), lr: 0.0007066139061616733\n",
      "step: 468, acc: 0.708, loss: 0.894 (data_loss: 0.894, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "training, acc: 0.751, loss: 0.669 (data_loss: 0.669, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "validation, acc: 0.831, loss: 0.466\n",
      "epoch: 10\n",
      "step: 0, acc: 0.766, loss: 0.688 (data_loss: 0.688, reg_loss: 0.000), lr: 0.0007031854299978904\n",
      "step: 100, acc: 0.766, loss: 0.706 (data_loss: 0.706, reg_loss: 0.000), lr: 0.0006982752601075343\n",
      "step: 200, acc: 0.820, loss: 0.577 (data_loss: 0.577, reg_loss: 0.000), lr: 0.000693433187712364\n",
      "step: 300, acc: 0.727, loss: 0.682 (data_loss: 0.682, reg_loss: 0.000), lr: 0.0006886578059362303\n",
      "step: 400, acc: 0.766, loss: 0.733 (data_loss: 0.733, reg_loss: 0.000), lr: 0.0006839477463921757\n",
      "step: 468, acc: 0.740, loss: 0.873 (data_loss: 0.873, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "training, acc: 0.757, loss: 0.650 (data_loss: 0.650, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "validation, acc: 0.839, loss: 0.451\n",
      "epoch: 1\n",
      "step: 0, acc: 0.109, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.312, loss: 1.582 (data_loss: 1.582, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 200, acc: 0.445, loss: 1.364 (data_loss: 1.364, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 300, acc: 0.461, loss: 1.271 (data_loss: 1.271, reg_loss: 0.000), lr: 0.0009708737864077671\n",
      "step: 400, acc: 0.484, loss: 1.198 (data_loss: 1.198, reg_loss: 0.000), lr: 0.0009615384615384615\n",
      "step: 468, acc: 0.427, loss: 1.257 (data_loss: 1.257, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "training, acc: 0.387, loss: 1.404 (data_loss: 1.404, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "validation, acc: 0.675, loss: 0.833\n",
      "epoch: 2\n",
      "step: 0, acc: 0.531, loss: 1.070 (data_loss: 1.070, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.562, loss: 1.030 (data_loss: 1.030, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 200, acc: 0.562, loss: 1.139 (data_loss: 1.139, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 300, acc: 0.539, loss: 0.961 (data_loss: 0.961, reg_loss: 0.000), lr: 0.0009285913269570063\n",
      "step: 400, acc: 0.656, loss: 0.834 (data_loss: 0.834, reg_loss: 0.000), lr: 0.0009200478424878093\n",
      "step: 468, acc: 0.594, loss: 1.032 (data_loss: 1.032, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "training, acc: 0.549, loss: 1.069 (data_loss: 1.069, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "validation, acc: 0.727, loss: 0.734\n",
      "epoch: 3\n",
      "step: 0, acc: 0.617, loss: 0.916 (data_loss: 0.916, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.586, loss: 1.039 (data_loss: 1.039, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 200, acc: 0.602, loss: 0.945 (data_loss: 0.945, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 300, acc: 0.625, loss: 0.946 (data_loss: 0.946, reg_loss: 0.000), lr: 0.0008898380494749957\n",
      "step: 400, acc: 0.594, loss: 0.958 (data_loss: 0.958, reg_loss: 0.000), lr: 0.0008819897689186807\n",
      "step: 468, acc: 0.510, loss: 1.016 (data_loss: 1.016, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "training, acc: 0.609, loss: 0.963 (data_loss: 0.963, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "validation, acc: 0.771, loss: 0.637\n",
      "epoch: 4\n",
      "step: 0, acc: 0.672, loss: 0.884 (data_loss: 0.884, reg_loss: 0.000), lr: 0.0008766546857192952\n",
      "step: 100, acc: 0.633, loss: 0.971 (data_loss: 0.971, reg_loss: 0.000), lr: 0.0008690362388111585\n",
      "step: 200, acc: 0.680, loss: 0.858 (data_loss: 0.858, reg_loss: 0.000), lr: 0.0008615490652192642\n",
      "step: 300, acc: 0.656, loss: 0.850 (data_loss: 0.850, reg_loss: 0.000), lr: 0.0008541898009737763\n",
      "step: 400, acc: 0.672, loss: 0.969 (data_loss: 0.969, reg_loss: 0.000), lr: 0.0008469551960701278\n",
      "step: 468, acc: 0.635, loss: 0.923 (data_loss: 0.923, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "training, acc: 0.644, loss: 0.897 (data_loss: 0.897, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "validation, acc: 0.786, loss: 0.573\n",
      "epoch: 5\n",
      "step: 0, acc: 0.773, loss: 0.693 (data_loss: 0.693, reg_loss: 0.000), lr: 0.0008420343550016842\n",
      "step: 100, acc: 0.648, loss: 0.916 (data_loss: 0.916, reg_loss: 0.000), lr: 0.0008350033400133601\n",
      "step: 200, acc: 0.688, loss: 0.837 (data_loss: 0.837, reg_loss: 0.000), lr: 0.0008280887711162637\n",
      "step: 300, acc: 0.703, loss: 0.753 (data_loss: 0.753, reg_loss: 0.000), lr: 0.0008212877792378449\n",
      "step: 400, acc: 0.672, loss: 0.783 (data_loss: 0.783, reg_loss: 0.000), lr: 0.0008145975887911372\n",
      "step: 468, acc: 0.667, loss: 0.928 (data_loss: 0.928, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "training, acc: 0.676, loss: 0.828 (data_loss: 0.828, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "validation, acc: 0.782, loss: 0.558\n",
      "epoch: 6\n",
      "step: 0, acc: 0.688, loss: 0.748 (data_loss: 0.748, reg_loss: 0.000), lr: 0.0008100445524503849\n",
      "step: 100, acc: 0.703, loss: 0.726 (data_loss: 0.726, reg_loss: 0.000), lr: 0.0008035355564483729\n",
      "step: 200, acc: 0.680, loss: 0.729 (data_loss: 0.729, reg_loss: 0.000), lr: 0.0007971303308090873\n",
      "step: 300, acc: 0.750, loss: 0.605 (data_loss: 0.605, reg_loss: 0.000), lr: 0.0007908264136022144\n",
      "step: 400, acc: 0.750, loss: 0.766 (data_loss: 0.766, reg_loss: 0.000), lr: 0.0007846214201647706\n",
      "step: 468, acc: 0.708, loss: 0.972 (data_loss: 0.972, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "training, acc: 0.700, loss: 0.776 (data_loss: 0.776, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "validation, acc: 0.798, loss: 0.535\n",
      "epoch: 7\n",
      "step: 0, acc: 0.672, loss: 0.896 (data_loss: 0.896, reg_loss: 0.000), lr: 0.0007803964413922272\n",
      "step: 100, acc: 0.727, loss: 0.737 (data_loss: 0.737, reg_loss: 0.000), lr: 0.0007743534148985598\n",
      "step: 200, acc: 0.703, loss: 0.835 (data_loss: 0.835, reg_loss: 0.000), lr: 0.0007684032580298141\n",
      "step: 300, acc: 0.719, loss: 0.751 (data_loss: 0.751, reg_loss: 0.000), lr: 0.0007625438462711606\n",
      "step: 400, acc: 0.734, loss: 0.712 (data_loss: 0.712, reg_loss: 0.000), lr: 0.0007567731194187983\n",
      "step: 468, acc: 0.740, loss: 0.918 (data_loss: 0.918, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "training, acc: 0.713, loss: 0.747 (data_loss: 0.747, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "validation, acc: 0.802, loss: 0.559\n",
      "epoch: 8\n",
      "step: 0, acc: 0.758, loss: 0.752 (data_loss: 0.752, reg_loss: 0.000), lr: 0.0007528419784687194\n",
      "step: 100, acc: 0.773, loss: 0.828 (data_loss: 0.828, reg_loss: 0.000), lr: 0.0007472166180975864\n",
      "step: 200, acc: 0.758, loss: 0.621 (data_loss: 0.621, reg_loss: 0.000), lr: 0.0007416747014759327\n",
      "step: 300, acc: 0.711, loss: 0.635 (data_loss: 0.635, reg_loss: 0.000), lr: 0.0007362143856290952\n",
      "step: 400, acc: 0.719, loss: 0.814 (data_loss: 0.814, reg_loss: 0.000), lr: 0.0007308338814587444\n",
      "step: 468, acc: 0.760, loss: 0.666 (data_loss: 0.666, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "training, acc: 0.727, loss: 0.722 (data_loss: 0.722, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "validation, acc: 0.817, loss: 0.515\n",
      "epoch: 9\n",
      "step: 0, acc: 0.750, loss: 0.674 (data_loss: 0.674, reg_loss: 0.000), lr: 0.0007271669575334498\n",
      "step: 100, acc: 0.664, loss: 0.946 (data_loss: 0.946, reg_loss: 0.000), lr: 0.000721917412647993\n",
      "step: 200, acc: 0.703, loss: 0.782 (data_loss: 0.782, reg_loss: 0.000), lr: 0.0007167431192660551\n",
      "step: 300, acc: 0.766, loss: 0.621 (data_loss: 0.621, reg_loss: 0.000), lr: 0.0007116424708226587\n",
      "step: 400, acc: 0.727, loss: 0.748 (data_loss: 0.748, reg_loss: 0.000), lr: 0.0007066139061616733\n",
      "step: 468, acc: 0.750, loss: 0.895 (data_loss: 0.895, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "training, acc: 0.740, loss: 0.693 (data_loss: 0.693, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "validation, acc: 0.835, loss: 0.481\n",
      "epoch: 10\n",
      "step: 0, acc: 0.758, loss: 0.630 (data_loss: 0.630, reg_loss: 0.000), lr: 0.0007031854299978904\n",
      "step: 100, acc: 0.781, loss: 0.695 (data_loss: 0.695, reg_loss: 0.000), lr: 0.0006982752601075343\n",
      "step: 200, acc: 0.734, loss: 0.572 (data_loss: 0.572, reg_loss: 0.000), lr: 0.000693433187712364\n",
      "step: 300, acc: 0.766, loss: 0.661 (data_loss: 0.661, reg_loss: 0.000), lr: 0.0006886578059362303\n",
      "step: 400, acc: 0.711, loss: 0.668 (data_loss: 0.668, reg_loss: 0.000), lr: 0.0006839477463921757\n",
      "step: 468, acc: 0.750, loss: 0.772 (data_loss: 0.772, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "training, acc: 0.750, loss: 0.677 (data_loss: 0.677, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "validation, acc: 0.838, loss: 0.468\n"
     ]
    }
   ],
   "source": [
    "# Train models with different nummber of layers\n",
    "Train_Model(X=X, y=y, X_test=X_test, y_test=y_test, model_number=\"_layers0\", number_of_layers=0)\n",
    "Train_Model(X=X, y=y, X_test=X_test, y_test=y_test, model_number=\"_layers1\", number_of_layers=1)\n",
    "Train_Model(X=X, y=y, X_test=X_test, y_test=y_test, model_number=\"_layers2\", number_of_layers=2)\n",
    "Train_Model(X=X, y=y, X_test=X_test, y_test=y_test, model_number=\"_layers3\", number_of_layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with 0 layers:\n",
      "validation, acc: 0.830, loss: 0.486\n",
      "Model with 1 layer:\n",
      "validation, acc: 0.820, loss: 0.506\n",
      "Model with 2 layers:\n",
      "validation, acc: 0.839, loss: 0.451\n",
      "Model with 3 layers:\n",
      "validation, acc: 0.838, loss: 0.468\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models\n",
    "print(\"Model with 0 layers:\")\n",
    "model = Model.load(\"fashion_mnist_layers0.model\")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Model with 1 layer:\")\n",
    "model = Model.load(\"fashion_mnist_layers1.model\")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Model with 2 layers:\")\n",
    "model = Model.load(\"fashion_mnist_layers2.model\")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Model with 3 layers:\")\n",
    "model = Model.load(\"fashion_mnist_layers3.model\")\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0, acc: 0.047, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.258, loss: 1.768 (data_loss: 1.768, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 200, acc: 0.328, loss: 1.537 (data_loss: 1.537, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 300, acc: 0.375, loss: 1.434 (data_loss: 1.434, reg_loss: 0.000), lr: 0.0009708737864077671\n",
      "step: 400, acc: 0.383, loss: 1.576 (data_loss: 1.576, reg_loss: 0.000), lr: 0.0009615384615384615\n",
      "step: 468, acc: 0.281, loss: 1.664 (data_loss: 1.664, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "training, acc: 0.275, loss: 1.697 (data_loss: 1.697, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "validation, acc: 0.594, loss: 1.099\n",
      "epoch: 2\n",
      "step: 0, acc: 0.352, loss: 1.499 (data_loss: 1.499, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.320, loss: 1.581 (data_loss: 1.581, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 200, acc: 0.305, loss: 1.591 (data_loss: 1.591, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 300, acc: 0.352, loss: 1.512 (data_loss: 1.512, reg_loss: 0.000), lr: 0.0009285913269570063\n",
      "step: 400, acc: 0.391, loss: 1.430 (data_loss: 1.430, reg_loss: 0.000), lr: 0.0009200478424878093\n",
      "step: 468, acc: 0.302, loss: 1.693 (data_loss: 1.693, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "training, acc: 0.329, loss: 1.555 (data_loss: 1.555, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "validation, acc: 0.644, loss: 1.050\n",
      "epoch: 3\n",
      "step: 0, acc: 0.375, loss: 1.448 (data_loss: 1.448, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.391, loss: 1.556 (data_loss: 1.556, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 200, acc: 0.375, loss: 1.488 (data_loss: 1.488, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 300, acc: 0.398, loss: 1.491 (data_loss: 1.491, reg_loss: 0.000), lr: 0.0008898380494749957\n",
      "step: 400, acc: 0.359, loss: 1.525 (data_loss: 1.525, reg_loss: 0.000), lr: 0.0008819897689186807\n",
      "step: 468, acc: 0.250, loss: 1.773 (data_loss: 1.773, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "training, acc: 0.346, loss: 1.530 (data_loss: 1.530, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "validation, acc: 0.679, loss: 1.005\n",
      "epoch: 4\n",
      "step: 0, acc: 0.375, loss: 1.519 (data_loss: 1.519, reg_loss: 0.000), lr: 0.0008766546857192952\n",
      "step: 100, acc: 0.398, loss: 1.435 (data_loss: 1.435, reg_loss: 0.000), lr: 0.0008690362388111585\n",
      "step: 200, acc: 0.375, loss: 1.570 (data_loss: 1.570, reg_loss: 0.000), lr: 0.0008615490652192642\n",
      "step: 300, acc: 0.414, loss: 1.435 (data_loss: 1.435, reg_loss: 0.000), lr: 0.0008541898009737763\n",
      "step: 400, acc: 0.398, loss: 1.459 (data_loss: 1.459, reg_loss: 0.000), lr: 0.0008469551960701278\n",
      "step: 468, acc: 0.312, loss: 1.613 (data_loss: 1.613, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "training, acc: 0.349, loss: 1.515 (data_loss: 1.515, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "validation, acc: 0.677, loss: 0.966\n",
      "epoch: 5\n",
      "step: 0, acc: 0.367, loss: 1.458 (data_loss: 1.458, reg_loss: 0.000), lr: 0.0008420343550016842\n",
      "step: 100, acc: 0.406, loss: 1.400 (data_loss: 1.400, reg_loss: 0.000), lr: 0.0008350033400133601\n",
      "step: 200, acc: 0.344, loss: 1.460 (data_loss: 1.460, reg_loss: 0.000), lr: 0.0008280887711162637\n",
      "step: 300, acc: 0.445, loss: 1.400 (data_loss: 1.400, reg_loss: 0.000), lr: 0.0008212877792378449\n",
      "step: 400, acc: 0.352, loss: 1.375 (data_loss: 1.375, reg_loss: 0.000), lr: 0.0008145975887911372\n",
      "step: 468, acc: 0.427, loss: 1.414 (data_loss: 1.414, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "training, acc: 0.357, loss: 1.507 (data_loss: 1.507, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "validation, acc: 0.672, loss: 0.993\n",
      "epoch: 6\n",
      "step: 0, acc: 0.312, loss: 1.636 (data_loss: 1.636, reg_loss: 0.000), lr: 0.0008100445524503849\n",
      "step: 100, acc: 0.297, loss: 1.581 (data_loss: 1.581, reg_loss: 0.000), lr: 0.0008035355564483729\n",
      "step: 200, acc: 0.430, loss: 1.384 (data_loss: 1.384, reg_loss: 0.000), lr: 0.0007971303308090873\n",
      "step: 300, acc: 0.367, loss: 1.532 (data_loss: 1.532, reg_loss: 0.000), lr: 0.0007908264136022144\n",
      "step: 400, acc: 0.477, loss: 1.448 (data_loss: 1.448, reg_loss: 0.000), lr: 0.0007846214201647706\n",
      "step: 468, acc: 0.375, loss: 1.466 (data_loss: 1.466, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "training, acc: 0.359, loss: 1.507 (data_loss: 1.507, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "validation, acc: 0.709, loss: 0.954\n",
      "epoch: 7\n",
      "step: 0, acc: 0.391, loss: 1.396 (data_loss: 1.396, reg_loss: 0.000), lr: 0.0007803964413922272\n",
      "step: 100, acc: 0.336, loss: 1.445 (data_loss: 1.445, reg_loss: 0.000), lr: 0.0007743534148985598\n",
      "step: 200, acc: 0.312, loss: 1.567 (data_loss: 1.567, reg_loss: 0.000), lr: 0.0007684032580298141\n",
      "step: 300, acc: 0.344, loss: 1.477 (data_loss: 1.477, reg_loss: 0.000), lr: 0.0007625438462711606\n",
      "step: 400, acc: 0.367, loss: 1.589 (data_loss: 1.589, reg_loss: 0.000), lr: 0.0007567731194187983\n",
      "step: 468, acc: 0.406, loss: 1.454 (data_loss: 1.454, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "training, acc: 0.366, loss: 1.498 (data_loss: 1.498, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "validation, acc: 0.670, loss: 0.941\n",
      "epoch: 8\n",
      "step: 0, acc: 0.422, loss: 1.451 (data_loss: 1.451, reg_loss: 0.000), lr: 0.0007528419784687194\n",
      "step: 100, acc: 0.383, loss: 1.477 (data_loss: 1.477, reg_loss: 0.000), lr: 0.0007472166180975864\n",
      "step: 200, acc: 0.336, loss: 1.564 (data_loss: 1.564, reg_loss: 0.000), lr: 0.0007416747014759327\n",
      "step: 300, acc: 0.344, loss: 1.495 (data_loss: 1.495, reg_loss: 0.000), lr: 0.0007362143856290952\n",
      "step: 400, acc: 0.352, loss: 1.473 (data_loss: 1.473, reg_loss: 0.000), lr: 0.0007308338814587444\n",
      "step: 468, acc: 0.417, loss: 1.359 (data_loss: 1.359, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "training, acc: 0.368, loss: 1.501 (data_loss: 1.501, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "validation, acc: 0.714, loss: 0.935\n",
      "epoch: 9\n",
      "step: 0, acc: 0.469, loss: 1.370 (data_loss: 1.370, reg_loss: 0.000), lr: 0.0007271669575334498\n",
      "step: 100, acc: 0.352, loss: 1.562 (data_loss: 1.562, reg_loss: 0.000), lr: 0.000721917412647993\n",
      "step: 200, acc: 0.328, loss: 1.511 (data_loss: 1.511, reg_loss: 0.000), lr: 0.0007167431192660551\n",
      "step: 300, acc: 0.367, loss: 1.444 (data_loss: 1.444, reg_loss: 0.000), lr: 0.0007116424708226587\n",
      "step: 400, acc: 0.367, loss: 1.497 (data_loss: 1.497, reg_loss: 0.000), lr: 0.0007066139061616733\n",
      "step: 468, acc: 0.375, loss: 1.488 (data_loss: 1.488, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "training, acc: 0.371, loss: 1.500 (data_loss: 1.500, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "validation, acc: 0.685, loss: 0.935\n",
      "epoch: 10\n",
      "step: 0, acc: 0.391, loss: 1.497 (data_loss: 1.497, reg_loss: 0.000), lr: 0.0007031854299978904\n",
      "step: 100, acc: 0.391, loss: 1.496 (data_loss: 1.496, reg_loss: 0.000), lr: 0.0006982752601075343\n",
      "step: 200, acc: 0.312, loss: 1.569 (data_loss: 1.569, reg_loss: 0.000), lr: 0.000693433187712364\n",
      "step: 300, acc: 0.328, loss: 1.548 (data_loss: 1.548, reg_loss: 0.000), lr: 0.0006886578059362303\n",
      "step: 400, acc: 0.352, loss: 1.540 (data_loss: 1.540, reg_loss: 0.000), lr: 0.0006839477463921757\n",
      "step: 468, acc: 0.312, loss: 1.643 (data_loss: 1.643, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "training, acc: 0.374, loss: 1.497 (data_loss: 1.497, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "validation, acc: 0.707, loss: 0.939\n",
      "epoch: 1\n",
      "step: 0, acc: 0.062, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.438, loss: 1.428 (data_loss: 1.428, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 200, acc: 0.461, loss: 1.281 (data_loss: 1.281, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 300, acc: 0.414, loss: 1.312 (data_loss: 1.312, reg_loss: 0.000), lr: 0.0009708737864077671\n",
      "step: 400, acc: 0.453, loss: 1.244 (data_loss: 1.244, reg_loss: 0.000), lr: 0.0009615384615384615\n",
      "step: 468, acc: 0.531, loss: 1.131 (data_loss: 1.131, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "training, acc: 0.410, loss: 1.380 (data_loss: 1.380, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "validation, acc: 0.736, loss: 0.795\n",
      "epoch: 2\n",
      "step: 0, acc: 0.445, loss: 1.176 (data_loss: 1.176, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.453, loss: 1.350 (data_loss: 1.350, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 200, acc: 0.484, loss: 1.133 (data_loss: 1.133, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 300, acc: 0.461, loss: 1.208 (data_loss: 1.208, reg_loss: 0.000), lr: 0.0009285913269570063\n",
      "step: 400, acc: 0.453, loss: 1.231 (data_loss: 1.231, reg_loss: 0.000), lr: 0.0009200478424878093\n",
      "step: 468, acc: 0.510, loss: 1.344 (data_loss: 1.344, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "training, acc: 0.495, loss: 1.204 (data_loss: 1.204, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "validation, acc: 0.764, loss: 0.715\n",
      "epoch: 3\n",
      "step: 0, acc: 0.570, loss: 1.117 (data_loss: 1.117, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.516, loss: 1.193 (data_loss: 1.193, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 200, acc: 0.453, loss: 1.203 (data_loss: 1.203, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 300, acc: 0.539, loss: 1.091 (data_loss: 1.091, reg_loss: 0.000), lr: 0.0008898380494749957\n",
      "step: 400, acc: 0.570, loss: 1.096 (data_loss: 1.096, reg_loss: 0.000), lr: 0.0008819897689186807\n",
      "step: 468, acc: 0.615, loss: 1.236 (data_loss: 1.236, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "training, acc: 0.514, loss: 1.170 (data_loss: 1.170, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "validation, acc: 0.762, loss: 0.704\n",
      "epoch: 4\n",
      "step: 0, acc: 0.617, loss: 0.949 (data_loss: 0.949, reg_loss: 0.000), lr: 0.0008766546857192952\n",
      "step: 100, acc: 0.625, loss: 0.976 (data_loss: 0.976, reg_loss: 0.000), lr: 0.0008690362388111585\n",
      "step: 200, acc: 0.555, loss: 1.081 (data_loss: 1.081, reg_loss: 0.000), lr: 0.0008615490652192642\n",
      "step: 300, acc: 0.562, loss: 1.129 (data_loss: 1.129, reg_loss: 0.000), lr: 0.0008541898009737763\n",
      "step: 400, acc: 0.578, loss: 1.080 (data_loss: 1.080, reg_loss: 0.000), lr: 0.0008469551960701278\n",
      "step: 468, acc: 0.458, loss: 1.277 (data_loss: 1.277, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "training, acc: 0.524, loss: 1.144 (data_loss: 1.144, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "validation, acc: 0.767, loss: 0.679\n",
      "epoch: 5\n",
      "step: 0, acc: 0.555, loss: 1.026 (data_loss: 1.026, reg_loss: 0.000), lr: 0.0008420343550016842\n",
      "step: 100, acc: 0.570, loss: 1.094 (data_loss: 1.094, reg_loss: 0.000), lr: 0.0008350033400133601\n",
      "step: 200, acc: 0.602, loss: 1.041 (data_loss: 1.041, reg_loss: 0.000), lr: 0.0008280887711162637\n",
      "step: 300, acc: 0.586, loss: 1.012 (data_loss: 1.012, reg_loss: 0.000), lr: 0.0008212877792378449\n",
      "step: 400, acc: 0.602, loss: 1.202 (data_loss: 1.202, reg_loss: 0.000), lr: 0.0008145975887911372\n",
      "step: 468, acc: 0.542, loss: 1.212 (data_loss: 1.212, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "training, acc: 0.537, loss: 1.126 (data_loss: 1.126, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "validation, acc: 0.779, loss: 0.655\n",
      "epoch: 6\n",
      "step: 0, acc: 0.578, loss: 1.119 (data_loss: 1.119, reg_loss: 0.000), lr: 0.0008100445524503849\n",
      "step: 100, acc: 0.523, loss: 1.150 (data_loss: 1.150, reg_loss: 0.000), lr: 0.0008035355564483729\n",
      "step: 200, acc: 0.555, loss: 1.151 (data_loss: 1.151, reg_loss: 0.000), lr: 0.0007971303308090873\n",
      "step: 300, acc: 0.531, loss: 1.034 (data_loss: 1.034, reg_loss: 0.000), lr: 0.0007908264136022144\n",
      "step: 400, acc: 0.602, loss: 1.127 (data_loss: 1.127, reg_loss: 0.000), lr: 0.0007846214201647706\n",
      "step: 468, acc: 0.510, loss: 1.291 (data_loss: 1.291, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "training, acc: 0.543, loss: 1.118 (data_loss: 1.118, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "validation, acc: 0.780, loss: 0.647\n",
      "epoch: 7\n",
      "step: 0, acc: 0.508, loss: 1.098 (data_loss: 1.098, reg_loss: 0.000), lr: 0.0007803964413922272\n",
      "step: 100, acc: 0.562, loss: 1.064 (data_loss: 1.064, reg_loss: 0.000), lr: 0.0007743534148985598\n",
      "step: 200, acc: 0.500, loss: 1.084 (data_loss: 1.084, reg_loss: 0.000), lr: 0.0007684032580298141\n",
      "step: 300, acc: 0.578, loss: 0.998 (data_loss: 0.998, reg_loss: 0.000), lr: 0.0007625438462711606\n",
      "step: 400, acc: 0.602, loss: 1.060 (data_loss: 1.060, reg_loss: 0.000), lr: 0.0007567731194187983\n",
      "step: 468, acc: 0.594, loss: 1.117 (data_loss: 1.117, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "training, acc: 0.550, loss: 1.105 (data_loss: 1.105, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "validation, acc: 0.777, loss: 0.639\n",
      "epoch: 8\n",
      "step: 0, acc: 0.531, loss: 1.041 (data_loss: 1.041, reg_loss: 0.000), lr: 0.0007528419784687194\n",
      "step: 100, acc: 0.602, loss: 1.183 (data_loss: 1.183, reg_loss: 0.000), lr: 0.0007472166180975864\n",
      "step: 200, acc: 0.586, loss: 1.181 (data_loss: 1.181, reg_loss: 0.000), lr: 0.0007416747014759327\n",
      "step: 300, acc: 0.594, loss: 1.022 (data_loss: 1.022, reg_loss: 0.000), lr: 0.0007362143856290952\n",
      "step: 400, acc: 0.516, loss: 1.107 (data_loss: 1.107, reg_loss: 0.000), lr: 0.0007308338814587444\n",
      "step: 468, acc: 0.469, loss: 1.199 (data_loss: 1.199, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "training, acc: 0.552, loss: 1.102 (data_loss: 1.102, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "validation, acc: 0.781, loss: 0.625\n",
      "epoch: 9\n",
      "step: 0, acc: 0.641, loss: 0.909 (data_loss: 0.909, reg_loss: 0.000), lr: 0.0007271669575334498\n",
      "step: 100, acc: 0.570, loss: 0.979 (data_loss: 0.979, reg_loss: 0.000), lr: 0.000721917412647993\n",
      "step: 200, acc: 0.508, loss: 1.287 (data_loss: 1.287, reg_loss: 0.000), lr: 0.0007167431192660551\n",
      "step: 300, acc: 0.555, loss: 0.977 (data_loss: 0.977, reg_loss: 0.000), lr: 0.0007116424708226587\n",
      "step: 400, acc: 0.547, loss: 1.089 (data_loss: 1.089, reg_loss: 0.000), lr: 0.0007066139061616733\n",
      "step: 468, acc: 0.646, loss: 0.890 (data_loss: 0.890, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "training, acc: 0.554, loss: 1.096 (data_loss: 1.096, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "validation, acc: 0.788, loss: 0.620\n",
      "epoch: 10\n",
      "step: 0, acc: 0.594, loss: 0.961 (data_loss: 0.961, reg_loss: 0.000), lr: 0.0007031854299978904\n",
      "step: 100, acc: 0.508, loss: 1.095 (data_loss: 1.095, reg_loss: 0.000), lr: 0.0006982752601075343\n",
      "step: 200, acc: 0.625, loss: 1.074 (data_loss: 1.074, reg_loss: 0.000), lr: 0.000693433187712364\n",
      "step: 300, acc: 0.578, loss: 0.987 (data_loss: 0.987, reg_loss: 0.000), lr: 0.0006886578059362303\n",
      "step: 400, acc: 0.555, loss: 1.128 (data_loss: 1.128, reg_loss: 0.000), lr: 0.0006839477463921757\n",
      "step: 468, acc: 0.573, loss: 1.196 (data_loss: 1.196, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "training, acc: 0.562, loss: 1.089 (data_loss: 1.089, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "validation, acc: 0.786, loss: 0.626\n",
      "epoch: 1\n",
      "step: 0, acc: 0.102, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.586, loss: 1.119 (data_loss: 1.119, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 200, acc: 0.602, loss: 0.999 (data_loss: 0.999, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 300, acc: 0.680, loss: 0.891 (data_loss: 0.891, reg_loss: 0.000), lr: 0.0009708737864077671\n",
      "step: 400, acc: 0.727, loss: 0.890 (data_loss: 0.890, reg_loss: 0.000), lr: 0.0009615384615384615\n",
      "step: 468, acc: 0.542, loss: 1.092 (data_loss: 1.092, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "training, acc: 0.557, loss: 1.101 (data_loss: 1.101, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "validation, acc: 0.769, loss: 0.615\n",
      "epoch: 2\n",
      "step: 0, acc: 0.672, loss: 0.795 (data_loss: 0.795, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.641, loss: 0.839 (data_loss: 0.839, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 200, acc: 0.711, loss: 0.801 (data_loss: 0.801, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 300, acc: 0.703, loss: 0.726 (data_loss: 0.726, reg_loss: 0.000), lr: 0.0009285913269570063\n",
      "step: 400, acc: 0.672, loss: 0.849 (data_loss: 0.849, reg_loss: 0.000), lr: 0.0009200478424878093\n",
      "step: 468, acc: 0.677, loss: 1.016 (data_loss: 1.016, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "training, acc: 0.660, loss: 0.870 (data_loss: 0.870, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "validation, acc: 0.795, loss: 0.562\n",
      "epoch: 3\n",
      "step: 0, acc: 0.688, loss: 0.690 (data_loss: 0.690, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.703, loss: 0.932 (data_loss: 0.932, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 200, acc: 0.656, loss: 0.878 (data_loss: 0.878, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 300, acc: 0.766, loss: 0.685 (data_loss: 0.685, reg_loss: 0.000), lr: 0.0008898380494749957\n",
      "step: 400, acc: 0.656, loss: 0.795 (data_loss: 0.795, reg_loss: 0.000), lr: 0.0008819897689186807\n",
      "step: 468, acc: 0.667, loss: 1.061 (data_loss: 1.061, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "training, acc: 0.685, loss: 0.823 (data_loss: 0.823, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "validation, acc: 0.801, loss: 0.545\n",
      "epoch: 4\n",
      "step: 0, acc: 0.664, loss: 0.840 (data_loss: 0.840, reg_loss: 0.000), lr: 0.0008766546857192952\n",
      "step: 100, acc: 0.742, loss: 0.828 (data_loss: 0.828, reg_loss: 0.000), lr: 0.0008690362388111585\n",
      "step: 200, acc: 0.719, loss: 0.868 (data_loss: 0.868, reg_loss: 0.000), lr: 0.0008615490652192642\n",
      "step: 300, acc: 0.695, loss: 0.673 (data_loss: 0.673, reg_loss: 0.000), lr: 0.0008541898009737763\n",
      "step: 400, acc: 0.664, loss: 0.954 (data_loss: 0.954, reg_loss: 0.000), lr: 0.0008469551960701278\n",
      "step: 468, acc: 0.646, loss: 0.962 (data_loss: 0.962, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "training, acc: 0.693, loss: 0.803 (data_loss: 0.803, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "validation, acc: 0.811, loss: 0.525\n",
      "epoch: 5\n",
      "step: 0, acc: 0.680, loss: 0.780 (data_loss: 0.780, reg_loss: 0.000), lr: 0.0008420343550016842\n",
      "step: 100, acc: 0.672, loss: 0.892 (data_loss: 0.892, reg_loss: 0.000), lr: 0.0008350033400133601\n",
      "step: 200, acc: 0.703, loss: 0.803 (data_loss: 0.803, reg_loss: 0.000), lr: 0.0008280887711162637\n",
      "step: 300, acc: 0.750, loss: 0.634 (data_loss: 0.634, reg_loss: 0.000), lr: 0.0008212877792378449\n",
      "step: 400, acc: 0.680, loss: 0.813 (data_loss: 0.813, reg_loss: 0.000), lr: 0.0008145975887911372\n",
      "step: 468, acc: 0.667, loss: 0.969 (data_loss: 0.969, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "training, acc: 0.701, loss: 0.787 (data_loss: 0.787, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "validation, acc: 0.813, loss: 0.513\n",
      "epoch: 6\n",
      "step: 0, acc: 0.688, loss: 0.793 (data_loss: 0.793, reg_loss: 0.000), lr: 0.0008100445524503849\n",
      "step: 100, acc: 0.695, loss: 0.757 (data_loss: 0.757, reg_loss: 0.000), lr: 0.0008035355564483729\n",
      "step: 200, acc: 0.727, loss: 0.820 (data_loss: 0.820, reg_loss: 0.000), lr: 0.0007971303308090873\n",
      "step: 300, acc: 0.734, loss: 0.707 (data_loss: 0.707, reg_loss: 0.000), lr: 0.0007908264136022144\n",
      "step: 400, acc: 0.797, loss: 0.581 (data_loss: 0.581, reg_loss: 0.000), lr: 0.0007846214201647706\n",
      "step: 468, acc: 0.625, loss: 0.905 (data_loss: 0.905, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "training, acc: 0.708, loss: 0.772 (data_loss: 0.772, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "validation, acc: 0.817, loss: 0.513\n",
      "epoch: 7\n",
      "step: 0, acc: 0.680, loss: 0.791 (data_loss: 0.791, reg_loss: 0.000), lr: 0.0007803964413922272\n",
      "step: 100, acc: 0.672, loss: 0.920 (data_loss: 0.920, reg_loss: 0.000), lr: 0.0007743534148985598\n",
      "step: 200, acc: 0.711, loss: 0.739 (data_loss: 0.739, reg_loss: 0.000), lr: 0.0007684032580298141\n",
      "step: 300, acc: 0.750, loss: 0.706 (data_loss: 0.706, reg_loss: 0.000), lr: 0.0007625438462711606\n",
      "step: 400, acc: 0.789, loss: 0.635 (data_loss: 0.635, reg_loss: 0.000), lr: 0.0007567731194187983\n",
      "step: 468, acc: 0.677, loss: 0.932 (data_loss: 0.932, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "training, acc: 0.711, loss: 0.763 (data_loss: 0.763, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "validation, acc: 0.818, loss: 0.508\n",
      "epoch: 8\n",
      "step: 0, acc: 0.641, loss: 0.837 (data_loss: 0.837, reg_loss: 0.000), lr: 0.0007528419784687194\n",
      "step: 100, acc: 0.695, loss: 0.749 (data_loss: 0.749, reg_loss: 0.000), lr: 0.0007472166180975864\n",
      "step: 200, acc: 0.719, loss: 0.736 (data_loss: 0.736, reg_loss: 0.000), lr: 0.0007416747014759327\n",
      "step: 300, acc: 0.742, loss: 0.685 (data_loss: 0.685, reg_loss: 0.000), lr: 0.0007362143856290952\n",
      "step: 400, acc: 0.719, loss: 0.659 (data_loss: 0.659, reg_loss: 0.000), lr: 0.0007308338814587444\n",
      "step: 468, acc: 0.656, loss: 0.900 (data_loss: 0.900, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "training, acc: 0.712, loss: 0.765 (data_loss: 0.765, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "validation, acc: 0.819, loss: 0.500\n",
      "epoch: 9\n",
      "step: 0, acc: 0.750, loss: 0.716 (data_loss: 0.716, reg_loss: 0.000), lr: 0.0007271669575334498\n",
      "step: 100, acc: 0.734, loss: 0.891 (data_loss: 0.891, reg_loss: 0.000), lr: 0.000721917412647993\n",
      "step: 200, acc: 0.773, loss: 0.672 (data_loss: 0.672, reg_loss: 0.000), lr: 0.0007167431192660551\n",
      "step: 300, acc: 0.742, loss: 0.655 (data_loss: 0.655, reg_loss: 0.000), lr: 0.0007116424708226587\n",
      "step: 400, acc: 0.758, loss: 0.636 (data_loss: 0.636, reg_loss: 0.000), lr: 0.0007066139061616733\n",
      "step: 468, acc: 0.646, loss: 0.828 (data_loss: 0.828, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "training, acc: 0.714, loss: 0.749 (data_loss: 0.749, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "validation, acc: 0.826, loss: 0.487\n",
      "epoch: 10\n",
      "step: 0, acc: 0.719, loss: 0.663 (data_loss: 0.663, reg_loss: 0.000), lr: 0.0007031854299978904\n",
      "step: 100, acc: 0.703, loss: 0.713 (data_loss: 0.713, reg_loss: 0.000), lr: 0.0006982752601075343\n",
      "step: 200, acc: 0.750, loss: 0.594 (data_loss: 0.594, reg_loss: 0.000), lr: 0.000693433187712364\n",
      "step: 300, acc: 0.758, loss: 0.591 (data_loss: 0.591, reg_loss: 0.000), lr: 0.0006886578059362303\n",
      "step: 400, acc: 0.758, loss: 0.692 (data_loss: 0.692, reg_loss: 0.000), lr: 0.0006839477463921757\n",
      "step: 468, acc: 0.771, loss: 0.776 (data_loss: 0.776, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "training, acc: 0.718, loss: 0.747 (data_loss: 0.747, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "validation, acc: 0.823, loss: 0.495\n",
      "epoch: 1\n",
      "step: 0, acc: 0.102, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.719, loss: 0.968 (data_loss: 0.968, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 200, acc: 0.727, loss: 0.712 (data_loss: 0.712, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 300, acc: 0.797, loss: 0.582 (data_loss: 0.582, reg_loss: 0.000), lr: 0.0009708737864077671\n",
      "step: 400, acc: 0.734, loss: 0.737 (data_loss: 0.737, reg_loss: 0.000), lr: 0.0009615384615384615\n",
      "step: 468, acc: 0.688, loss: 0.864 (data_loss: 0.864, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "training, acc: 0.679, loss: 0.861 (data_loss: 0.861, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "validation, acc: 0.798, loss: 0.545\n",
      "epoch: 2\n",
      "step: 0, acc: 0.734, loss: 0.635 (data_loss: 0.635, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.742, loss: 0.814 (data_loss: 0.814, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 200, acc: 0.773, loss: 0.662 (data_loss: 0.662, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 300, acc: 0.766, loss: 0.553 (data_loss: 0.553, reg_loss: 0.000), lr: 0.0009285913269570063\n",
      "step: 400, acc: 0.805, loss: 0.765 (data_loss: 0.765, reg_loss: 0.000), lr: 0.0009200478424878093\n",
      "step: 468, acc: 0.802, loss: 0.659 (data_loss: 0.659, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "training, acc: 0.763, loss: 0.668 (data_loss: 0.668, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "validation, acc: 0.820, loss: 0.495\n",
      "epoch: 3\n",
      "step: 0, acc: 0.719, loss: 0.624 (data_loss: 0.624, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.766, loss: 0.766 (data_loss: 0.766, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 200, acc: 0.758, loss: 0.729 (data_loss: 0.729, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 300, acc: 0.781, loss: 0.485 (data_loss: 0.485, reg_loss: 0.000), lr: 0.0008898380494749957\n",
      "step: 400, acc: 0.797, loss: 0.653 (data_loss: 0.653, reg_loss: 0.000), lr: 0.0008819897689186807\n",
      "step: 468, acc: 0.750, loss: 0.784 (data_loss: 0.784, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "training, acc: 0.772, loss: 0.643 (data_loss: 0.643, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "validation, acc: 0.821, loss: 0.502\n",
      "epoch: 4\n",
      "step: 0, acc: 0.781, loss: 0.608 (data_loss: 0.608, reg_loss: 0.000), lr: 0.0008766546857192952\n",
      "step: 100, acc: 0.773, loss: 0.701 (data_loss: 0.701, reg_loss: 0.000), lr: 0.0008690362388111585\n",
      "step: 200, acc: 0.789, loss: 0.575 (data_loss: 0.575, reg_loss: 0.000), lr: 0.0008615490652192642\n",
      "step: 300, acc: 0.750, loss: 0.591 (data_loss: 0.591, reg_loss: 0.000), lr: 0.0008541898009737763\n",
      "step: 400, acc: 0.789, loss: 0.643 (data_loss: 0.643, reg_loss: 0.000), lr: 0.0008469551960701278\n",
      "step: 468, acc: 0.802, loss: 0.786 (data_loss: 0.786, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "training, acc: 0.780, loss: 0.618 (data_loss: 0.618, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "validation, acc: 0.832, loss: 0.478\n",
      "epoch: 5\n",
      "step: 0, acc: 0.742, loss: 0.623 (data_loss: 0.623, reg_loss: 0.000), lr: 0.0008420343550016842\n",
      "step: 100, acc: 0.758, loss: 0.799 (data_loss: 0.799, reg_loss: 0.000), lr: 0.0008350033400133601\n",
      "step: 200, acc: 0.828, loss: 0.603 (data_loss: 0.603, reg_loss: 0.000), lr: 0.0008280887711162637\n",
      "step: 300, acc: 0.797, loss: 0.513 (data_loss: 0.513, reg_loss: 0.000), lr: 0.0008212877792378449\n",
      "step: 400, acc: 0.781, loss: 0.621 (data_loss: 0.621, reg_loss: 0.000), lr: 0.0008145975887911372\n",
      "step: 468, acc: 0.729, loss: 0.702 (data_loss: 0.702, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "training, acc: 0.785, loss: 0.607 (data_loss: 0.607, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "validation, acc: 0.833, loss: 0.469\n",
      "epoch: 6\n",
      "step: 0, acc: 0.805, loss: 0.516 (data_loss: 0.516, reg_loss: 0.000), lr: 0.0008100445524503849\n",
      "step: 100, acc: 0.766, loss: 0.765 (data_loss: 0.765, reg_loss: 0.000), lr: 0.0008035355564483729\n",
      "step: 200, acc: 0.766, loss: 0.603 (data_loss: 0.603, reg_loss: 0.000), lr: 0.0007971303308090873\n",
      "step: 300, acc: 0.805, loss: 0.448 (data_loss: 0.448, reg_loss: 0.000), lr: 0.0007908264136022144\n",
      "step: 400, acc: 0.789, loss: 0.541 (data_loss: 0.541, reg_loss: 0.000), lr: 0.0007846214201647706\n",
      "step: 468, acc: 0.771, loss: 0.789 (data_loss: 0.789, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "training, acc: 0.787, loss: 0.601 (data_loss: 0.601, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "validation, acc: 0.832, loss: 0.468\n",
      "epoch: 7\n",
      "step: 0, acc: 0.789, loss: 0.572 (data_loss: 0.572, reg_loss: 0.000), lr: 0.0007803964413922272\n",
      "step: 100, acc: 0.812, loss: 0.569 (data_loss: 0.569, reg_loss: 0.000), lr: 0.0007743534148985598\n",
      "step: 200, acc: 0.766, loss: 0.584 (data_loss: 0.584, reg_loss: 0.000), lr: 0.0007684032580298141\n",
      "step: 300, acc: 0.828, loss: 0.452 (data_loss: 0.452, reg_loss: 0.000), lr: 0.0007625438462711606\n",
      "step: 400, acc: 0.812, loss: 0.541 (data_loss: 0.541, reg_loss: 0.000), lr: 0.0007567731194187983\n",
      "step: 468, acc: 0.781, loss: 0.661 (data_loss: 0.661, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "training, acc: 0.793, loss: 0.585 (data_loss: 0.585, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "validation, acc: 0.834, loss: 0.461\n",
      "epoch: 8\n",
      "step: 0, acc: 0.789, loss: 0.543 (data_loss: 0.543, reg_loss: 0.000), lr: 0.0007528419784687194\n",
      "step: 100, acc: 0.766, loss: 0.722 (data_loss: 0.722, reg_loss: 0.000), lr: 0.0007472166180975864\n",
      "step: 200, acc: 0.781, loss: 0.560 (data_loss: 0.560, reg_loss: 0.000), lr: 0.0007416747014759327\n",
      "step: 300, acc: 0.820, loss: 0.452 (data_loss: 0.452, reg_loss: 0.000), lr: 0.0007362143856290952\n",
      "step: 400, acc: 0.789, loss: 0.665 (data_loss: 0.665, reg_loss: 0.000), lr: 0.0007308338814587444\n",
      "step: 468, acc: 0.760, loss: 0.710 (data_loss: 0.710, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "training, acc: 0.793, loss: 0.582 (data_loss: 0.582, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "validation, acc: 0.839, loss: 0.457\n",
      "epoch: 9\n",
      "step: 0, acc: 0.758, loss: 0.660 (data_loss: 0.660, reg_loss: 0.000), lr: 0.0007271669575334498\n",
      "step: 100, acc: 0.797, loss: 0.636 (data_loss: 0.636, reg_loss: 0.000), lr: 0.000721917412647993\n",
      "step: 200, acc: 0.781, loss: 0.502 (data_loss: 0.502, reg_loss: 0.000), lr: 0.0007167431192660551\n",
      "step: 300, acc: 0.805, loss: 0.499 (data_loss: 0.499, reg_loss: 0.000), lr: 0.0007116424708226587\n",
      "step: 400, acc: 0.844, loss: 0.596 (data_loss: 0.596, reg_loss: 0.000), lr: 0.0007066139061616733\n",
      "step: 468, acc: 0.740, loss: 0.840 (data_loss: 0.840, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "training, acc: 0.795, loss: 0.575 (data_loss: 0.575, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "validation, acc: 0.836, loss: 0.464\n",
      "epoch: 10\n",
      "step: 0, acc: 0.766, loss: 0.535 (data_loss: 0.535, reg_loss: 0.000), lr: 0.0007031854299978904\n",
      "step: 100, acc: 0.836, loss: 0.566 (data_loss: 0.566, reg_loss: 0.000), lr: 0.0006982752601075343\n",
      "step: 200, acc: 0.820, loss: 0.521 (data_loss: 0.521, reg_loss: 0.000), lr: 0.000693433187712364\n",
      "step: 300, acc: 0.812, loss: 0.501 (data_loss: 0.501, reg_loss: 0.000), lr: 0.0006886578059362303\n",
      "step: 400, acc: 0.844, loss: 0.451 (data_loss: 0.451, reg_loss: 0.000), lr: 0.0006839477463921757\n",
      "step: 468, acc: 0.771, loss: 0.705 (data_loss: 0.705, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "training, acc: 0.798, loss: 0.568 (data_loss: 0.568, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "validation, acc: 0.832, loss: 0.468\n"
     ]
    }
   ],
   "source": [
    "# Train models with different nummber of neurons\n",
    "test_neurons = [32, 64, 128, 256]\n",
    "\n",
    "for neurons in test_neurons:\n",
    "    Train_Model(X=X, y=y, X_test=X_test, y_test=y_test, model_number=\"_neurons\"+str(neurons), number_of_neurons=neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with 32 neurons:\n",
      "validation, acc: 0.707, loss: 0.939\n",
      "Model with 64 neurons:\n",
      "validation, acc: 0.786, loss: 0.626\n",
      "Model with 128 neurons:\n",
      "validation, acc: 0.823, loss: 0.495\n",
      "Model with 256 neurons:\n",
      "validation, acc: 0.832, loss: 0.468\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models\n",
    "print(\"Model with 32 neurons:\")\n",
    "model = Model.load(\"fashion_mnist_neurons32.model\")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Model with 64 neurons:\")\n",
    "model = Model.load(\"fashion_mnist_neurons64.model\")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Model with 128 neurons:\")\n",
    "model = Model.load(\"fashion_mnist_neurons128.model\")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Model with 256 neurons:\")\n",
    "model = Model.load(\"fashion_mnist_neurons256.model\")\n",
    "model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0, acc: 0.062, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.562, loss: 1.416 (data_loss: 1.416, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 200, acc: 0.625, loss: 0.928 (data_loss: 0.928, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 300, acc: 0.500, loss: 1.245 (data_loss: 1.245, reg_loss: 0.000), lr: 0.0009708737864077671\n",
      "step: 400, acc: 0.625, loss: 0.873 (data_loss: 0.873, reg_loss: 0.000), lr: 0.0009615384615384615\n",
      "step: 500, acc: 0.562, loss: 0.852 (data_loss: 0.852, reg_loss: 0.000), lr: 0.0009523809523809524\n",
      "step: 600, acc: 0.312, loss: 1.434 (data_loss: 1.434, reg_loss: 0.000), lr: 0.0009433962264150942\n",
      "step: 700, acc: 0.375, loss: 1.244 (data_loss: 1.244, reg_loss: 0.000), lr: 0.0009345794392523365\n",
      "step: 800, acc: 0.438, loss: 2.078 (data_loss: 2.078, reg_loss: 0.000), lr: 0.0009259259259259259\n",
      "step: 900, acc: 0.625, loss: 0.941 (data_loss: 0.941, reg_loss: 0.000), lr: 0.0009174311926605504\n",
      "step: 1000, acc: 0.625, loss: 1.037 (data_loss: 1.037, reg_loss: 0.000), lr: 0.0009090909090909091\n",
      "step: 1100, acc: 0.625, loss: 0.993 (data_loss: 0.993, reg_loss: 0.000), lr: 0.0009009009009009008\n",
      "step: 1200, acc: 0.562, loss: 1.017 (data_loss: 1.017, reg_loss: 0.000), lr: 0.0008928571428571428\n",
      "step: 1300, acc: 0.750, loss: 0.702 (data_loss: 0.702, reg_loss: 0.000), lr: 0.0008849557522123895\n",
      "step: 1400, acc: 0.625, loss: 0.995 (data_loss: 0.995, reg_loss: 0.000), lr: 0.0008771929824561404\n",
      "step: 1500, acc: 0.375, loss: 1.590 (data_loss: 1.590, reg_loss: 0.000), lr: 0.0008695652173913045\n",
      "step: 1600, acc: 0.625, loss: 0.848 (data_loss: 0.848, reg_loss: 0.000), lr: 0.0008620689655172415\n",
      "step: 1700, acc: 0.625, loss: 0.927 (data_loss: 0.927, reg_loss: 0.000), lr: 0.0008547008547008548\n",
      "step: 1800, acc: 0.500, loss: 2.045 (data_loss: 2.045, reg_loss: 0.000), lr: 0.0008474576271186442\n",
      "step: 1900, acc: 0.625, loss: 0.736 (data_loss: 0.736, reg_loss: 0.000), lr: 0.0008403361344537816\n",
      "step: 2000, acc: 0.375, loss: 1.972 (data_loss: 1.972, reg_loss: 0.000), lr: 0.0008333333333333334\n",
      "step: 2100, acc: 0.625, loss: 1.032 (data_loss: 1.032, reg_loss: 0.000), lr: 0.0008264462809917356\n",
      "step: 2200, acc: 0.750, loss: 0.614 (data_loss: 0.614, reg_loss: 0.000), lr: 0.000819672131147541\n",
      "step: 2300, acc: 0.625, loss: 1.018 (data_loss: 1.018, reg_loss: 0.000), lr: 0.0008130081300813008\n",
      "step: 2400, acc: 0.562, loss: 0.784 (data_loss: 0.784, reg_loss: 0.000), lr: 0.0008064516129032259\n",
      "step: 2500, acc: 0.750, loss: 0.619 (data_loss: 0.619, reg_loss: 0.000), lr: 0.0008\n",
      "step: 2600, acc: 0.500, loss: 1.118 (data_loss: 1.118, reg_loss: 0.000), lr: 0.0007936507936507937\n",
      "step: 2700, acc: 0.625, loss: 1.361 (data_loss: 1.361, reg_loss: 0.000), lr: 0.0007874015748031496\n",
      "step: 2800, acc: 0.688, loss: 0.805 (data_loss: 0.805, reg_loss: 0.000), lr: 0.00078125\n",
      "step: 2900, acc: 0.812, loss: 0.656 (data_loss: 0.656, reg_loss: 0.000), lr: 0.0007751937984496124\n",
      "step: 3000, acc: 0.438, loss: 1.927 (data_loss: 1.927, reg_loss: 0.000), lr: 0.0007692307692307692\n",
      "step: 3100, acc: 0.875, loss: 0.531 (data_loss: 0.531, reg_loss: 0.000), lr: 0.0007633587786259542\n",
      "step: 3200, acc: 0.688, loss: 0.846 (data_loss: 0.846, reg_loss: 0.000), lr: 0.0007575757575757576\n",
      "step: 3300, acc: 0.688, loss: 0.850 (data_loss: 0.850, reg_loss: 0.000), lr: 0.0007518796992481202\n",
      "step: 3400, acc: 0.375, loss: 1.072 (data_loss: 1.072, reg_loss: 0.000), lr: 0.0007462686567164178\n",
      "step: 3500, acc: 0.750, loss: 0.705 (data_loss: 0.705, reg_loss: 0.000), lr: 0.0007407407407407407\n",
      "step: 3600, acc: 0.750, loss: 0.795 (data_loss: 0.795, reg_loss: 0.000), lr: 0.0007352941176470588\n",
      "step: 3700, acc: 0.562, loss: 0.976 (data_loss: 0.976, reg_loss: 0.000), lr: 0.0007299270072992701\n",
      "step: 3749, acc: 0.500, loss: 1.467 (data_loss: 1.467, reg_loss: 0.000), lr: 0.0007273256236817223\n",
      "training, acc: 0.594, loss: 1.047 (data_loss: 1.047, reg_loss: 0.000), lr: 0.0007273256236817223\n",
      "validation, acc: 0.782, loss: 0.578\n",
      "epoch: 2\n",
      "step: 0, acc: 0.688, loss: 0.587 (data_loss: 0.587, reg_loss: 0.000), lr: 0.0007272727272727273\n",
      "step: 100, acc: 0.750, loss: 0.739 (data_loss: 0.739, reg_loss: 0.000), lr: 0.0007220216606498196\n",
      "step: 200, acc: 0.688, loss: 0.776 (data_loss: 0.776, reg_loss: 0.000), lr: 0.0007168458781362007\n",
      "step: 300, acc: 0.688, loss: 0.838 (data_loss: 0.838, reg_loss: 0.000), lr: 0.0007117437722419929\n",
      "step: 400, acc: 0.625, loss: 0.965 (data_loss: 0.965, reg_loss: 0.000), lr: 0.000706713780918728\n",
      "step: 500, acc: 0.562, loss: 1.262 (data_loss: 1.262, reg_loss: 0.000), lr: 0.0007017543859649122\n",
      "step: 600, acc: 0.688, loss: 0.907 (data_loss: 0.907, reg_loss: 0.000), lr: 0.0006968641114982578\n",
      "step: 700, acc: 0.500, loss: 1.336 (data_loss: 1.336, reg_loss: 0.000), lr: 0.0006920415224913495\n",
      "step: 800, acc: 0.312, loss: 2.276 (data_loss: 2.276, reg_loss: 0.000), lr: 0.0006872852233676976\n",
      "step: 900, acc: 0.562, loss: 0.768 (data_loss: 0.768, reg_loss: 0.000), lr: 0.0006825938566552901\n",
      "step: 1000, acc: 0.375, loss: 1.270 (data_loss: 1.270, reg_loss: 0.000), lr: 0.0006779661016949152\n",
      "step: 1100, acc: 0.625, loss: 0.777 (data_loss: 0.777, reg_loss: 0.000), lr: 0.0006734006734006733\n",
      "step: 1200, acc: 0.562, loss: 0.794 (data_loss: 0.794, reg_loss: 0.000), lr: 0.0006688963210702341\n",
      "step: 1300, acc: 0.812, loss: 0.446 (data_loss: 0.446, reg_loss: 0.000), lr: 0.000664451827242525\n",
      "step: 1400, acc: 0.688, loss: 0.627 (data_loss: 0.627, reg_loss: 0.000), lr: 0.0006600660066006601\n",
      "step: 1500, acc: 0.625, loss: 1.466 (data_loss: 1.466, reg_loss: 0.000), lr: 0.0006557377049180329\n",
      "step: 1600, acc: 0.625, loss: 1.324 (data_loss: 1.324, reg_loss: 0.000), lr: 0.0006514657980456025\n",
      "step: 1700, acc: 0.562, loss: 1.079 (data_loss: 1.079, reg_loss: 0.000), lr: 0.0006472491909385114\n",
      "step: 1800, acc: 0.500, loss: 1.024 (data_loss: 1.024, reg_loss: 0.000), lr: 0.0006430868167202571\n",
      "step: 1900, acc: 0.562, loss: 1.153 (data_loss: 1.153, reg_loss: 0.000), lr: 0.0006389776357827476\n",
      "step: 2000, acc: 0.375, loss: 1.475 (data_loss: 1.475, reg_loss: 0.000), lr: 0.0006349206349206349\n",
      "step: 2100, acc: 0.500, loss: 1.001 (data_loss: 1.001, reg_loss: 0.000), lr: 0.0006309148264984227\n",
      "step: 2200, acc: 0.688, loss: 0.837 (data_loss: 0.837, reg_loss: 0.000), lr: 0.0006269592476489029\n",
      "step: 2300, acc: 0.625, loss: 1.260 (data_loss: 1.260, reg_loss: 0.000), lr: 0.0006230529595015575\n",
      "step: 2400, acc: 0.625, loss: 0.615 (data_loss: 0.615, reg_loss: 0.000), lr: 0.0006191950464396285\n",
      "step: 2500, acc: 0.625, loss: 0.845 (data_loss: 0.845, reg_loss: 0.000), lr: 0.0006153846153846154\n",
      "step: 2600, acc: 0.688, loss: 1.142 (data_loss: 1.142, reg_loss: 0.000), lr: 0.0006116207951070336\n",
      "step: 2700, acc: 0.562, loss: 1.099 (data_loss: 1.099, reg_loss: 0.000), lr: 0.00060790273556231\n",
      "step: 2800, acc: 0.688, loss: 1.755 (data_loss: 1.755, reg_loss: 0.000), lr: 0.0006042296072507553\n",
      "step: 2900, acc: 0.500, loss: 1.008 (data_loss: 1.008, reg_loss: 0.000), lr: 0.0006006006006006006\n",
      "step: 3000, acc: 0.562, loss: 1.139 (data_loss: 1.139, reg_loss: 0.000), lr: 0.0005970149253731343\n",
      "step: 3100, acc: 0.812, loss: 0.552 (data_loss: 0.552, reg_loss: 0.000), lr: 0.0005934718100890207\n",
      "step: 3200, acc: 0.875, loss: 0.517 (data_loss: 0.517, reg_loss: 0.000), lr: 0.0005899705014749262\n",
      "step: 3300, acc: 0.812, loss: 0.819 (data_loss: 0.819, reg_loss: 0.000), lr: 0.0005865102639296187\n",
      "step: 3400, acc: 0.438, loss: 0.988 (data_loss: 0.988, reg_loss: 0.000), lr: 0.0005830903790087462\n",
      "step: 3500, acc: 0.688, loss: 0.770 (data_loss: 0.770, reg_loss: 0.000), lr: 0.0005797101449275362\n",
      "step: 3600, acc: 0.938, loss: 0.469 (data_loss: 0.469, reg_loss: 0.000), lr: 0.0005763688760806917\n",
      "step: 3700, acc: 0.688, loss: 0.706 (data_loss: 0.706, reg_loss: 0.000), lr: 0.0005730659025787965\n",
      "step: 3749, acc: 0.625, loss: 1.565 (data_loss: 1.565, reg_loss: 0.000), lr: 0.0005714612263557917\n",
      "training, acc: 0.648, loss: 0.907 (data_loss: 0.907, reg_loss: 0.000), lr: 0.0005714612263557917\n",
      "validation, acc: 0.796, loss: 0.552\n",
      "epoch: 3\n",
      "step: 0, acc: 0.875, loss: 0.461 (data_loss: 0.461, reg_loss: 0.000), lr: 0.0005714285714285714\n",
      "step: 100, acc: 0.625, loss: 1.334 (data_loss: 1.334, reg_loss: 0.000), lr: 0.0005681818181818183\n",
      "step: 200, acc: 0.688, loss: 0.679 (data_loss: 0.679, reg_loss: 0.000), lr: 0.0005649717514124294\n",
      "step: 300, acc: 0.688, loss: 0.740 (data_loss: 0.740, reg_loss: 0.000), lr: 0.0005617977528089888\n",
      "step: 400, acc: 0.688, loss: 1.020 (data_loss: 1.020, reg_loss: 0.000), lr: 0.0005586592178770949\n",
      "step: 500, acc: 0.625, loss: 0.925 (data_loss: 0.925, reg_loss: 0.000), lr: 0.0005555555555555556\n",
      "step: 600, acc: 0.750, loss: 0.640 (data_loss: 0.640, reg_loss: 0.000), lr: 0.0005524861878453039\n",
      "step: 700, acc: 0.562, loss: 0.872 (data_loss: 0.872, reg_loss: 0.000), lr: 0.0005494505494505495\n",
      "step: 800, acc: 0.625, loss: 1.553 (data_loss: 1.553, reg_loss: 0.000), lr: 0.000546448087431694\n",
      "step: 900, acc: 0.875, loss: 0.484 (data_loss: 0.484, reg_loss: 0.000), lr: 0.0005434782608695652\n",
      "step: 1000, acc: 0.500, loss: 1.221 (data_loss: 1.221, reg_loss: 0.000), lr: 0.0005405405405405404\n",
      "step: 1100, acc: 0.500, loss: 1.083 (data_loss: 1.083, reg_loss: 0.000), lr: 0.0005376344086021506\n",
      "step: 1200, acc: 0.688, loss: 0.757 (data_loss: 0.757, reg_loss: 0.000), lr: 0.00053475935828877\n",
      "step: 1300, acc: 0.688, loss: 0.585 (data_loss: 0.585, reg_loss: 0.000), lr: 0.0005319148936170213\n",
      "step: 1400, acc: 0.688, loss: 0.793 (data_loss: 0.793, reg_loss: 0.000), lr: 0.0005291005291005291\n",
      "step: 1500, acc: 0.438, loss: 1.374 (data_loss: 1.374, reg_loss: 0.000), lr: 0.0005263157894736842\n",
      "step: 1600, acc: 0.750, loss: 0.736 (data_loss: 0.736, reg_loss: 0.000), lr: 0.0005235602094240838\n",
      "step: 1700, acc: 0.688, loss: 0.861 (data_loss: 0.861, reg_loss: 0.000), lr: 0.0005208333333333334\n",
      "step: 1800, acc: 0.500, loss: 2.353 (data_loss: 2.353, reg_loss: 0.000), lr: 0.0005181347150259067\n",
      "step: 1900, acc: 0.812, loss: 0.394 (data_loss: 0.394, reg_loss: 0.000), lr: 0.0005154639175257733\n",
      "step: 2000, acc: 0.375, loss: 1.540 (data_loss: 1.540, reg_loss: 0.000), lr: 0.0005128205128205128\n",
      "step: 2100, acc: 0.562, loss: 1.015 (data_loss: 1.015, reg_loss: 0.000), lr: 0.0005102040816326531\n",
      "step: 2200, acc: 0.812, loss: 0.577 (data_loss: 0.577, reg_loss: 0.000), lr: 0.0005076142131979696\n",
      "step: 2300, acc: 0.625, loss: 0.817 (data_loss: 0.817, reg_loss: 0.000), lr: 0.000505050505050505\n",
      "step: 2400, acc: 0.688, loss: 0.951 (data_loss: 0.951, reg_loss: 0.000), lr: 0.0005025125628140703\n",
      "step: 2500, acc: 0.688, loss: 0.549 (data_loss: 0.549, reg_loss: 0.000), lr: 0.0005\n",
      "step: 2600, acc: 0.688, loss: 0.828 (data_loss: 0.828, reg_loss: 0.000), lr: 0.0004975124378109454\n",
      "step: 2700, acc: 0.562, loss: 0.794 (data_loss: 0.794, reg_loss: 0.000), lr: 0.0004950495049504951\n",
      "step: 2800, acc: 0.750, loss: 1.145 (data_loss: 1.145, reg_loss: 0.000), lr: 0.0004926108374384236\n",
      "step: 2900, acc: 0.688, loss: 0.664 (data_loss: 0.664, reg_loss: 0.000), lr: 0.0004901960784313725\n",
      "step: 3000, acc: 0.562, loss: 0.866 (data_loss: 0.866, reg_loss: 0.000), lr: 0.00048780487804878054\n",
      "step: 3100, acc: 0.875, loss: 0.376 (data_loss: 0.376, reg_loss: 0.000), lr: 0.00048543689320388353\n",
      "step: 3200, acc: 0.812, loss: 0.569 (data_loss: 0.569, reg_loss: 0.000), lr: 0.00048309178743961346\n",
      "step: 3300, acc: 0.688, loss: 0.800 (data_loss: 0.800, reg_loss: 0.000), lr: 0.00048076923076923074\n",
      "step: 3400, acc: 0.625, loss: 0.847 (data_loss: 0.847, reg_loss: 0.000), lr: 0.00047846889952153117\n",
      "step: 3500, acc: 0.812, loss: 0.952 (data_loss: 0.952, reg_loss: 0.000), lr: 0.0004761904761904762\n",
      "step: 3600, acc: 0.750, loss: 0.802 (data_loss: 0.802, reg_loss: 0.000), lr: 0.0004739336492890995\n",
      "step: 3700, acc: 0.688, loss: 1.066 (data_loss: 1.066, reg_loss: 0.000), lr: 0.0004716981132075471\n",
      "step: 3749, acc: 0.500, loss: 1.575 (data_loss: 1.575, reg_loss: 0.000), lr: 0.0004706103816650195\n",
      "training, acc: 0.670, loss: 0.861 (data_loss: 0.861, reg_loss: 0.000), lr: 0.0004706103816650195\n",
      "validation, acc: 0.816, loss: 0.528\n",
      "epoch: 4\n",
      "step: 0, acc: 0.812, loss: 0.478 (data_loss: 0.478, reg_loss: 0.000), lr: 0.00047058823529411766\n",
      "step: 100, acc: 0.562, loss: 1.115 (data_loss: 1.115, reg_loss: 0.000), lr: 0.00046838407494145204\n",
      "step: 200, acc: 0.625, loss: 0.704 (data_loss: 0.704, reg_loss: 0.000), lr: 0.0004662004662004662\n",
      "step: 300, acc: 0.500, loss: 1.521 (data_loss: 1.521, reg_loss: 0.000), lr: 0.00046403712296983754\n",
      "step: 400, acc: 0.562, loss: 0.913 (data_loss: 0.913, reg_loss: 0.000), lr: 0.00046189376443418013\n",
      "step: 500, acc: 0.688, loss: 0.825 (data_loss: 0.825, reg_loss: 0.000), lr: 0.00045977011494252877\n",
      "step: 600, acc: 0.750, loss: 0.745 (data_loss: 0.745, reg_loss: 0.000), lr: 0.00045766590389016015\n",
      "step: 700, acc: 0.688, loss: 0.775 (data_loss: 0.775, reg_loss: 0.000), lr: 0.0004555808656036446\n",
      "step: 800, acc: 0.438, loss: 2.392 (data_loss: 2.392, reg_loss: 0.000), lr: 0.0004535147392290249\n",
      "step: 900, acc: 0.500, loss: 0.738 (data_loss: 0.738, reg_loss: 0.000), lr: 0.0004514672686230249\n",
      "step: 1000, acc: 0.562, loss: 1.934 (data_loss: 1.934, reg_loss: 0.000), lr: 0.00044943820224719103\n",
      "step: 1100, acc: 0.562, loss: 1.115 (data_loss: 1.115, reg_loss: 0.000), lr: 0.0004474272930648769\n",
      "step: 1200, acc: 0.438, loss: 1.042 (data_loss: 1.042, reg_loss: 0.000), lr: 0.0004454342984409799\n",
      "step: 1300, acc: 0.688, loss: 0.583 (data_loss: 0.583, reg_loss: 0.000), lr: 0.00044345898004434595\n",
      "step: 1400, acc: 0.688, loss: 0.785 (data_loss: 0.785, reg_loss: 0.000), lr: 0.0004415011037527594\n",
      "step: 1500, acc: 0.500, loss: 0.845 (data_loss: 0.845, reg_loss: 0.000), lr: 0.0004395604395604395\n",
      "step: 1600, acc: 0.688, loss: 0.780 (data_loss: 0.780, reg_loss: 0.000), lr: 0.000437636761487965\n",
      "step: 1700, acc: 0.875, loss: 0.466 (data_loss: 0.466, reg_loss: 0.000), lr: 0.0004357298474945534\n",
      "step: 1800, acc: 0.562, loss: 1.540 (data_loss: 1.540, reg_loss: 0.000), lr: 0.0004338394793926247\n",
      "step: 1900, acc: 0.812, loss: 0.904 (data_loss: 0.904, reg_loss: 0.000), lr: 0.0004319654427645788\n",
      "step: 2000, acc: 0.500, loss: 1.202 (data_loss: 1.202, reg_loss: 0.000), lr: 0.0004301075268817204\n",
      "step: 2100, acc: 0.562, loss: 0.895 (data_loss: 0.895, reg_loss: 0.000), lr: 0.0004282655246252677\n",
      "step: 2200, acc: 0.750, loss: 0.668 (data_loss: 0.668, reg_loss: 0.000), lr: 0.0004264392324093817\n",
      "step: 2300, acc: 0.688, loss: 0.874 (data_loss: 0.874, reg_loss: 0.000), lr: 0.0004246284501061571\n",
      "step: 2400, acc: 0.750, loss: 0.888 (data_loss: 0.888, reg_loss: 0.000), lr: 0.00042283298097251583\n",
      "step: 2500, acc: 0.875, loss: 0.419 (data_loss: 0.419, reg_loss: 0.000), lr: 0.00042105263157894734\n",
      "step: 2600, acc: 0.625, loss: 1.259 (data_loss: 1.259, reg_loss: 0.000), lr: 0.00041928721174004196\n",
      "step: 2700, acc: 0.688, loss: 1.159 (data_loss: 1.159, reg_loss: 0.000), lr: 0.0004175365344467641\n",
      "step: 2800, acc: 0.750, loss: 0.620 (data_loss: 0.620, reg_loss: 0.000), lr: 0.00041580041580041577\n",
      "step: 2900, acc: 0.812, loss: 0.711 (data_loss: 0.711, reg_loss: 0.000), lr: 0.00041407867494824016\n",
      "step: 3000, acc: 0.562, loss: 0.809 (data_loss: 0.809, reg_loss: 0.000), lr: 0.0004123711340206186\n",
      "step: 3100, acc: 0.812, loss: 0.417 (data_loss: 0.417, reg_loss: 0.000), lr: 0.0004106776180698152\n",
      "step: 3200, acc: 0.688, loss: 0.750 (data_loss: 0.750, reg_loss: 0.000), lr: 0.0004089979550102249\n",
      "step: 3300, acc: 0.625, loss: 0.760 (data_loss: 0.760, reg_loss: 0.000), lr: 0.00040733197556008143\n",
      "step: 3400, acc: 0.562, loss: 0.876 (data_loss: 0.876, reg_loss: 0.000), lr: 0.0004056795131845842\n",
      "step: 3500, acc: 0.562, loss: 0.885 (data_loss: 0.885, reg_loss: 0.000), lr: 0.00040404040404040404\n",
      "step: 3600, acc: 0.625, loss: 0.708 (data_loss: 0.708, reg_loss: 0.000), lr: 0.00040241448692152917\n",
      "step: 3700, acc: 0.500, loss: 1.398 (data_loss: 1.398, reg_loss: 0.000), lr: 0.00040080160320641277\n",
      "step: 3749, acc: 0.688, loss: 1.637 (data_loss: 1.637, reg_loss: 0.000), lr: 0.0004000160006400256\n",
      "training, acc: 0.685, loss: 0.824 (data_loss: 0.824, reg_loss: 0.000), lr: 0.0004000160006400256\n",
      "validation, acc: 0.816, loss: 0.514\n",
      "epoch: 5\n",
      "step: 0, acc: 0.750, loss: 0.569 (data_loss: 0.569, reg_loss: 0.000), lr: 0.0004\n",
      "step: 100, acc: 0.625, loss: 1.242 (data_loss: 1.242, reg_loss: 0.000), lr: 0.0003984063745019921\n",
      "step: 200, acc: 0.750, loss: 0.589 (data_loss: 0.589, reg_loss: 0.000), lr: 0.0003968253968253968\n",
      "step: 300, acc: 0.625, loss: 0.631 (data_loss: 0.631, reg_loss: 0.000), lr: 0.00039525691699604737\n",
      "step: 400, acc: 0.562, loss: 0.981 (data_loss: 0.981, reg_loss: 0.000), lr: 0.0003937007874015748\n",
      "step: 500, acc: 0.625, loss: 1.042 (data_loss: 1.042, reg_loss: 0.000), lr: 0.0003921568627450981\n",
      "step: 600, acc: 0.750, loss: 0.667 (data_loss: 0.667, reg_loss: 0.000), lr: 0.000390625\n",
      "step: 700, acc: 0.688, loss: 0.642 (data_loss: 0.642, reg_loss: 0.000), lr: 0.00038910505836575873\n",
      "step: 800, acc: 0.500, loss: 1.919 (data_loss: 1.919, reg_loss: 0.000), lr: 0.0003875968992248062\n",
      "step: 900, acc: 0.812, loss: 0.576 (data_loss: 0.576, reg_loss: 0.000), lr: 0.0003861003861003861\n",
      "step: 1000, acc: 0.375, loss: 1.586 (data_loss: 1.586, reg_loss: 0.000), lr: 0.0003846153846153846\n",
      "step: 1100, acc: 0.688, loss: 0.958 (data_loss: 0.958, reg_loss: 0.000), lr: 0.00038314176245210724\n",
      "step: 1200, acc: 0.500, loss: 1.081 (data_loss: 1.081, reg_loss: 0.000), lr: 0.0003816793893129771\n",
      "step: 1300, acc: 0.750, loss: 0.737 (data_loss: 0.737, reg_loss: 0.000), lr: 0.0003802281368821293\n",
      "step: 1400, acc: 0.625, loss: 0.768 (data_loss: 0.768, reg_loss: 0.000), lr: 0.0003787878787878788\n",
      "step: 1500, acc: 0.500, loss: 1.528 (data_loss: 1.528, reg_loss: 0.000), lr: 0.0003773584905660377\n",
      "step: 1600, acc: 0.625, loss: 0.671 (data_loss: 0.671, reg_loss: 0.000), lr: 0.0003759398496240601\n",
      "step: 1700, acc: 0.688, loss: 0.893 (data_loss: 0.893, reg_loss: 0.000), lr: 0.0003745318352059925\n",
      "step: 1800, acc: 0.438, loss: 1.194 (data_loss: 1.194, reg_loss: 0.000), lr: 0.0003731343283582089\n",
      "step: 1900, acc: 0.812, loss: 0.654 (data_loss: 0.654, reg_loss: 0.000), lr: 0.0003717472118959107\n",
      "step: 2000, acc: 0.625, loss: 0.790 (data_loss: 0.790, reg_loss: 0.000), lr: 0.00037037037037037035\n",
      "step: 2100, acc: 0.750, loss: 0.667 (data_loss: 0.667, reg_loss: 0.000), lr: 0.00036900369003690036\n",
      "step: 2200, acc: 0.875, loss: 0.445 (data_loss: 0.445, reg_loss: 0.000), lr: 0.00036764705882352946\n",
      "step: 2300, acc: 0.750, loss: 0.662 (data_loss: 0.662, reg_loss: 0.000), lr: 0.0003663003663003663\n",
      "step: 2400, acc: 0.812, loss: 0.531 (data_loss: 0.531, reg_loss: 0.000), lr: 0.00036496350364963507\n",
      "step: 2500, acc: 0.875, loss: 0.444 (data_loss: 0.444, reg_loss: 0.000), lr: 0.00036363636363636367\n",
      "step: 2600, acc: 0.688, loss: 0.771 (data_loss: 0.771, reg_loss: 0.000), lr: 0.0003623188405797102\n",
      "step: 2700, acc: 0.500, loss: 1.192 (data_loss: 1.192, reg_loss: 0.000), lr: 0.0003610108303249098\n",
      "step: 2800, acc: 0.812, loss: 1.157 (data_loss: 1.157, reg_loss: 0.000), lr: 0.00035971223021582735\n",
      "step: 2900, acc: 0.812, loss: 0.607 (data_loss: 0.607, reg_loss: 0.000), lr: 0.00035842293906810036\n",
      "step: 3000, acc: 0.812, loss: 0.605 (data_loss: 0.605, reg_loss: 0.000), lr: 0.00035714285714285714\n",
      "step: 3100, acc: 0.875, loss: 0.334 (data_loss: 0.334, reg_loss: 0.000), lr: 0.00035587188612099647\n",
      "step: 3200, acc: 0.875, loss: 0.476 (data_loss: 0.476, reg_loss: 0.000), lr: 0.00035460992907801415\n",
      "step: 3300, acc: 0.562, loss: 0.850 (data_loss: 0.850, reg_loss: 0.000), lr: 0.000353356890459364\n",
      "step: 3400, acc: 0.562, loss: 0.879 (data_loss: 0.879, reg_loss: 0.000), lr: 0.00035211267605633805\n",
      "step: 3500, acc: 0.750, loss: 0.718 (data_loss: 0.718, reg_loss: 0.000), lr: 0.0003508771929824561\n",
      "step: 3600, acc: 0.812, loss: 0.522 (data_loss: 0.522, reg_loss: 0.000), lr: 0.00034965034965034965\n",
      "step: 3700, acc: 0.688, loss: 0.728 (data_loss: 0.728, reg_loss: 0.000), lr: 0.0003484320557491289\n",
      "step: 3749, acc: 0.750, loss: 0.599 (data_loss: 0.599, reg_loss: 0.000), lr: 0.0003478381856760235\n",
      "training, acc: 0.689, loss: 0.809 (data_loss: 0.809, reg_loss: 0.000), lr: 0.0003478381856760235\n",
      "validation, acc: 0.821, loss: 0.512\n",
      "epoch: 6\n",
      "step: 0, acc: 0.750, loss: 0.560 (data_loss: 0.560, reg_loss: 0.000), lr: 0.00034782608695652176\n",
      "step: 100, acc: 0.500, loss: 1.655 (data_loss: 1.655, reg_loss: 0.000), lr: 0.0003466204506065858\n",
      "step: 200, acc: 0.625, loss: 0.795 (data_loss: 0.795, reg_loss: 0.000), lr: 0.0003454231433506045\n",
      "step: 300, acc: 0.688, loss: 0.812 (data_loss: 0.812, reg_loss: 0.000), lr: 0.0003442340791738382\n",
      "step: 400, acc: 0.625, loss: 0.806 (data_loss: 0.806, reg_loss: 0.000), lr: 0.00034305317324185246\n",
      "step: 500, acc: 0.562, loss: 0.786 (data_loss: 0.786, reg_loss: 0.000), lr: 0.0003418803418803419\n",
      "step: 600, acc: 0.750, loss: 0.727 (data_loss: 0.727, reg_loss: 0.000), lr: 0.00034071550255536625\n",
      "step: 700, acc: 0.562, loss: 1.064 (data_loss: 1.064, reg_loss: 0.000), lr: 0.00033955857385398983\n",
      "step: 800, acc: 0.438, loss: 1.414 (data_loss: 1.414, reg_loss: 0.000), lr: 0.00033840947546531303\n",
      "step: 900, acc: 0.812, loss: 0.564 (data_loss: 0.564, reg_loss: 0.000), lr: 0.0003372681281618887\n",
      "step: 1000, acc: 0.562, loss: 1.395 (data_loss: 1.395, reg_loss: 0.000), lr: 0.00033613445378151256\n",
      "step: 1100, acc: 0.688, loss: 0.698 (data_loss: 0.698, reg_loss: 0.000), lr: 0.0003350083752093802\n",
      "step: 1200, acc: 0.688, loss: 0.631 (data_loss: 0.631, reg_loss: 0.000), lr: 0.000333889816360601\n",
      "step: 1300, acc: 0.625, loss: 0.856 (data_loss: 0.856, reg_loss: 0.000), lr: 0.0003327787021630616\n",
      "step: 1400, acc: 0.688, loss: 0.597 (data_loss: 0.597, reg_loss: 0.000), lr: 0.0003316749585406302\n",
      "step: 1500, acc: 0.625, loss: 1.163 (data_loss: 1.163, reg_loss: 0.000), lr: 0.00033057851239669424\n",
      "step: 1600, acc: 0.625, loss: 0.763 (data_loss: 0.763, reg_loss: 0.000), lr: 0.00032948929159802305\n",
      "step: 1700, acc: 0.812, loss: 0.532 (data_loss: 0.532, reg_loss: 0.000), lr: 0.0003284072249589491\n",
      "step: 1800, acc: 0.562, loss: 2.027 (data_loss: 2.027, reg_loss: 0.000), lr: 0.0003273322422258592\n",
      "step: 1900, acc: 0.688, loss: 0.537 (data_loss: 0.537, reg_loss: 0.000), lr: 0.0003262642740619902\n",
      "step: 2000, acc: 0.312, loss: 1.183 (data_loss: 1.183, reg_loss: 0.000), lr: 0.0003252032520325203\n",
      "step: 2100, acc: 0.625, loss: 0.834 (data_loss: 0.834, reg_loss: 0.000), lr: 0.0003241491085899514\n",
      "step: 2200, acc: 0.750, loss: 0.587 (data_loss: 0.587, reg_loss: 0.000), lr: 0.00032310177705977385\n",
      "step: 2300, acc: 0.875, loss: 0.673 (data_loss: 0.673, reg_loss: 0.000), lr: 0.00032206119162640903\n",
      "step: 2400, acc: 0.625, loss: 0.755 (data_loss: 0.755, reg_loss: 0.000), lr: 0.00032102728731942215\n",
      "step: 2500, acc: 0.750, loss: 0.472 (data_loss: 0.472, reg_loss: 0.000), lr: 0.00032\n",
      "step: 2600, acc: 0.562, loss: 1.055 (data_loss: 1.055, reg_loss: 0.000), lr: 0.0003189792663476874\n",
      "step: 2700, acc: 0.625, loss: 0.778 (data_loss: 0.778, reg_loss: 0.000), lr: 0.0003179650238473768\n",
      "step: 2800, acc: 0.688, loss: 1.612 (data_loss: 1.612, reg_loss: 0.000), lr: 0.0003169572107765452\n",
      "step: 2900, acc: 0.812, loss: 0.658 (data_loss: 0.658, reg_loss: 0.000), lr: 0.00031595576619273305\n",
      "step: 3000, acc: 0.625, loss: 0.631 (data_loss: 0.631, reg_loss: 0.000), lr: 0.00031496062992125983\n",
      "step: 3100, acc: 0.688, loss: 1.092 (data_loss: 1.092, reg_loss: 0.000), lr: 0.0003139717425431711\n",
      "step: 3200, acc: 0.875, loss: 0.600 (data_loss: 0.600, reg_loss: 0.000), lr: 0.00031298904538341156\n",
      "step: 3300, acc: 0.688, loss: 0.852 (data_loss: 0.852, reg_loss: 0.000), lr: 0.00031201248049921997\n",
      "step: 3400, acc: 0.750, loss: 0.664 (data_loss: 0.664, reg_loss: 0.000), lr: 0.00031104199066874026\n",
      "step: 3500, acc: 0.812, loss: 0.686 (data_loss: 0.686, reg_loss: 0.000), lr: 0.000310077519379845\n",
      "step: 3600, acc: 0.812, loss: 0.501 (data_loss: 0.501, reg_loss: 0.000), lr: 0.0003091190108191654\n",
      "step: 3700, acc: 0.625, loss: 0.802 (data_loss: 0.802, reg_loss: 0.000), lr: 0.0003081664098613251\n",
      "step: 3749, acc: 0.562, loss: 1.199 (data_loss: 1.199, reg_loss: 0.000), lr: 0.00030770177543924424\n",
      "training, acc: 0.698, loss: 0.788 (data_loss: 0.788, reg_loss: 0.000), lr: 0.00030770177543924424\n",
      "validation, acc: 0.821, loss: 0.498\n",
      "epoch: 7\n",
      "step: 0, acc: 0.750, loss: 0.588 (data_loss: 0.588, reg_loss: 0.000), lr: 0.0003076923076923077\n",
      "step: 100, acc: 0.625, loss: 0.820 (data_loss: 0.820, reg_loss: 0.000), lr: 0.00030674846625766873\n",
      "step: 200, acc: 0.812, loss: 0.517 (data_loss: 0.517, reg_loss: 0.000), lr: 0.0003058103975535168\n",
      "step: 300, acc: 0.625, loss: 1.211 (data_loss: 1.211, reg_loss: 0.000), lr: 0.0003048780487804878\n",
      "step: 400, acc: 0.562, loss: 0.890 (data_loss: 0.890, reg_loss: 0.000), lr: 0.000303951367781155\n",
      "step: 500, acc: 0.625, loss: 0.903 (data_loss: 0.903, reg_loss: 0.000), lr: 0.000303030303030303\n",
      "step: 600, acc: 0.750, loss: 0.683 (data_loss: 0.683, reg_loss: 0.000), lr: 0.00030211480362537764\n",
      "step: 700, acc: 0.625, loss: 1.140 (data_loss: 1.140, reg_loss: 0.000), lr: 0.0003012048192771084\n",
      "step: 800, acc: 0.562, loss: 2.108 (data_loss: 2.108, reg_loss: 0.000), lr: 0.0003003003003003003\n",
      "step: 900, acc: 0.938, loss: 0.386 (data_loss: 0.386, reg_loss: 0.000), lr: 0.0002994011976047904\n",
      "step: 1000, acc: 0.750, loss: 0.962 (data_loss: 0.962, reg_loss: 0.000), lr: 0.00029850746268656717\n",
      "step: 1100, acc: 0.688, loss: 0.812 (data_loss: 0.812, reg_loss: 0.000), lr: 0.00029761904761904765\n",
      "step: 1200, acc: 0.750, loss: 0.769 (data_loss: 0.769, reg_loss: 0.000), lr: 0.0002967359050445104\n",
      "step: 1300, acc: 0.688, loss: 0.618 (data_loss: 0.618, reg_loss: 0.000), lr: 0.0002958579881656805\n",
      "step: 1400, acc: 0.875, loss: 0.612 (data_loss: 0.612, reg_loss: 0.000), lr: 0.0002949852507374631\n",
      "step: 1500, acc: 0.625, loss: 2.022 (data_loss: 2.022, reg_loss: 0.000), lr: 0.00029411764705882356\n",
      "step: 1600, acc: 0.750, loss: 0.694 (data_loss: 0.694, reg_loss: 0.000), lr: 0.00029325513196480933\n",
      "step: 1700, acc: 0.938, loss: 0.471 (data_loss: 0.471, reg_loss: 0.000), lr: 0.00029239766081871346\n",
      "step: 1800, acc: 0.312, loss: 1.449 (data_loss: 1.449, reg_loss: 0.000), lr: 0.0002915451895043731\n",
      "step: 1900, acc: 0.688, loss: 0.720 (data_loss: 0.720, reg_loss: 0.000), lr: 0.00029069767441860465\n",
      "step: 2000, acc: 0.375, loss: 1.482 (data_loss: 1.482, reg_loss: 0.000), lr: 0.0002898550724637681\n",
      "step: 2100, acc: 0.562, loss: 0.870 (data_loss: 0.870, reg_loss: 0.000), lr: 0.00028901734104046245\n",
      "step: 2200, acc: 0.938, loss: 0.429 (data_loss: 0.429, reg_loss: 0.000), lr: 0.00028818443804034583\n",
      "step: 2300, acc: 0.750, loss: 0.686 (data_loss: 0.686, reg_loss: 0.000), lr: 0.00028735632183908046\n",
      "step: 2400, acc: 0.750, loss: 0.679 (data_loss: 0.679, reg_loss: 0.000), lr: 0.00028653295128939826\n",
      "step: 2500, acc: 0.812, loss: 0.379 (data_loss: 0.379, reg_loss: 0.000), lr: 0.0002857142857142857\n",
      "step: 2600, acc: 0.812, loss: 0.902 (data_loss: 0.902, reg_loss: 0.000), lr: 0.00028490028490028494\n",
      "step: 2700, acc: 0.562, loss: 1.135 (data_loss: 1.135, reg_loss: 0.000), lr: 0.00028409090909090913\n",
      "step: 2800, acc: 0.562, loss: 1.868 (data_loss: 1.868, reg_loss: 0.000), lr: 0.00028328611898017\n",
      "step: 2900, acc: 0.625, loss: 0.752 (data_loss: 0.752, reg_loss: 0.000), lr: 0.0002824858757062147\n",
      "step: 3000, acc: 0.375, loss: 1.106 (data_loss: 1.106, reg_loss: 0.000), lr: 0.0002816901408450704\n",
      "step: 3100, acc: 0.750, loss: 0.668 (data_loss: 0.668, reg_loss: 0.000), lr: 0.0002808988764044944\n",
      "step: 3200, acc: 0.812, loss: 0.783 (data_loss: 0.783, reg_loss: 0.000), lr: 0.0002801120448179272\n",
      "step: 3300, acc: 0.625, loss: 0.717 (data_loss: 0.717, reg_loss: 0.000), lr: 0.00027932960893854746\n",
      "step: 3400, acc: 0.500, loss: 0.935 (data_loss: 0.935, reg_loss: 0.000), lr: 0.00027855153203342614\n",
      "step: 3500, acc: 0.688, loss: 0.647 (data_loss: 0.647, reg_loss: 0.000), lr: 0.0002777777777777778\n",
      "step: 3600, acc: 0.750, loss: 0.592 (data_loss: 0.592, reg_loss: 0.000), lr: 0.00027700831024930745\n",
      "step: 3700, acc: 0.688, loss: 0.682 (data_loss: 0.682, reg_loss: 0.000), lr: 0.00027624309392265195\n",
      "step: 3749, acc: 0.688, loss: 1.334 (data_loss: 1.334, reg_loss: 0.000), lr: 0.00027586967916356315\n",
      "training, acc: 0.701, loss: 0.775 (data_loss: 0.775, reg_loss: 0.000), lr: 0.00027586967916356315\n",
      "validation, acc: 0.825, loss: 0.498\n",
      "epoch: 8\n",
      "step: 0, acc: 0.938, loss: 0.323 (data_loss: 0.323, reg_loss: 0.000), lr: 0.00027586206896551725\n",
      "step: 100, acc: 0.688, loss: 1.030 (data_loss: 1.030, reg_loss: 0.000), lr: 0.0002751031636863824\n",
      "step: 200, acc: 0.750, loss: 0.616 (data_loss: 0.616, reg_loss: 0.000), lr: 0.00027434842249657066\n",
      "step: 300, acc: 0.750, loss: 0.623 (data_loss: 0.623, reg_loss: 0.000), lr: 0.00027359781121751026\n",
      "step: 400, acc: 0.625, loss: 1.256 (data_loss: 1.256, reg_loss: 0.000), lr: 0.00027285129604365623\n",
      "step: 500, acc: 0.562, loss: 0.915 (data_loss: 0.915, reg_loss: 0.000), lr: 0.00027210884353741496\n",
      "step: 600, acc: 0.812, loss: 0.607 (data_loss: 0.607, reg_loss: 0.000), lr: 0.00027137042062415194\n",
      "step: 700, acc: 0.562, loss: 1.031 (data_loss: 1.031, reg_loss: 0.000), lr: 0.00027063599458728013\n",
      "step: 800, acc: 0.500, loss: 1.520 (data_loss: 1.520, reg_loss: 0.000), lr: 0.0002699055330634278\n",
      "step: 900, acc: 0.812, loss: 0.512 (data_loss: 0.512, reg_loss: 0.000), lr: 0.00026917900403768504\n",
      "step: 1000, acc: 0.750, loss: 0.804 (data_loss: 0.804, reg_loss: 0.000), lr: 0.0002684563758389262\n",
      "step: 1100, acc: 0.812, loss: 0.657 (data_loss: 0.657, reg_loss: 0.000), lr: 0.0002677376171352075\n",
      "step: 1200, acc: 0.750, loss: 0.540 (data_loss: 0.540, reg_loss: 0.000), lr: 0.000267022696929239\n",
      "step: 1300, acc: 0.750, loss: 0.528 (data_loss: 0.528, reg_loss: 0.000), lr: 0.0002663115845539281\n",
      "step: 1400, acc: 0.625, loss: 0.755 (data_loss: 0.755, reg_loss: 0.000), lr: 0.0002656042496679947\n",
      "step: 1500, acc: 0.438, loss: 1.316 (data_loss: 1.316, reg_loss: 0.000), lr: 0.00026490066225165563\n",
      "step: 1600, acc: 0.938, loss: 0.453 (data_loss: 0.453, reg_loss: 0.000), lr: 0.0002642007926023778\n",
      "step: 1700, acc: 0.688, loss: 0.696 (data_loss: 0.696, reg_loss: 0.000), lr: 0.0002635046113306983\n",
      "step: 1800, acc: 0.562, loss: 1.057 (data_loss: 1.057, reg_loss: 0.000), lr: 0.0002628120893561104\n",
      "step: 1900, acc: 0.812, loss: 0.603 (data_loss: 0.603, reg_loss: 0.000), lr: 0.0002621231979030144\n",
      "step: 2000, acc: 0.438, loss: 1.126 (data_loss: 1.126, reg_loss: 0.000), lr: 0.00026143790849673205\n",
      "step: 2100, acc: 0.688, loss: 0.670 (data_loss: 0.670, reg_loss: 0.000), lr: 0.0002607561929595828\n",
      "step: 2200, acc: 0.812, loss: 0.520 (data_loss: 0.520, reg_loss: 0.000), lr: 0.0002600780234070221\n",
      "step: 2300, acc: 0.750, loss: 0.727 (data_loss: 0.727, reg_loss: 0.000), lr: 0.0002594033722438392\n",
      "step: 2400, acc: 0.812, loss: 0.808 (data_loss: 0.808, reg_loss: 0.000), lr: 0.000258732212160414\n",
      "step: 2500, acc: 0.750, loss: 0.889 (data_loss: 0.889, reg_loss: 0.000), lr: 0.00025806451612903227\n",
      "step: 2600, acc: 0.562, loss: 1.327 (data_loss: 1.327, reg_loss: 0.000), lr: 0.00025740025740025744\n",
      "step: 2700, acc: 0.875, loss: 0.712 (data_loss: 0.712, reg_loss: 0.000), lr: 0.00025673940949935817\n",
      "step: 2800, acc: 0.812, loss: 1.249 (data_loss: 1.249, reg_loss: 0.000), lr: 0.00025608194622279127\n",
      "step: 2900, acc: 0.812, loss: 0.448 (data_loss: 0.448, reg_loss: 0.000), lr: 0.0002554278416347382\n",
      "step: 3000, acc: 0.688, loss: 0.779 (data_loss: 0.779, reg_loss: 0.000), lr: 0.0002547770700636943\n",
      "step: 3100, acc: 0.812, loss: 0.587 (data_loss: 0.587, reg_loss: 0.000), lr: 0.0002541296060991106\n",
      "step: 3200, acc: 0.812, loss: 0.627 (data_loss: 0.627, reg_loss: 0.000), lr: 0.00025348542458808617\n",
      "step: 3300, acc: 0.688, loss: 0.602 (data_loss: 0.602, reg_loss: 0.000), lr: 0.00025284450063211124\n",
      "step: 3400, acc: 0.562, loss: 1.100 (data_loss: 1.100, reg_loss: 0.000), lr: 0.00025220680958385876\n",
      "step: 3500, acc: 0.750, loss: 0.674 (data_loss: 0.674, reg_loss: 0.000), lr: 0.00025157232704402514\n",
      "step: 3600, acc: 0.812, loss: 0.599 (data_loss: 0.599, reg_loss: 0.000), lr: 0.0002509410288582183\n",
      "step: 3700, acc: 0.625, loss: 0.770 (data_loss: 0.770, reg_loss: 0.000), lr: 0.0002503128911138924\n",
      "step: 3749, acc: 0.812, loss: 1.058 (data_loss: 1.058, reg_loss: 0.000), lr: 0.0002500062501562539\n",
      "training, acc: 0.706, loss: 0.764 (data_loss: 0.764, reg_loss: 0.000), lr: 0.0002500062501562539\n",
      "validation, acc: 0.825, loss: 0.497\n",
      "epoch: 9\n",
      "step: 0, acc: 0.750, loss: 0.439 (data_loss: 0.439, reg_loss: 0.000), lr: 0.00025\n",
      "step: 100, acc: 0.688, loss: 1.133 (data_loss: 1.133, reg_loss: 0.000), lr: 0.0002493765586034913\n",
      "step: 200, acc: 0.625, loss: 0.790 (data_loss: 0.790, reg_loss: 0.000), lr: 0.0002487562189054727\n",
      "step: 300, acc: 0.750, loss: 0.608 (data_loss: 0.608, reg_loss: 0.000), lr: 0.00024813895781637717\n",
      "step: 400, acc: 0.750, loss: 0.639 (data_loss: 0.639, reg_loss: 0.000), lr: 0.00024752475247524753\n",
      "step: 500, acc: 0.812, loss: 0.523 (data_loss: 0.523, reg_loss: 0.000), lr: 0.00024691358024691353\n",
      "step: 600, acc: 0.625, loss: 1.067 (data_loss: 1.067, reg_loss: 0.000), lr: 0.0002463054187192118\n",
      "step: 700, acc: 0.688, loss: 0.760 (data_loss: 0.760, reg_loss: 0.000), lr: 0.0002457002457002457\n",
      "step: 800, acc: 0.562, loss: 1.446 (data_loss: 1.446, reg_loss: 0.000), lr: 0.00024509803921568627\n",
      "step: 900, acc: 0.938, loss: 0.455 (data_loss: 0.455, reg_loss: 0.000), lr: 0.0002444987775061125\n",
      "step: 1000, acc: 0.750, loss: 0.935 (data_loss: 0.935, reg_loss: 0.000), lr: 0.00024390243902439027\n",
      "step: 1100, acc: 0.688, loss: 0.959 (data_loss: 0.959, reg_loss: 0.000), lr: 0.00024330900243309\n",
      "step: 1200, acc: 0.562, loss: 0.822 (data_loss: 0.822, reg_loss: 0.000), lr: 0.00024271844660194176\n",
      "step: 1300, acc: 0.688, loss: 0.469 (data_loss: 0.469, reg_loss: 0.000), lr: 0.00024213075060532682\n",
      "step: 1400, acc: 0.562, loss: 0.826 (data_loss: 0.826, reg_loss: 0.000), lr: 0.00024154589371980673\n",
      "step: 1500, acc: 0.688, loss: 0.679 (data_loss: 0.679, reg_loss: 0.000), lr: 0.00024096385542168674\n",
      "step: 1600, acc: 0.625, loss: 0.864 (data_loss: 0.864, reg_loss: 0.000), lr: 0.00024038461538461537\n",
      "step: 1700, acc: 0.750, loss: 0.597 (data_loss: 0.597, reg_loss: 0.000), lr: 0.00023980815347721823\n",
      "step: 1800, acc: 0.375, loss: 1.039 (data_loss: 1.039, reg_loss: 0.000), lr: 0.00023923444976076558\n",
      "step: 1900, acc: 0.875, loss: 0.433 (data_loss: 0.433, reg_loss: 0.000), lr: 0.00023866348448687357\n",
      "step: 2000, acc: 0.625, loss: 0.874 (data_loss: 0.874, reg_loss: 0.000), lr: 0.0002380952380952381\n",
      "step: 2100, acc: 0.688, loss: 0.891 (data_loss: 0.891, reg_loss: 0.000), lr: 0.00023752969121140145\n",
      "step: 2200, acc: 0.812, loss: 0.456 (data_loss: 0.456, reg_loss: 0.000), lr: 0.00023696682464454974\n",
      "step: 2300, acc: 0.625, loss: 0.641 (data_loss: 0.641, reg_loss: 0.000), lr: 0.00023640661938534278\n",
      "step: 2400, acc: 0.625, loss: 1.012 (data_loss: 1.012, reg_loss: 0.000), lr: 0.00023584905660377356\n",
      "step: 2500, acc: 0.812, loss: 0.384 (data_loss: 0.384, reg_loss: 0.000), lr: 0.00023529411764705883\n",
      "step: 2600, acc: 0.562, loss: 0.904 (data_loss: 0.904, reg_loss: 0.000), lr: 0.00023474178403755868\n",
      "step: 2700, acc: 0.625, loss: 1.108 (data_loss: 1.108, reg_loss: 0.000), lr: 0.00023419203747072602\n",
      "step: 2800, acc: 0.688, loss: 0.610 (data_loss: 0.610, reg_loss: 0.000), lr: 0.00023364485981308412\n",
      "step: 2900, acc: 0.812, loss: 0.614 (data_loss: 0.614, reg_loss: 0.000), lr: 0.0002331002331002331\n",
      "step: 3000, acc: 0.688, loss: 0.701 (data_loss: 0.701, reg_loss: 0.000), lr: 0.0002325581395348837\n",
      "step: 3100, acc: 0.750, loss: 0.755 (data_loss: 0.755, reg_loss: 0.000), lr: 0.00023201856148491877\n",
      "step: 3200, acc: 0.812, loss: 0.461 (data_loss: 0.461, reg_loss: 0.000), lr: 0.00023148148148148146\n",
      "step: 3300, acc: 0.625, loss: 0.898 (data_loss: 0.898, reg_loss: 0.000), lr: 0.00023094688221709007\n",
      "step: 3400, acc: 0.562, loss: 0.769 (data_loss: 0.769, reg_loss: 0.000), lr: 0.0002304147465437788\n",
      "step: 3500, acc: 0.688, loss: 0.805 (data_loss: 0.805, reg_loss: 0.000), lr: 0.00022988505747126439\n",
      "step: 3600, acc: 0.688, loss: 0.532 (data_loss: 0.532, reg_loss: 0.000), lr: 0.0002293577981651376\n",
      "step: 3700, acc: 0.625, loss: 1.235 (data_loss: 1.235, reg_loss: 0.000), lr: 0.00022883295194508008\n",
      "step: 3749, acc: 0.688, loss: 1.518 (data_loss: 1.518, reg_loss: 0.000), lr: 0.00022857665318064412\n",
      "training, acc: 0.709, loss: 0.762 (data_loss: 0.762, reg_loss: 0.000), lr: 0.00022857665318064412\n",
      "validation, acc: 0.826, loss: 0.495\n",
      "epoch: 10\n",
      "step: 0, acc: 0.812, loss: 0.435 (data_loss: 0.435, reg_loss: 0.000), lr: 0.00022857142857142857\n",
      "step: 100, acc: 0.812, loss: 0.923 (data_loss: 0.923, reg_loss: 0.000), lr: 0.0002280501710376283\n",
      "step: 200, acc: 0.812, loss: 0.512 (data_loss: 0.512, reg_loss: 0.000), lr: 0.0002275312855517634\n",
      "step: 300, acc: 0.688, loss: 0.965 (data_loss: 0.965, reg_loss: 0.000), lr: 0.00022701475595913735\n",
      "step: 400, acc: 0.688, loss: 0.768 (data_loss: 0.768, reg_loss: 0.000), lr: 0.00022650056625141563\n",
      "step: 500, acc: 0.688, loss: 0.934 (data_loss: 0.934, reg_loss: 0.000), lr: 0.00022598870056497172\n",
      "step: 600, acc: 0.812, loss: 0.720 (data_loss: 0.720, reg_loss: 0.000), lr: 0.0002254791431792559\n",
      "step: 700, acc: 0.812, loss: 0.623 (data_loss: 0.623, reg_loss: 0.000), lr: 0.0002249718785151856\n",
      "step: 800, acc: 0.500, loss: 1.175 (data_loss: 1.175, reg_loss: 0.000), lr: 0.0002244668911335578\n",
      "step: 900, acc: 0.750, loss: 0.482 (data_loss: 0.482, reg_loss: 0.000), lr: 0.00022396416573348266\n",
      "step: 1000, acc: 0.375, loss: 1.260 (data_loss: 1.260, reg_loss: 0.000), lr: 0.00022346368715083802\n",
      "step: 1100, acc: 0.625, loss: 0.679 (data_loss: 0.679, reg_loss: 0.000), lr: 0.00022296544035674468\n",
      "step: 1200, acc: 0.562, loss: 0.859 (data_loss: 0.859, reg_loss: 0.000), lr: 0.00022246941045606227\n",
      "step: 1300, acc: 0.750, loss: 0.642 (data_loss: 0.642, reg_loss: 0.000), lr: 0.0002219755826859045\n",
      "step: 1400, acc: 0.562, loss: 0.890 (data_loss: 0.890, reg_loss: 0.000), lr: 0.00022148394241417493\n",
      "step: 1500, acc: 0.688, loss: 1.091 (data_loss: 1.091, reg_loss: 0.000), lr: 0.00022099447513812155\n",
      "step: 1600, acc: 0.750, loss: 0.632 (data_loss: 0.632, reg_loss: 0.000), lr: 0.0002205071664829107\n",
      "step: 1700, acc: 0.875, loss: 0.482 (data_loss: 0.482, reg_loss: 0.000), lr: 0.00022002200220022004\n",
      "step: 1800, acc: 0.500, loss: 1.910 (data_loss: 1.910, reg_loss: 0.000), lr: 0.0002195389681668496\n",
      "step: 1900, acc: 0.750, loss: 0.632 (data_loss: 0.632, reg_loss: 0.000), lr: 0.00021905805038335157\n",
      "step: 2000, acc: 0.562, loss: 0.919 (data_loss: 0.919, reg_loss: 0.000), lr: 0.0002185792349726776\n",
      "step: 2100, acc: 0.688, loss: 0.877 (data_loss: 0.877, reg_loss: 0.000), lr: 0.00021810250817884405\n",
      "step: 2200, acc: 0.938, loss: 0.282 (data_loss: 0.282, reg_loss: 0.000), lr: 0.00021762785636561478\n",
      "step: 2300, acc: 0.625, loss: 0.886 (data_loss: 0.886, reg_loss: 0.000), lr: 0.00021715526601520085\n",
      "step: 2400, acc: 0.750, loss: 0.583 (data_loss: 0.583, reg_loss: 0.000), lr: 0.00021668472372697725\n",
      "step: 2500, acc: 0.812, loss: 0.323 (data_loss: 0.323, reg_loss: 0.000), lr: 0.00021621621621621624\n",
      "step: 2600, acc: 0.812, loss: 0.589 (data_loss: 0.589, reg_loss: 0.000), lr: 0.0002157497303128371\n",
      "step: 2700, acc: 0.688, loss: 1.007 (data_loss: 1.007, reg_loss: 0.000), lr: 0.00021528525296017227\n",
      "step: 2800, acc: 0.750, loss: 1.654 (data_loss: 1.654, reg_loss: 0.000), lr: 0.00021482277121374866\n",
      "step: 2900, acc: 0.938, loss: 0.506 (data_loss: 0.506, reg_loss: 0.000), lr: 0.00021436227224008576\n",
      "step: 3000, acc: 0.688, loss: 0.575 (data_loss: 0.575, reg_loss: 0.000), lr: 0.00021390374331550798\n",
      "step: 3100, acc: 0.875, loss: 0.379 (data_loss: 0.379, reg_loss: 0.000), lr: 0.0002134471718249733\n",
      "step: 3200, acc: 0.875, loss: 0.461 (data_loss: 0.461, reg_loss: 0.000), lr: 0.00021299254526091586\n",
      "step: 3300, acc: 0.688, loss: 0.649 (data_loss: 0.649, reg_loss: 0.000), lr: 0.00021253985122210415\n",
      "step: 3400, acc: 0.625, loss: 0.668 (data_loss: 0.668, reg_loss: 0.000), lr: 0.00021208907741251327\n",
      "step: 3500, acc: 0.750, loss: 0.799 (data_loss: 0.799, reg_loss: 0.000), lr: 0.00021164021164021165\n",
      "step: 3600, acc: 0.688, loss: 0.548 (data_loss: 0.548, reg_loss: 0.000), lr: 0.00021119324181626187\n",
      "step: 3700, acc: 0.625, loss: 0.580 (data_loss: 0.580, reg_loss: 0.000), lr: 0.00021074815595363542\n",
      "step: 3749, acc: 0.625, loss: 1.253 (data_loss: 1.253, reg_loss: 0.000), lr: 0.0002105307480157477\n",
      "training, acc: 0.712, loss: 0.752 (data_loss: 0.752, reg_loss: 0.000), lr: 0.0002105307480157477\n",
      "validation, acc: 0.827, loss: 0.486\n",
      "epoch: 1\n",
      "step: 0, acc: 0.156, loss: 2.302 (data_loss: 2.302, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.438, loss: 1.387 (data_loss: 1.387, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 200, acc: 0.531, loss: 1.429 (data_loss: 1.429, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 300, acc: 0.656, loss: 1.303 (data_loss: 1.303, reg_loss: 0.000), lr: 0.0009708737864077671\n",
      "step: 400, acc: 0.562, loss: 1.566 (data_loss: 1.566, reg_loss: 0.000), lr: 0.0009615384615384615\n",
      "step: 500, acc: 0.719, loss: 0.940 (data_loss: 0.940, reg_loss: 0.000), lr: 0.0009523809523809524\n",
      "step: 600, acc: 0.406, loss: 1.005 (data_loss: 1.005, reg_loss: 0.000), lr: 0.0009433962264150942\n",
      "step: 700, acc: 0.750, loss: 0.765 (data_loss: 0.765, reg_loss: 0.000), lr: 0.0009345794392523365\n",
      "step: 800, acc: 0.562, loss: 1.001 (data_loss: 1.001, reg_loss: 0.000), lr: 0.0009259259259259259\n",
      "step: 900, acc: 0.531, loss: 1.480 (data_loss: 1.480, reg_loss: 0.000), lr: 0.0009174311926605504\n",
      "step: 1000, acc: 0.562, loss: 1.027 (data_loss: 1.027, reg_loss: 0.000), lr: 0.0009090909090909091\n",
      "step: 1100, acc: 0.656, loss: 0.782 (data_loss: 0.782, reg_loss: 0.000), lr: 0.0009009009009009008\n",
      "step: 1200, acc: 0.625, loss: 1.174 (data_loss: 1.174, reg_loss: 0.000), lr: 0.0008928571428571428\n",
      "step: 1300, acc: 0.625, loss: 1.077 (data_loss: 1.077, reg_loss: 0.000), lr: 0.0008849557522123895\n",
      "step: 1400, acc: 0.656, loss: 1.272 (data_loss: 1.272, reg_loss: 0.000), lr: 0.0008771929824561404\n",
      "step: 1500, acc: 0.500, loss: 1.108 (data_loss: 1.108, reg_loss: 0.000), lr: 0.0008695652173913045\n",
      "step: 1600, acc: 0.594, loss: 0.941 (data_loss: 0.941, reg_loss: 0.000), lr: 0.0008620689655172415\n",
      "step: 1700, acc: 0.812, loss: 0.655 (data_loss: 0.655, reg_loss: 0.000), lr: 0.0008547008547008548\n",
      "step: 1800, acc: 0.562, loss: 0.987 (data_loss: 0.987, reg_loss: 0.000), lr: 0.0008474576271186442\n",
      "step: 1874, acc: 0.719, loss: 0.941 (data_loss: 0.941, reg_loss: 0.000), lr: 0.0008421761832575375\n",
      "training, acc: 0.600, loss: 1.028 (data_loss: 1.028, reg_loss: 0.000), lr: 0.0008421761832575375\n",
      "validation, acc: 0.780, loss: 0.580\n",
      "epoch: 2\n",
      "step: 0, acc: 0.656, loss: 0.832 (data_loss: 0.832, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "step: 100, acc: 0.625, loss: 0.740 (data_loss: 0.740, reg_loss: 0.000), lr: 0.0008350730688935282\n",
      "step: 200, acc: 0.594, loss: 1.184 (data_loss: 1.184, reg_loss: 0.000), lr: 0.0008281573498964803\n",
      "step: 300, acc: 0.688, loss: 1.044 (data_loss: 1.044, reg_loss: 0.000), lr: 0.0008213552361396304\n",
      "step: 400, acc: 0.719, loss: 0.790 (data_loss: 0.790, reg_loss: 0.000), lr: 0.0008146639511201629\n",
      "step: 500, acc: 0.594, loss: 1.136 (data_loss: 1.136, reg_loss: 0.000), lr: 0.0008080808080808081\n",
      "step: 600, acc: 0.781, loss: 0.705 (data_loss: 0.705, reg_loss: 0.000), lr: 0.0008016032064128255\n",
      "step: 700, acc: 0.688, loss: 0.758 (data_loss: 0.758, reg_loss: 0.000), lr: 0.0007952286282306162\n",
      "step: 800, acc: 0.688, loss: 0.820 (data_loss: 0.820, reg_loss: 0.000), lr: 0.0007889546351084812\n",
      "step: 900, acc: 0.531, loss: 1.248 (data_loss: 1.248, reg_loss: 0.000), lr: 0.0007827788649706458\n",
      "step: 1000, acc: 0.594, loss: 0.941 (data_loss: 0.941, reg_loss: 0.000), lr: 0.0007766990291262136\n",
      "step: 1100, acc: 0.625, loss: 1.004 (data_loss: 1.004, reg_loss: 0.000), lr: 0.0007707129094412332\n",
      "step: 1200, acc: 0.594, loss: 0.903 (data_loss: 0.903, reg_loss: 0.000), lr: 0.0007648183556405354\n",
      "step: 1300, acc: 0.594, loss: 0.890 (data_loss: 0.890, reg_loss: 0.000), lr: 0.0007590132827324478\n",
      "step: 1400, acc: 0.625, loss: 0.950 (data_loss: 0.950, reg_loss: 0.000), lr: 0.0007532956685499058\n",
      "step: 1500, acc: 0.688, loss: 0.793 (data_loss: 0.793, reg_loss: 0.000), lr: 0.0007476635514018692\n",
      "step: 1600, acc: 0.750, loss: 0.751 (data_loss: 0.751, reg_loss: 0.000), lr: 0.0007421150278293135\n",
      "step: 1700, acc: 0.688, loss: 0.806 (data_loss: 0.806, reg_loss: 0.000), lr: 0.0007366482504604051\n",
      "step: 1800, acc: 0.719, loss: 0.737 (data_loss: 0.737, reg_loss: 0.000), lr: 0.0007312614259597807\n",
      "step: 1874, acc: 0.719, loss: 0.993 (data_loss: 0.993, reg_loss: 0.000), lr: 0.0007273256236817223\n",
      "training, acc: 0.659, loss: 0.886 (data_loss: 0.886, reg_loss: 0.000), lr: 0.0007273256236817223\n",
      "validation, acc: 0.797, loss: 0.551\n",
      "epoch: 3\n",
      "step: 0, acc: 0.781, loss: 0.707 (data_loss: 0.707, reg_loss: 0.000), lr: 0.0007272727272727273\n",
      "step: 100, acc: 0.750, loss: 0.894 (data_loss: 0.894, reg_loss: 0.000), lr: 0.0007220216606498196\n",
      "step: 200, acc: 0.656, loss: 0.945 (data_loss: 0.945, reg_loss: 0.000), lr: 0.0007168458781362007\n",
      "step: 300, acc: 0.719, loss: 0.889 (data_loss: 0.889, reg_loss: 0.000), lr: 0.0007117437722419929\n",
      "step: 400, acc: 0.719, loss: 1.240 (data_loss: 1.240, reg_loss: 0.000), lr: 0.000706713780918728\n",
      "step: 500, acc: 0.656, loss: 1.081 (data_loss: 1.081, reg_loss: 0.000), lr: 0.0007017543859649122\n",
      "step: 600, acc: 0.719, loss: 0.641 (data_loss: 0.641, reg_loss: 0.000), lr: 0.0006968641114982578\n",
      "step: 700, acc: 0.688, loss: 0.778 (data_loss: 0.778, reg_loss: 0.000), lr: 0.0006920415224913495\n",
      "step: 800, acc: 0.719, loss: 0.695 (data_loss: 0.695, reg_loss: 0.000), lr: 0.0006872852233676976\n",
      "step: 900, acc: 0.625, loss: 0.990 (data_loss: 0.990, reg_loss: 0.000), lr: 0.0006825938566552901\n",
      "step: 1000, acc: 0.750, loss: 0.715 (data_loss: 0.715, reg_loss: 0.000), lr: 0.0006779661016949152\n",
      "step: 1100, acc: 0.781, loss: 0.558 (data_loss: 0.558, reg_loss: 0.000), lr: 0.0006734006734006733\n",
      "step: 1200, acc: 0.688, loss: 0.799 (data_loss: 0.799, reg_loss: 0.000), lr: 0.0006688963210702341\n",
      "step: 1300, acc: 0.719, loss: 1.140 (data_loss: 1.140, reg_loss: 0.000), lr: 0.000664451827242525\n",
      "step: 1400, acc: 0.656, loss: 1.002 (data_loss: 1.002, reg_loss: 0.000), lr: 0.0006600660066006601\n",
      "step: 1500, acc: 0.750, loss: 0.749 (data_loss: 0.749, reg_loss: 0.000), lr: 0.0006557377049180329\n",
      "step: 1600, acc: 0.719, loss: 0.823 (data_loss: 0.823, reg_loss: 0.000), lr: 0.0006514657980456025\n",
      "step: 1700, acc: 0.625, loss: 0.819 (data_loss: 0.819, reg_loss: 0.000), lr: 0.0006472491909385114\n",
      "step: 1800, acc: 0.688, loss: 0.835 (data_loss: 0.835, reg_loss: 0.000), lr: 0.0006430868167202571\n",
      "step: 1874, acc: 0.688, loss: 1.125 (data_loss: 1.125, reg_loss: 0.000), lr: 0.0006400409626216078\n",
      "training, acc: 0.681, loss: 0.838 (data_loss: 0.838, reg_loss: 0.000), lr: 0.0006400409626216078\n",
      "validation, acc: 0.802, loss: 0.538\n",
      "epoch: 4\n",
      "step: 0, acc: 0.656, loss: 0.709 (data_loss: 0.709, reg_loss: 0.000), lr: 0.00064\n",
      "step: 100, acc: 0.656, loss: 0.715 (data_loss: 0.715, reg_loss: 0.000), lr: 0.0006359300476947536\n",
      "step: 200, acc: 0.594, loss: 1.085 (data_loss: 1.085, reg_loss: 0.000), lr: 0.0006319115323854661\n",
      "step: 300, acc: 0.750, loss: 0.964 (data_loss: 0.964, reg_loss: 0.000), lr: 0.0006279434850863422\n",
      "step: 400, acc: 0.594, loss: 1.295 (data_loss: 1.295, reg_loss: 0.000), lr: 0.0006240249609984399\n",
      "step: 500, acc: 0.594, loss: 1.065 (data_loss: 1.065, reg_loss: 0.000), lr: 0.00062015503875969\n",
      "step: 600, acc: 0.656, loss: 0.691 (data_loss: 0.691, reg_loss: 0.000), lr: 0.0006163328197226503\n",
      "step: 700, acc: 0.750, loss: 0.578 (data_loss: 0.578, reg_loss: 0.000), lr: 0.0006125574272588055\n",
      "step: 800, acc: 0.844, loss: 0.712 (data_loss: 0.712, reg_loss: 0.000), lr: 0.00060882800608828\n",
      "step: 900, acc: 0.531, loss: 1.185 (data_loss: 1.185, reg_loss: 0.000), lr: 0.000605143721633888\n",
      "step: 1000, acc: 0.656, loss: 0.831 (data_loss: 0.831, reg_loss: 0.000), lr: 0.0006015037593984962\n",
      "step: 1100, acc: 0.781, loss: 0.647 (data_loss: 0.647, reg_loss: 0.000), lr: 0.0005979073243647235\n",
      "step: 1200, acc: 0.688, loss: 0.753 (data_loss: 0.753, reg_loss: 0.000), lr: 0.0005943536404160476\n",
      "step: 1300, acc: 0.812, loss: 0.798 (data_loss: 0.798, reg_loss: 0.000), lr: 0.0005908419497784343\n",
      "step: 1400, acc: 0.656, loss: 1.123 (data_loss: 1.123, reg_loss: 0.000), lr: 0.0005873715124816445\n",
      "step: 1500, acc: 0.750, loss: 0.733 (data_loss: 0.733, reg_loss: 0.000), lr: 0.0005839416058394161\n",
      "step: 1600, acc: 0.719, loss: 0.588 (data_loss: 0.588, reg_loss: 0.000), lr: 0.0005805515239477503\n",
      "step: 1700, acc: 0.719, loss: 0.663 (data_loss: 0.663, reg_loss: 0.000), lr: 0.0005772005772005773\n",
      "step: 1800, acc: 0.781, loss: 0.664 (data_loss: 0.664, reg_loss: 0.000), lr: 0.0005738880918220947\n",
      "step: 1874, acc: 0.688, loss: 0.774 (data_loss: 0.774, reg_loss: 0.000), lr: 0.0005714612263557917\n",
      "training, acc: 0.692, loss: 0.816 (data_loss: 0.816, reg_loss: 0.000), lr: 0.0005714612263557917\n",
      "validation, acc: 0.812, loss: 0.522\n",
      "epoch: 5\n",
      "step: 0, acc: 0.688, loss: 0.685 (data_loss: 0.685, reg_loss: 0.000), lr: 0.0005714285714285714\n",
      "step: 100, acc: 0.625, loss: 0.901 (data_loss: 0.901, reg_loss: 0.000), lr: 0.0005681818181818183\n",
      "step: 200, acc: 0.594, loss: 0.994 (data_loss: 0.994, reg_loss: 0.000), lr: 0.0005649717514124294\n",
      "step: 300, acc: 0.656, loss: 0.747 (data_loss: 0.747, reg_loss: 0.000), lr: 0.0005617977528089888\n",
      "step: 400, acc: 0.531, loss: 1.039 (data_loss: 1.039, reg_loss: 0.000), lr: 0.0005586592178770949\n",
      "step: 500, acc: 0.656, loss: 0.945 (data_loss: 0.945, reg_loss: 0.000), lr: 0.0005555555555555556\n",
      "step: 600, acc: 0.656, loss: 0.875 (data_loss: 0.875, reg_loss: 0.000), lr: 0.0005524861878453039\n",
      "step: 700, acc: 0.656, loss: 0.689 (data_loss: 0.689, reg_loss: 0.000), lr: 0.0005494505494505495\n",
      "step: 800, acc: 0.594, loss: 0.924 (data_loss: 0.924, reg_loss: 0.000), lr: 0.000546448087431694\n",
      "step: 900, acc: 0.594, loss: 1.277 (data_loss: 1.277, reg_loss: 0.000), lr: 0.0005434782608695652\n",
      "step: 1000, acc: 0.594, loss: 0.798 (data_loss: 0.798, reg_loss: 0.000), lr: 0.0005405405405405404\n",
      "step: 1100, acc: 0.875, loss: 0.483 (data_loss: 0.483, reg_loss: 0.000), lr: 0.0005376344086021506\n",
      "step: 1200, acc: 0.750, loss: 0.695 (data_loss: 0.695, reg_loss: 0.000), lr: 0.00053475935828877\n",
      "step: 1300, acc: 0.656, loss: 1.459 (data_loss: 1.459, reg_loss: 0.000), lr: 0.0005319148936170213\n",
      "step: 1400, acc: 0.688, loss: 0.773 (data_loss: 0.773, reg_loss: 0.000), lr: 0.0005291005291005291\n",
      "step: 1500, acc: 0.719, loss: 0.608 (data_loss: 0.608, reg_loss: 0.000), lr: 0.0005263157894736842\n",
      "step: 1600, acc: 0.781, loss: 0.543 (data_loss: 0.543, reg_loss: 0.000), lr: 0.0005235602094240838\n",
      "step: 1700, acc: 0.688, loss: 0.858 (data_loss: 0.858, reg_loss: 0.000), lr: 0.0005208333333333334\n",
      "step: 1800, acc: 0.688, loss: 0.714 (data_loss: 0.714, reg_loss: 0.000), lr: 0.0005181347150259067\n",
      "step: 1874, acc: 0.719, loss: 0.739 (data_loss: 0.739, reg_loss: 0.000), lr: 0.0005161556725508414\n",
      "training, acc: 0.695, loss: 0.801 (data_loss: 0.801, reg_loss: 0.000), lr: 0.0005161556725508414\n",
      "validation, acc: 0.816, loss: 0.504\n",
      "epoch: 6\n",
      "step: 0, acc: 0.750, loss: 0.659 (data_loss: 0.659, reg_loss: 0.000), lr: 0.0005161290322580645\n",
      "step: 100, acc: 0.719, loss: 0.553 (data_loss: 0.553, reg_loss: 0.000), lr: 0.0005134788189987163\n",
      "step: 200, acc: 0.625, loss: 1.131 (data_loss: 1.131, reg_loss: 0.000), lr: 0.0005108556832694764\n",
      "step: 300, acc: 0.656, loss: 0.739 (data_loss: 0.739, reg_loss: 0.000), lr: 0.0005082592121982212\n",
      "step: 400, acc: 0.688, loss: 1.001 (data_loss: 1.001, reg_loss: 0.000), lr: 0.0005056890012642225\n",
      "step: 500, acc: 0.719, loss: 0.820 (data_loss: 0.820, reg_loss: 0.000), lr: 0.0005031446540880503\n",
      "step: 600, acc: 0.688, loss: 0.748 (data_loss: 0.748, reg_loss: 0.000), lr: 0.0005006257822277848\n",
      "step: 700, acc: 0.719, loss: 0.741 (data_loss: 0.741, reg_loss: 0.000), lr: 0.00049813200498132\n",
      "step: 800, acc: 0.781, loss: 0.772 (data_loss: 0.772, reg_loss: 0.000), lr: 0.0004956629491945477\n",
      "step: 900, acc: 0.594, loss: 1.310 (data_loss: 1.310, reg_loss: 0.000), lr: 0.0004932182490752159\n",
      "step: 1000, acc: 0.688, loss: 0.742 (data_loss: 0.742, reg_loss: 0.000), lr: 0.00049079754601227\n",
      "step: 1100, acc: 0.625, loss: 0.762 (data_loss: 0.762, reg_loss: 0.000), lr: 0.0004884004884004883\n",
      "step: 1200, acc: 0.656, loss: 0.727 (data_loss: 0.727, reg_loss: 0.000), lr: 0.0004860267314702309\n",
      "step: 1300, acc: 0.719, loss: 0.858 (data_loss: 0.858, reg_loss: 0.000), lr: 0.0004836759371221282\n",
      "step: 1400, acc: 0.656, loss: 1.137 (data_loss: 1.137, reg_loss: 0.000), lr: 0.0004813477737665463\n",
      "step: 1500, acc: 0.750, loss: 0.702 (data_loss: 0.702, reg_loss: 0.000), lr: 0.0004790419161676646\n",
      "step: 1600, acc: 0.688, loss: 0.818 (data_loss: 0.818, reg_loss: 0.000), lr: 0.00047675804529201426\n",
      "step: 1700, acc: 0.719, loss: 0.798 (data_loss: 0.798, reg_loss: 0.000), lr: 0.0004744958481613286\n",
      "step: 1800, acc: 0.812, loss: 0.578 (data_loss: 0.578, reg_loss: 0.000), lr: 0.00047225501770956313\n",
      "step: 1874, acc: 0.625, loss: 1.056 (data_loss: 1.056, reg_loss: 0.000), lr: 0.0004706103816650195\n",
      "training, acc: 0.701, loss: 0.786 (data_loss: 0.786, reg_loss: 0.000), lr: 0.0004706103816650195\n",
      "validation, acc: 0.820, loss: 0.500\n",
      "epoch: 7\n",
      "step: 0, acc: 0.812, loss: 0.557 (data_loss: 0.557, reg_loss: 0.000), lr: 0.00047058823529411766\n",
      "step: 100, acc: 0.781, loss: 0.585 (data_loss: 0.585, reg_loss: 0.000), lr: 0.00046838407494145204\n",
      "step: 200, acc: 0.781, loss: 0.955 (data_loss: 0.955, reg_loss: 0.000), lr: 0.0004662004662004662\n",
      "step: 300, acc: 0.844, loss: 0.704 (data_loss: 0.704, reg_loss: 0.000), lr: 0.00046403712296983754\n",
      "step: 400, acc: 0.719, loss: 0.798 (data_loss: 0.798, reg_loss: 0.000), lr: 0.00046189376443418013\n",
      "step: 500, acc: 0.688, loss: 0.827 (data_loss: 0.827, reg_loss: 0.000), lr: 0.00045977011494252877\n",
      "step: 600, acc: 0.719, loss: 0.731 (data_loss: 0.731, reg_loss: 0.000), lr: 0.00045766590389016015\n",
      "step: 700, acc: 0.781, loss: 0.693 (data_loss: 0.693, reg_loss: 0.000), lr: 0.0004555808656036446\n",
      "step: 800, acc: 0.562, loss: 0.890 (data_loss: 0.890, reg_loss: 0.000), lr: 0.0004535147392290249\n",
      "step: 900, acc: 0.594, loss: 0.961 (data_loss: 0.961, reg_loss: 0.000), lr: 0.0004514672686230249\n",
      "step: 1000, acc: 0.625, loss: 0.890 (data_loss: 0.890, reg_loss: 0.000), lr: 0.00044943820224719103\n",
      "step: 1100, acc: 0.719, loss: 0.582 (data_loss: 0.582, reg_loss: 0.000), lr: 0.0004474272930648769\n",
      "step: 1200, acc: 0.656, loss: 0.795 (data_loss: 0.795, reg_loss: 0.000), lr: 0.0004454342984409799\n",
      "step: 1300, acc: 0.719, loss: 1.234 (data_loss: 1.234, reg_loss: 0.000), lr: 0.00044345898004434595\n",
      "step: 1400, acc: 0.719, loss: 1.071 (data_loss: 1.071, reg_loss: 0.000), lr: 0.0004415011037527594\n",
      "step: 1500, acc: 0.688, loss: 0.787 (data_loss: 0.787, reg_loss: 0.000), lr: 0.0004395604395604395\n",
      "step: 1600, acc: 0.688, loss: 0.763 (data_loss: 0.763, reg_loss: 0.000), lr: 0.000437636761487965\n",
      "step: 1700, acc: 0.688, loss: 0.700 (data_loss: 0.700, reg_loss: 0.000), lr: 0.0004357298474945534\n",
      "step: 1800, acc: 0.688, loss: 0.748 (data_loss: 0.748, reg_loss: 0.000), lr: 0.0004338394793926247\n",
      "step: 1874, acc: 0.625, loss: 1.267 (data_loss: 1.267, reg_loss: 0.000), lr: 0.0004324511330219685\n",
      "training, acc: 0.706, loss: 0.774 (data_loss: 0.774, reg_loss: 0.000), lr: 0.0004324511330219685\n",
      "validation, acc: 0.817, loss: 0.501\n",
      "epoch: 8\n",
      "step: 0, acc: 0.781, loss: 0.603 (data_loss: 0.603, reg_loss: 0.000), lr: 0.0004324324324324325\n",
      "step: 100, acc: 0.750, loss: 0.594 (data_loss: 0.594, reg_loss: 0.000), lr: 0.00043057050592034454\n",
      "step: 200, acc: 0.750, loss: 0.788 (data_loss: 0.788, reg_loss: 0.000), lr: 0.0004287245444801715\n",
      "step: 300, acc: 0.750, loss: 0.665 (data_loss: 0.665, reg_loss: 0.000), lr: 0.0004268943436499466\n",
      "step: 400, acc: 0.625, loss: 1.190 (data_loss: 1.190, reg_loss: 0.000), lr: 0.0004250797024442083\n",
      "step: 500, acc: 0.625, loss: 0.897 (data_loss: 0.897, reg_loss: 0.000), lr: 0.0004232804232804233\n",
      "step: 600, acc: 0.750, loss: 0.744 (data_loss: 0.744, reg_loss: 0.000), lr: 0.00042149631190727084\n",
      "step: 700, acc: 0.844, loss: 0.595 (data_loss: 0.595, reg_loss: 0.000), lr: 0.00041972717733473235\n",
      "step: 800, acc: 0.781, loss: 0.694 (data_loss: 0.694, reg_loss: 0.000), lr: 0.0004179728317659352\n",
      "step: 900, acc: 0.562, loss: 1.120 (data_loss: 1.120, reg_loss: 0.000), lr: 0.00041623309053069726\n",
      "step: 1000, acc: 0.656, loss: 0.860 (data_loss: 0.860, reg_loss: 0.000), lr: 0.0004145077720207254\n",
      "step: 1100, acc: 0.812, loss: 0.705 (data_loss: 0.705, reg_loss: 0.000), lr: 0.0004127966976264189\n",
      "step: 1200, acc: 0.625, loss: 0.663 (data_loss: 0.663, reg_loss: 0.000), lr: 0.00041109969167523125\n",
      "step: 1300, acc: 0.562, loss: 1.282 (data_loss: 1.282, reg_loss: 0.000), lr: 0.0004094165813715456\n",
      "step: 1400, acc: 0.844, loss: 0.704 (data_loss: 0.704, reg_loss: 0.000), lr: 0.0004077471967380224\n",
      "step: 1500, acc: 0.625, loss: 0.663 (data_loss: 0.663, reg_loss: 0.000), lr: 0.00040609137055837557\n",
      "step: 1600, acc: 0.906, loss: 0.359 (data_loss: 0.359, reg_loss: 0.000), lr: 0.0004044489383215369\n",
      "step: 1700, acc: 0.719, loss: 0.664 (data_loss: 0.664, reg_loss: 0.000), lr: 0.0004028197381671702\n",
      "step: 1800, acc: 0.656, loss: 0.762 (data_loss: 0.762, reg_loss: 0.000), lr: 0.00040120361083249747\n",
      "step: 1874, acc: 0.688, loss: 0.856 (data_loss: 0.856, reg_loss: 0.000), lr: 0.0004000160006400256\n",
      "training, acc: 0.710, loss: 0.761 (data_loss: 0.761, reg_loss: 0.000), lr: 0.0004000160006400256\n",
      "validation, acc: 0.819, loss: 0.491\n",
      "epoch: 9\n",
      "step: 0, acc: 0.750, loss: 0.606 (data_loss: 0.606, reg_loss: 0.000), lr: 0.0004\n",
      "step: 100, acc: 0.781, loss: 0.943 (data_loss: 0.943, reg_loss: 0.000), lr: 0.0003984063745019921\n",
      "step: 200, acc: 0.594, loss: 0.853 (data_loss: 0.853, reg_loss: 0.000), lr: 0.0003968253968253968\n",
      "step: 300, acc: 0.812, loss: 0.695 (data_loss: 0.695, reg_loss: 0.000), lr: 0.00039525691699604737\n",
      "step: 400, acc: 0.719, loss: 0.879 (data_loss: 0.879, reg_loss: 0.000), lr: 0.0003937007874015748\n",
      "step: 500, acc: 0.688, loss: 0.766 (data_loss: 0.766, reg_loss: 0.000), lr: 0.0003921568627450981\n",
      "step: 600, acc: 0.781, loss: 0.751 (data_loss: 0.751, reg_loss: 0.000), lr: 0.000390625\n",
      "step: 700, acc: 0.719, loss: 0.693 (data_loss: 0.693, reg_loss: 0.000), lr: 0.00038910505836575873\n",
      "step: 800, acc: 0.750, loss: 0.677 (data_loss: 0.677, reg_loss: 0.000), lr: 0.0003875968992248062\n",
      "step: 900, acc: 0.531, loss: 1.413 (data_loss: 1.413, reg_loss: 0.000), lr: 0.0003861003861003861\n",
      "step: 1000, acc: 0.750, loss: 0.795 (data_loss: 0.795, reg_loss: 0.000), lr: 0.0003846153846153846\n",
      "step: 1100, acc: 0.625, loss: 0.631 (data_loss: 0.631, reg_loss: 0.000), lr: 0.00038314176245210724\n",
      "step: 1200, acc: 0.562, loss: 0.924 (data_loss: 0.924, reg_loss: 0.000), lr: 0.0003816793893129771\n",
      "step: 1300, acc: 0.719, loss: 1.337 (data_loss: 1.337, reg_loss: 0.000), lr: 0.0003802281368821293\n",
      "step: 1400, acc: 0.812, loss: 0.735 (data_loss: 0.735, reg_loss: 0.000), lr: 0.0003787878787878788\n",
      "step: 1500, acc: 0.688, loss: 0.727 (data_loss: 0.727, reg_loss: 0.000), lr: 0.0003773584905660377\n",
      "step: 1600, acc: 0.781, loss: 0.431 (data_loss: 0.431, reg_loss: 0.000), lr: 0.0003759398496240601\n",
      "step: 1700, acc: 0.750, loss: 0.621 (data_loss: 0.621, reg_loss: 0.000), lr: 0.0003745318352059925\n",
      "step: 1800, acc: 0.719, loss: 0.800 (data_loss: 0.800, reg_loss: 0.000), lr: 0.0003731343283582089\n",
      "step: 1874, acc: 0.719, loss: 0.594 (data_loss: 0.594, reg_loss: 0.000), lr: 0.00037210686909280345\n",
      "training, acc: 0.717, loss: 0.751 (data_loss: 0.751, reg_loss: 0.000), lr: 0.00037210686909280345\n",
      "validation, acc: 0.821, loss: 0.494\n",
      "epoch: 10\n",
      "step: 0, acc: 0.656, loss: 0.785 (data_loss: 0.785, reg_loss: 0.000), lr: 0.00037209302325581393\n",
      "step: 100, acc: 0.656, loss: 0.767 (data_loss: 0.767, reg_loss: 0.000), lr: 0.00037071362372567197\n",
      "step: 200, acc: 0.594, loss: 1.103 (data_loss: 1.103, reg_loss: 0.000), lr: 0.0003693444136657433\n",
      "step: 300, acc: 0.781, loss: 0.559 (data_loss: 0.559, reg_loss: 0.000), lr: 0.0003679852805887764\n",
      "step: 400, acc: 0.719, loss: 0.772 (data_loss: 0.772, reg_loss: 0.000), lr: 0.00036663611365719525\n",
      "step: 500, acc: 0.750, loss: 0.835 (data_loss: 0.835, reg_loss: 0.000), lr: 0.0003652968036529681\n",
      "step: 600, acc: 0.719, loss: 0.676 (data_loss: 0.676, reg_loss: 0.000), lr: 0.0003639672429481347\n",
      "step: 700, acc: 0.625, loss: 0.681 (data_loss: 0.681, reg_loss: 0.000), lr: 0.0003626473254759746\n",
      "step: 800, acc: 0.750, loss: 0.634 (data_loss: 0.634, reg_loss: 0.000), lr: 0.0003613369467028004\n",
      "step: 900, acc: 0.531, loss: 1.269 (data_loss: 1.269, reg_loss: 0.000), lr: 0.00036003600360036\n",
      "step: 1000, acc: 0.656, loss: 0.809 (data_loss: 0.809, reg_loss: 0.000), lr: 0.00035874439461883406\n",
      "step: 1100, acc: 0.781, loss: 0.569 (data_loss: 0.569, reg_loss: 0.000), lr: 0.000357462019660411\n",
      "step: 1200, acc: 0.719, loss: 0.682 (data_loss: 0.682, reg_loss: 0.000), lr: 0.00035618878005342833\n",
      "step: 1300, acc: 0.656, loss: 0.933 (data_loss: 0.933, reg_loss: 0.000), lr: 0.000354924578527063\n",
      "step: 1400, acc: 0.719, loss: 1.175 (data_loss: 1.175, reg_loss: 0.000), lr: 0.0003536693191865605\n",
      "step: 1500, acc: 0.656, loss: 0.606 (data_loss: 0.606, reg_loss: 0.000), lr: 0.00035242290748898676\n",
      "step: 1600, acc: 0.781, loss: 0.682 (data_loss: 0.682, reg_loss: 0.000), lr: 0.00035118525021949075\n",
      "step: 1700, acc: 0.781, loss: 0.563 (data_loss: 0.563, reg_loss: 0.000), lr: 0.0003499562554680665\n",
      "step: 1800, acc: 0.719, loss: 0.729 (data_loss: 0.729, reg_loss: 0.000), lr: 0.0003487358326068004\n",
      "step: 1874, acc: 0.656, loss: 0.911 (data_loss: 0.911, reg_loss: 0.000), lr: 0.0003478381856760235\n",
      "training, acc: 0.718, loss: 0.743 (data_loss: 0.743, reg_loss: 0.000), lr: 0.0003478381856760235\n",
      "validation, acc: 0.824, loss: 0.487\n",
      "epoch: 1\n",
      "step: 0, acc: 0.109, loss: 2.301 (data_loss: 2.301, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.531, loss: 1.151 (data_loss: 1.151, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 200, acc: 0.547, loss: 1.300 (data_loss: 1.300, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 300, acc: 0.609, loss: 1.090 (data_loss: 1.090, reg_loss: 0.000), lr: 0.0009708737864077671\n",
      "step: 400, acc: 0.656, loss: 0.799 (data_loss: 0.799, reg_loss: 0.000), lr: 0.0009615384615384615\n",
      "step: 500, acc: 0.625, loss: 1.027 (data_loss: 1.027, reg_loss: 0.000), lr: 0.0009523809523809524\n",
      "step: 600, acc: 0.625, loss: 0.838 (data_loss: 0.838, reg_loss: 0.000), lr: 0.0009433962264150942\n",
      "step: 700, acc: 0.578, loss: 1.108 (data_loss: 1.108, reg_loss: 0.000), lr: 0.0009345794392523365\n",
      "step: 800, acc: 0.766, loss: 0.771 (data_loss: 0.771, reg_loss: 0.000), lr: 0.0009259259259259259\n",
      "step: 900, acc: 0.672, loss: 0.818 (data_loss: 0.818, reg_loss: 0.000), lr: 0.0009174311926605504\n",
      "step: 937, acc: 0.594, loss: 0.961 (data_loss: 0.961, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "training, acc: 0.586, loss: 1.041 (data_loss: 1.041, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "validation, acc: 0.777, loss: 0.596\n",
      "epoch: 2\n",
      "step: 0, acc: 0.750, loss: 0.749 (data_loss: 0.749, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.641, loss: 0.718 (data_loss: 0.718, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 200, acc: 0.703, loss: 0.860 (data_loss: 0.860, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 300, acc: 0.656, loss: 0.903 (data_loss: 0.903, reg_loss: 0.000), lr: 0.0008898380494749957\n",
      "step: 400, acc: 0.688, loss: 0.739 (data_loss: 0.739, reg_loss: 0.000), lr: 0.0008819897689186807\n",
      "step: 500, acc: 0.719, loss: 0.948 (data_loss: 0.948, reg_loss: 0.000), lr: 0.0008742787200559539\n",
      "step: 600, acc: 0.672, loss: 0.749 (data_loss: 0.749, reg_loss: 0.000), lr: 0.0008667013347200555\n",
      "step: 700, acc: 0.703, loss: 0.952 (data_loss: 0.952, reg_loss: 0.000), lr: 0.0008592541673827119\n",
      "step: 800, acc: 0.781, loss: 0.728 (data_loss: 0.728, reg_loss: 0.000), lr: 0.0008519338899301414\n",
      "step: 900, acc: 0.719, loss: 0.793 (data_loss: 0.793, reg_loss: 0.000), lr: 0.0008447372867038352\n",
      "step: 937, acc: 0.656, loss: 0.907 (data_loss: 0.907, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "training, acc: 0.664, loss: 0.867 (data_loss: 0.867, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "validation, acc: 0.797, loss: 0.544\n",
      "epoch: 3\n",
      "step: 0, acc: 0.750, loss: 0.692 (data_loss: 0.692, reg_loss: 0.000), lr: 0.0008420343550016842\n",
      "step: 100, acc: 0.688, loss: 0.793 (data_loss: 0.793, reg_loss: 0.000), lr: 0.0008350033400133601\n",
      "step: 200, acc: 0.703, loss: 0.806 (data_loss: 0.806, reg_loss: 0.000), lr: 0.0008280887711162637\n",
      "step: 300, acc: 0.688, loss: 0.940 (data_loss: 0.940, reg_loss: 0.000), lr: 0.0008212877792378449\n",
      "step: 400, acc: 0.766, loss: 0.671 (data_loss: 0.671, reg_loss: 0.000), lr: 0.0008145975887911372\n",
      "step: 500, acc: 0.641, loss: 0.932 (data_loss: 0.932, reg_loss: 0.000), lr: 0.0008080155138978669\n",
      "step: 600, acc: 0.719, loss: 0.771 (data_loss: 0.771, reg_loss: 0.000), lr: 0.0008015389547932029\n",
      "step: 700, acc: 0.625, loss: 0.922 (data_loss: 0.922, reg_loss: 0.000), lr: 0.0007951653944020356\n",
      "step: 800, acc: 0.797, loss: 0.708 (data_loss: 0.708, reg_loss: 0.000), lr: 0.0007888923950773115\n",
      "step: 900, acc: 0.688, loss: 1.009 (data_loss: 1.009, reg_loss: 0.000), lr: 0.0007827175954915466\n",
      "step: 937, acc: 0.656, loss: 0.728 (data_loss: 0.728, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "training, acc: 0.678, loss: 0.831 (data_loss: 0.831, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "validation, acc: 0.805, loss: 0.531\n",
      "epoch: 4\n",
      "step: 0, acc: 0.781, loss: 0.790 (data_loss: 0.790, reg_loss: 0.000), lr: 0.0007803964413922272\n",
      "step: 100, acc: 0.703, loss: 0.802 (data_loss: 0.802, reg_loss: 0.000), lr: 0.0007743534148985598\n",
      "step: 200, acc: 0.703, loss: 0.925 (data_loss: 0.925, reg_loss: 0.000), lr: 0.0007684032580298141\n",
      "step: 300, acc: 0.719, loss: 0.750 (data_loss: 0.750, reg_loss: 0.000), lr: 0.0007625438462711606\n",
      "step: 400, acc: 0.766, loss: 0.772 (data_loss: 0.772, reg_loss: 0.000), lr: 0.0007567731194187983\n",
      "step: 500, acc: 0.750, loss: 0.852 (data_loss: 0.852, reg_loss: 0.000), lr: 0.000751089079164789\n",
      "step: 600, acc: 0.750, loss: 0.776 (data_loss: 0.776, reg_loss: 0.000), lr: 0.000745489786789921\n",
      "step: 700, acc: 0.672, loss: 1.178 (data_loss: 1.178, reg_loss: 0.000), lr: 0.0007399733609590056\n",
      "step: 800, acc: 0.812, loss: 0.563 (data_loss: 0.563, reg_loss: 0.000), lr: 0.0007345379756133392\n",
      "step: 900, acc: 0.703, loss: 0.754 (data_loss: 0.754, reg_loss: 0.000), lr: 0.000729181857955374\n",
      "step: 937, acc: 0.688, loss: 1.138 (data_loss: 1.138, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "training, acc: 0.695, loss: 0.806 (data_loss: 0.806, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "validation, acc: 0.811, loss: 0.534\n",
      "epoch: 5\n",
      "step: 0, acc: 0.672, loss: 0.769 (data_loss: 0.769, reg_loss: 0.000), lr: 0.0007271669575334498\n",
      "step: 100, acc: 0.781, loss: 0.719 (data_loss: 0.719, reg_loss: 0.000), lr: 0.000721917412647993\n",
      "step: 200, acc: 0.719, loss: 0.723 (data_loss: 0.723, reg_loss: 0.000), lr: 0.0007167431192660551\n",
      "step: 300, acc: 0.812, loss: 0.955 (data_loss: 0.955, reg_loss: 0.000), lr: 0.0007116424708226587\n",
      "step: 400, acc: 0.781, loss: 0.615 (data_loss: 0.615, reg_loss: 0.000), lr: 0.0007066139061616733\n",
      "step: 500, acc: 0.703, loss: 0.773 (data_loss: 0.773, reg_loss: 0.000), lr: 0.0007016559079427449\n",
      "step: 600, acc: 0.703, loss: 0.734 (data_loss: 0.734, reg_loss: 0.000), lr: 0.0006967670011148271\n",
      "step: 700, acc: 0.641, loss: 0.944 (data_loss: 0.944, reg_loss: 0.000), lr: 0.0006919457514530861\n",
      "step: 800, acc: 0.875, loss: 0.436 (data_loss: 0.436, reg_loss: 0.000), lr: 0.0006871907641561297\n",
      "step: 900, acc: 0.703, loss: 0.683 (data_loss: 0.683, reg_loss: 0.000), lr: 0.0006825006825006825\n",
      "step: 937, acc: 0.625, loss: 0.843 (data_loss: 0.843, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "training, acc: 0.702, loss: 0.789 (data_loss: 0.789, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "validation, acc: 0.820, loss: 0.506\n",
      "epoch: 6\n",
      "step: 0, acc: 0.672, loss: 0.807 (data_loss: 0.807, reg_loss: 0.000), lr: 0.0006807351940095304\n",
      "step: 100, acc: 0.594, loss: 0.913 (data_loss: 0.913, reg_loss: 0.000), lr: 0.000676132521974307\n",
      "step: 200, acc: 0.719, loss: 0.943 (data_loss: 0.943, reg_loss: 0.000), lr: 0.0006715916722632639\n",
      "step: 300, acc: 0.703, loss: 1.045 (data_loss: 1.045, reg_loss: 0.000), lr: 0.0006671114076050701\n",
      "step: 400, acc: 0.812, loss: 0.624 (data_loss: 0.624, reg_loss: 0.000), lr: 0.0006626905235255136\n",
      "step: 500, acc: 0.688, loss: 0.703 (data_loss: 0.703, reg_loss: 0.000), lr: 0.0006583278472679394\n",
      "step: 600, acc: 0.672, loss: 0.819 (data_loss: 0.819, reg_loss: 0.000), lr: 0.0006540222367560497\n",
      "step: 700, acc: 0.703, loss: 0.845 (data_loss: 0.845, reg_loss: 0.000), lr: 0.000649772579597141\n",
      "step: 800, acc: 0.750, loss: 0.739 (data_loss: 0.739, reg_loss: 0.000), lr: 0.000645577792123951\n",
      "step: 900, acc: 0.766, loss: 0.822 (data_loss: 0.822, reg_loss: 0.000), lr: 0.0006414368184733803\n",
      "step: 937, acc: 0.750, loss: 0.915 (data_loss: 0.915, reg_loss: 0.000), lr: 0.0006399180904844181\n",
      "training, acc: 0.707, loss: 0.771 (data_loss: 0.771, reg_loss: 0.000), lr: 0.0006399180904844181\n",
      "validation, acc: 0.821, loss: 0.499\n",
      "epoch: 7\n",
      "step: 0, acc: 0.719, loss: 0.866 (data_loss: 0.866, reg_loss: 0.000), lr: 0.0006398771435884309\n",
      "step: 100, acc: 0.781, loss: 0.650 (data_loss: 0.650, reg_loss: 0.000), lr: 0.0006358087487283825\n",
      "step: 200, acc: 0.766, loss: 0.875 (data_loss: 0.875, reg_loss: 0.000), lr: 0.0006317917614354309\n",
      "step: 300, acc: 0.656, loss: 0.900 (data_loss: 0.900, reg_loss: 0.000), lr: 0.0006278252134605726\n",
      "step: 400, acc: 0.828, loss: 0.567 (data_loss: 0.567, reg_loss: 0.000), lr: 0.0006239081607187422\n",
      "step: 500, acc: 0.750, loss: 0.935 (data_loss: 0.935, reg_loss: 0.000), lr: 0.0006200396825396826\n",
      "step: 600, acc: 0.688, loss: 0.655 (data_loss: 0.655, reg_loss: 0.000), lr: 0.0006162188809465122\n",
      "step: 700, acc: 0.656, loss: 0.981 (data_loss: 0.981, reg_loss: 0.000), lr: 0.0006124448799608035\n",
      "step: 800, acc: 0.781, loss: 0.676 (data_loss: 0.676, reg_loss: 0.000), lr: 0.0006087168249330412\n",
      "step: 900, acc: 0.719, loss: 0.831 (data_loss: 0.831, reg_loss: 0.000), lr: 0.0006050338818973863\n",
      "step: 937, acc: 0.688, loss: 0.990 (data_loss: 0.990, reg_loss: 0.000), lr: 0.0006036824630244491\n",
      "training, acc: 0.708, loss: 0.776 (data_loss: 0.776, reg_loss: 0.000), lr: 0.0006036824630244491\n",
      "validation, acc: 0.817, loss: 0.505\n",
      "epoch: 8\n",
      "step: 0, acc: 0.703, loss: 0.794 (data_loss: 0.794, reg_loss: 0.000), lr: 0.0006036460219727152\n",
      "step: 100, acc: 0.719, loss: 0.925 (data_loss: 0.925, reg_loss: 0.000), lr: 0.0006000240009600384\n",
      "step: 200, acc: 0.719, loss: 0.873 (data_loss: 0.873, reg_loss: 0.000), lr: 0.0005964451866873433\n",
      "step: 300, acc: 0.672, loss: 0.911 (data_loss: 0.911, reg_loss: 0.000), lr: 0.000592908810624926\n",
      "step: 400, acc: 0.766, loss: 0.522 (data_loss: 0.522, reg_loss: 0.000), lr: 0.0005894141223623718\n",
      "step: 500, acc: 0.719, loss: 0.854 (data_loss: 0.854, reg_loss: 0.000), lr: 0.0005859603890776984\n",
      "step: 600, acc: 0.625, loss: 0.742 (data_loss: 0.742, reg_loss: 0.000), lr: 0.0005825468950250495\n",
      "step: 700, acc: 0.641, loss: 1.048 (data_loss: 1.048, reg_loss: 0.000), lr: 0.0005791729410401946\n",
      "step: 800, acc: 0.797, loss: 0.557 (data_loss: 0.557, reg_loss: 0.000), lr: 0.0005758378440631118\n",
      "step: 900, acc: 0.656, loss: 0.883 (data_loss: 0.883, reg_loss: 0.000), lr: 0.0005725409366769725\n",
      "step: 937, acc: 0.719, loss: 1.032 (data_loss: 1.032, reg_loss: 0.000), lr: 0.0005713306290350225\n",
      "training, acc: 0.710, loss: 0.763 (data_loss: 0.763, reg_loss: 0.000), lr: 0.0005713306290350225\n",
      "validation, acc: 0.827, loss: 0.486\n",
      "epoch: 9\n",
      "step: 0, acc: 0.766, loss: 0.796 (data_loss: 0.796, reg_loss: 0.000), lr: 0.0005712979890310786\n",
      "step: 100, acc: 0.766, loss: 0.604 (data_loss: 0.604, reg_loss: 0.000), lr: 0.000568052715291979\n",
      "step: 200, acc: 0.734, loss: 0.670 (data_loss: 0.670, reg_loss: 0.000), lr: 0.0005648441030275643\n",
      "step: 300, acc: 0.719, loss: 0.915 (data_loss: 0.915, reg_loss: 0.000), lr: 0.0005616715344866322\n",
      "step: 400, acc: 0.688, loss: 0.685 (data_loss: 0.685, reg_loss: 0.000), lr: 0.0005585344057193923\n",
      "step: 500, acc: 0.641, loss: 1.094 (data_loss: 1.094, reg_loss: 0.000), lr: 0.000555432126194179\n",
      "step: 600, acc: 0.719, loss: 0.649 (data_loss: 0.649, reg_loss: 0.000), lr: 0.0005523641184268669\n",
      "step: 700, acc: 0.719, loss: 0.790 (data_loss: 0.790, reg_loss: 0.000), lr: 0.0005493298176225006\n",
      "step: 800, acc: 0.844, loss: 0.482 (data_loss: 0.482, reg_loss: 0.000), lr: 0.0005463286713286714\n",
      "step: 900, acc: 0.703, loss: 0.825 (data_loss: 0.825, reg_loss: 0.000), lr: 0.0005433601391001957\n",
      "step: 937, acc: 0.688, loss: 0.984 (data_loss: 0.984, reg_loss: 0.000), lr: 0.0005422699419771162\n",
      "training, acc: 0.715, loss: 0.755 (data_loss: 0.755, reg_loss: 0.000), lr: 0.0005422699419771162\n",
      "validation, acc: 0.825, loss: 0.488\n",
      "epoch: 10\n",
      "step: 0, acc: 0.766, loss: 0.715 (data_loss: 0.715, reg_loss: 0.000), lr: 0.0005422405379026137\n",
      "step: 100, acc: 0.734, loss: 0.664 (data_loss: 0.664, reg_loss: 0.000), lr: 0.0005393161471254449\n",
      "step: 200, acc: 0.734, loss: 0.723 (data_loss: 0.723, reg_loss: 0.000), lr: 0.0005364231305653899\n",
      "step: 300, acc: 0.719, loss: 0.868 (data_loss: 0.868, reg_loss: 0.000), lr: 0.0005335609860207021\n",
      "step: 400, acc: 0.766, loss: 0.651 (data_loss: 0.651, reg_loss: 0.000), lr: 0.0005307292219509606\n",
      "step: 500, acc: 0.734, loss: 0.677 (data_loss: 0.677, reg_loss: 0.000), lr: 0.0005279273571956499\n",
      "step: 600, acc: 0.719, loss: 0.716 (data_loss: 0.716, reg_loss: 0.000), lr: 0.000525154920701607\n",
      "step: 700, acc: 0.750, loss: 0.813 (data_loss: 0.813, reg_loss: 0.000), lr: 0.0005224114512590116\n",
      "step: 800, acc: 0.812, loss: 0.680 (data_loss: 0.680, reg_loss: 0.000), lr: 0.0005196964972456087\n",
      "step: 900, acc: 0.703, loss: 0.895 (data_loss: 0.895, reg_loss: 0.000), lr: 0.0005170096163788645\n",
      "step: 937, acc: 0.719, loss: 0.897 (data_loss: 0.897, reg_loss: 0.000), lr: 0.0005160224985809382\n",
      "training, acc: 0.721, loss: 0.744 (data_loss: 0.744, reg_loss: 0.000), lr: 0.0005160224985809382\n",
      "validation, acc: 0.825, loss: 0.492\n",
      "epoch: 1\n",
      "step: 0, acc: 0.070, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.578, loss: 1.131 (data_loss: 1.131, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 200, acc: 0.609, loss: 0.981 (data_loss: 0.981, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 300, acc: 0.609, loss: 0.838 (data_loss: 0.838, reg_loss: 0.000), lr: 0.0009708737864077671\n",
      "step: 400, acc: 0.695, loss: 0.844 (data_loss: 0.844, reg_loss: 0.000), lr: 0.0009615384615384615\n",
      "step: 468, acc: 0.646, loss: 0.927 (data_loss: 0.927, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "training, acc: 0.572, loss: 1.075 (data_loss: 1.075, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "validation, acc: 0.776, loss: 0.591\n",
      "epoch: 2\n",
      "step: 0, acc: 0.734, loss: 0.770 (data_loss: 0.770, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.703, loss: 1.058 (data_loss: 1.058, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 200, acc: 0.711, loss: 0.773 (data_loss: 0.773, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 300, acc: 0.672, loss: 0.806 (data_loss: 0.806, reg_loss: 0.000), lr: 0.0009285913269570063\n",
      "step: 400, acc: 0.734, loss: 0.793 (data_loss: 0.793, reg_loss: 0.000), lr: 0.0009200478424878093\n",
      "step: 468, acc: 0.667, loss: 0.840 (data_loss: 0.840, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "training, acc: 0.671, loss: 0.855 (data_loss: 0.855, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "validation, acc: 0.790, loss: 0.552\n",
      "epoch: 3\n",
      "step: 0, acc: 0.711, loss: 0.778 (data_loss: 0.778, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.734, loss: 0.827 (data_loss: 0.827, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 200, acc: 0.695, loss: 0.831 (data_loss: 0.831, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 300, acc: 0.672, loss: 0.764 (data_loss: 0.764, reg_loss: 0.000), lr: 0.0008898380494749957\n",
      "step: 400, acc: 0.695, loss: 0.814 (data_loss: 0.814, reg_loss: 0.000), lr: 0.0008819897689186807\n",
      "step: 468, acc: 0.625, loss: 0.914 (data_loss: 0.914, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "training, acc: 0.687, loss: 0.821 (data_loss: 0.821, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "validation, acc: 0.809, loss: 0.532\n",
      "epoch: 4\n",
      "step: 0, acc: 0.719, loss: 0.688 (data_loss: 0.688, reg_loss: 0.000), lr: 0.0008766546857192952\n",
      "step: 100, acc: 0.664, loss: 0.932 (data_loss: 0.932, reg_loss: 0.000), lr: 0.0008690362388111585\n",
      "step: 200, acc: 0.672, loss: 0.796 (data_loss: 0.796, reg_loss: 0.000), lr: 0.0008615490652192642\n",
      "step: 300, acc: 0.703, loss: 0.698 (data_loss: 0.698, reg_loss: 0.000), lr: 0.0008541898009737763\n",
      "step: 400, acc: 0.695, loss: 0.816 (data_loss: 0.816, reg_loss: 0.000), lr: 0.0008469551960701278\n",
      "step: 468, acc: 0.688, loss: 0.929 (data_loss: 0.929, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "training, acc: 0.695, loss: 0.798 (data_loss: 0.798, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "validation, acc: 0.812, loss: 0.514\n",
      "epoch: 5\n",
      "step: 0, acc: 0.719, loss: 0.728 (data_loss: 0.728, reg_loss: 0.000), lr: 0.0008420343550016842\n",
      "step: 100, acc: 0.672, loss: 0.832 (data_loss: 0.832, reg_loss: 0.000), lr: 0.0008350033400133601\n",
      "step: 200, acc: 0.695, loss: 0.747 (data_loss: 0.747, reg_loss: 0.000), lr: 0.0008280887711162637\n",
      "step: 300, acc: 0.727, loss: 0.679 (data_loss: 0.679, reg_loss: 0.000), lr: 0.0008212877792378449\n",
      "step: 400, acc: 0.695, loss: 0.784 (data_loss: 0.784, reg_loss: 0.000), lr: 0.0008145975887911372\n",
      "step: 468, acc: 0.708, loss: 0.956 (data_loss: 0.956, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "training, acc: 0.701, loss: 0.786 (data_loss: 0.786, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "validation, acc: 0.815, loss: 0.524\n",
      "epoch: 6\n",
      "step: 0, acc: 0.750, loss: 0.630 (data_loss: 0.630, reg_loss: 0.000), lr: 0.0008100445524503849\n",
      "step: 100, acc: 0.719, loss: 0.731 (data_loss: 0.731, reg_loss: 0.000), lr: 0.0008035355564483729\n",
      "step: 200, acc: 0.789, loss: 0.630 (data_loss: 0.630, reg_loss: 0.000), lr: 0.0007971303308090873\n",
      "step: 300, acc: 0.750, loss: 0.727 (data_loss: 0.727, reg_loss: 0.000), lr: 0.0007908264136022144\n",
      "step: 400, acc: 0.711, loss: 0.750 (data_loss: 0.750, reg_loss: 0.000), lr: 0.0007846214201647706\n",
      "step: 468, acc: 0.688, loss: 0.719 (data_loss: 0.719, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "training, acc: 0.707, loss: 0.768 (data_loss: 0.768, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "validation, acc: 0.819, loss: 0.502\n",
      "epoch: 7\n",
      "step: 0, acc: 0.734, loss: 0.682 (data_loss: 0.682, reg_loss: 0.000), lr: 0.0007803964413922272\n",
      "step: 100, acc: 0.672, loss: 0.879 (data_loss: 0.879, reg_loss: 0.000), lr: 0.0007743534148985598\n",
      "step: 200, acc: 0.742, loss: 0.745 (data_loss: 0.745, reg_loss: 0.000), lr: 0.0007684032580298141\n",
      "step: 300, acc: 0.719, loss: 0.640 (data_loss: 0.640, reg_loss: 0.000), lr: 0.0007625438462711606\n",
      "step: 400, acc: 0.758, loss: 0.810 (data_loss: 0.810, reg_loss: 0.000), lr: 0.0007567731194187983\n",
      "step: 468, acc: 0.635, loss: 0.764 (data_loss: 0.764, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "training, acc: 0.711, loss: 0.762 (data_loss: 0.762, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "validation, acc: 0.819, loss: 0.502\n",
      "epoch: 8\n",
      "step: 0, acc: 0.633, loss: 0.778 (data_loss: 0.778, reg_loss: 0.000), lr: 0.0007528419784687194\n",
      "step: 100, acc: 0.703, loss: 0.820 (data_loss: 0.820, reg_loss: 0.000), lr: 0.0007472166180975864\n",
      "step: 200, acc: 0.672, loss: 0.813 (data_loss: 0.813, reg_loss: 0.000), lr: 0.0007416747014759327\n",
      "step: 300, acc: 0.641, loss: 0.747 (data_loss: 0.747, reg_loss: 0.000), lr: 0.0007362143856290952\n",
      "step: 400, acc: 0.742, loss: 0.697 (data_loss: 0.697, reg_loss: 0.000), lr: 0.0007308338814587444\n",
      "step: 468, acc: 0.615, loss: 0.986 (data_loss: 0.986, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "training, acc: 0.710, loss: 0.761 (data_loss: 0.761, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "validation, acc: 0.821, loss: 0.494\n",
      "epoch: 9\n",
      "step: 0, acc: 0.727, loss: 0.710 (data_loss: 0.710, reg_loss: 0.000), lr: 0.0007271669575334498\n",
      "step: 100, acc: 0.758, loss: 0.687 (data_loss: 0.687, reg_loss: 0.000), lr: 0.000721917412647993\n",
      "step: 200, acc: 0.727, loss: 0.742 (data_loss: 0.742, reg_loss: 0.000), lr: 0.0007167431192660551\n",
      "step: 300, acc: 0.695, loss: 0.713 (data_loss: 0.713, reg_loss: 0.000), lr: 0.0007116424708226587\n",
      "step: 400, acc: 0.695, loss: 0.712 (data_loss: 0.712, reg_loss: 0.000), lr: 0.0007066139061616733\n",
      "step: 468, acc: 0.656, loss: 0.909 (data_loss: 0.909, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "training, acc: 0.714, loss: 0.749 (data_loss: 0.749, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "validation, acc: 0.824, loss: 0.496\n",
      "epoch: 10\n",
      "step: 0, acc: 0.719, loss: 0.668 (data_loss: 0.668, reg_loss: 0.000), lr: 0.0007031854299978904\n",
      "step: 100, acc: 0.703, loss: 0.839 (data_loss: 0.839, reg_loss: 0.000), lr: 0.0006982752601075343\n",
      "step: 200, acc: 0.695, loss: 0.678 (data_loss: 0.678, reg_loss: 0.000), lr: 0.000693433187712364\n",
      "step: 300, acc: 0.711, loss: 0.708 (data_loss: 0.708, reg_loss: 0.000), lr: 0.0006886578059362303\n",
      "step: 400, acc: 0.750, loss: 0.691 (data_loss: 0.691, reg_loss: 0.000), lr: 0.0006839477463921757\n",
      "step: 468, acc: 0.740, loss: 0.924 (data_loss: 0.924, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "training, acc: 0.717, loss: 0.744 (data_loss: 0.744, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "validation, acc: 0.825, loss: 0.487\n",
      "epoch: 1\n",
      "step: 0, acc: 0.109, loss: 2.302 (data_loss: 2.302, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.598, loss: 1.024 (data_loss: 1.024, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 200, acc: 0.645, loss: 0.977 (data_loss: 0.977, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 234, acc: 0.646, loss: 0.979 (data_loss: 0.979, reg_loss: 0.000), lr: 0.0009771350400625367\n",
      "training, acc: 0.529, loss: 1.167 (data_loss: 1.167, reg_loss: 0.000), lr: 0.0009771350400625367\n",
      "validation, acc: 0.766, loss: 0.633\n",
      "epoch: 2\n",
      "step: 0, acc: 0.637, loss: 0.942 (data_loss: 0.942, reg_loss: 0.000), lr: 0.0009770395701025891\n",
      "step: 100, acc: 0.656, loss: 0.815 (data_loss: 0.815, reg_loss: 0.000), lr: 0.0009675858732462506\n",
      "step: 200, acc: 0.703, loss: 0.845 (data_loss: 0.845, reg_loss: 0.000), lr: 0.0009583133684714901\n",
      "step: 234, acc: 0.708, loss: 0.895 (data_loss: 0.895, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "training, acc: 0.657, loss: 0.874 (data_loss: 0.874, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "validation, acc: 0.786, loss: 0.559\n",
      "epoch: 3\n",
      "step: 0, acc: 0.680, loss: 0.841 (data_loss: 0.841, reg_loss: 0.000), lr: 0.0009551098376313277\n",
      "step: 100, acc: 0.672, loss: 0.831 (data_loss: 0.831, reg_loss: 0.000), lr: 0.0009460737937559131\n",
      "step: 200, acc: 0.680, loss: 0.819 (data_loss: 0.819, reg_loss: 0.000), lr: 0.0009372071227741331\n",
      "step: 234, acc: 0.656, loss: 0.960 (data_loss: 0.960, reg_loss: 0.000), lr: 0.0009342301943198805\n",
      "training, acc: 0.684, loss: 0.830 (data_loss: 0.830, reg_loss: 0.000), lr: 0.0009342301943198805\n",
      "validation, acc: 0.803, loss: 0.538\n",
      "epoch: 4\n",
      "step: 0, acc: 0.723, loss: 0.727 (data_loss: 0.727, reg_loss: 0.000), lr: 0.0009341429238673517\n",
      "step: 100, acc: 0.668, loss: 0.829 (data_loss: 0.829, reg_loss: 0.000), lr: 0.0009254974548819991\n",
      "step: 200, acc: 0.738, loss: 0.728 (data_loss: 0.728, reg_loss: 0.000), lr: 0.0009170105456212746\n",
      "step: 234, acc: 0.729, loss: 0.781 (data_loss: 0.781, reg_loss: 0.000), lr: 0.0009141603437242892\n",
      "training, acc: 0.696, loss: 0.803 (data_loss: 0.803, reg_loss: 0.000), lr: 0.0009141603437242892\n",
      "validation, acc: 0.813, loss: 0.524\n",
      "epoch: 5\n",
      "step: 0, acc: 0.699, loss: 0.758 (data_loss: 0.758, reg_loss: 0.000), lr: 0.0009140767824497257\n",
      "step: 100, acc: 0.719, loss: 0.827 (data_loss: 0.827, reg_loss: 0.000), lr: 0.0009057971014492753\n",
      "step: 200, acc: 0.723, loss: 0.732 (data_loss: 0.732, reg_loss: 0.000), lr: 0.0008976660682226211\n",
      "step: 234, acc: 0.677, loss: 0.917 (data_loss: 0.917, reg_loss: 0.000), lr: 0.0008949346697691068\n",
      "training, acc: 0.704, loss: 0.788 (data_loss: 0.788, reg_loss: 0.000), lr: 0.0008949346697691068\n",
      "validation, acc: 0.814, loss: 0.512\n",
      "epoch: 6\n",
      "step: 0, acc: 0.723, loss: 0.761 (data_loss: 0.761, reg_loss: 0.000), lr: 0.000894854586129754\n",
      "step: 100, acc: 0.738, loss: 0.762 (data_loss: 0.762, reg_loss: 0.000), lr: 0.0008869179600886919\n",
      "step: 200, acc: 0.727, loss: 0.722 (data_loss: 0.722, reg_loss: 0.000), lr: 0.0008791208791208791\n",
      "step: 234, acc: 0.688, loss: 0.828 (data_loss: 0.828, reg_loss: 0.000), lr: 0.0008765010079761591\n",
      "training, acc: 0.710, loss: 0.770 (data_loss: 0.770, reg_loss: 0.000), lr: 0.0008765010079761591\n",
      "validation, acc: 0.810, loss: 0.517\n",
      "epoch: 7\n",
      "step: 0, acc: 0.750, loss: 0.681 (data_loss: 0.681, reg_loss: 0.000), lr: 0.0008764241893076249\n",
      "step: 100, acc: 0.723, loss: 0.820 (data_loss: 0.820, reg_loss: 0.000), lr: 0.0008688097306689836\n",
      "step: 200, acc: 0.766, loss: 0.703 (data_loss: 0.703, reg_loss: 0.000), lr: 0.0008613264427217915\n",
      "step: 234, acc: 0.719, loss: 0.811 (data_loss: 0.811, reg_loss: 0.000), lr: 0.0008588114050154585\n",
      "training, acc: 0.716, loss: 0.766 (data_loss: 0.766, reg_loss: 0.000), lr: 0.0008588114050154585\n",
      "validation, acc: 0.823, loss: 0.502\n",
      "epoch: 8\n",
      "step: 0, acc: 0.699, loss: 0.847 (data_loss: 0.847, reg_loss: 0.000), lr: 0.0008587376556462\n",
      "step: 100, acc: 0.738, loss: 0.763 (data_loss: 0.763, reg_loss: 0.000), lr: 0.0008514261387824605\n",
      "step: 200, acc: 0.746, loss: 0.704 (data_loss: 0.704, reg_loss: 0.000), lr: 0.0008442380751371888\n",
      "step: 234, acc: 0.688, loss: 0.937 (data_loss: 0.937, reg_loss: 0.000), lr: 0.0008418217021634818\n",
      "training, acc: 0.717, loss: 0.758 (data_loss: 0.758, reg_loss: 0.000), lr: 0.0008418217021634818\n",
      "validation, acc: 0.821, loss: 0.495\n",
      "epoch: 9\n",
      "step: 0, acc: 0.746, loss: 0.716 (data_loss: 0.716, reg_loss: 0.000), lr: 0.0008417508417508418\n",
      "step: 100, acc: 0.746, loss: 0.745 (data_loss: 0.745, reg_loss: 0.000), lr: 0.0008347245409015025\n",
      "step: 200, acc: 0.750, loss: 0.786 (data_loss: 0.786, reg_loss: 0.000), lr: 0.0008278145695364238\n",
      "step: 234, acc: 0.760, loss: 0.724 (data_loss: 0.724, reg_loss: 0.000), lr: 0.0008254911672445105\n",
      "training, acc: 0.718, loss: 0.750 (data_loss: 0.750, reg_loss: 0.000), lr: 0.0008254911672445105\n",
      "validation, acc: 0.820, loss: 0.505\n",
      "epoch: 10\n",
      "step: 0, acc: 0.699, loss: 0.765 (data_loss: 0.765, reg_loss: 0.000), lr: 0.0008254230293025176\n",
      "step: 100, acc: 0.738, loss: 0.697 (data_loss: 0.697, reg_loss: 0.000), lr: 0.0008186655751125666\n",
      "step: 200, acc: 0.758, loss: 0.774 (data_loss: 0.774, reg_loss: 0.000), lr: 0.0008120178643930166\n",
      "step: 234, acc: 0.750, loss: 0.819 (data_loss: 0.819, reg_loss: 0.000), lr: 0.0008097821685966475\n",
      "training, acc: 0.720, loss: 0.749 (data_loss: 0.749, reg_loss: 0.000), lr: 0.0008097821685966475\n",
      "validation, acc: 0.822, loss: 0.497\n",
      "epoch: 1\n",
      "step: 0, acc: 0.084, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.611, loss: 0.947 (data_loss: 0.947, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 117, acc: 0.625, loss: 1.065 (data_loss: 1.065, reg_loss: 0.000), lr: 0.0009884353069091628\n",
      "training, acc: 0.485, loss: 1.281 (data_loss: 1.281, reg_loss: 0.000), lr: 0.0009884353069091628\n",
      "validation, acc: 0.747, loss: 0.716\n",
      "epoch: 2\n",
      "step: 0, acc: 0.621, loss: 1.007 (data_loss: 1.007, reg_loss: 0.000), lr: 0.0009883376161296698\n",
      "step: 100, acc: 0.672, loss: 0.882 (data_loss: 0.882, reg_loss: 0.000), lr: 0.0009786651008025053\n",
      "step: 117, acc: 0.583, loss: 1.001 (data_loss: 1.001, reg_loss: 0.000), lr: 0.0009770395701025891\n",
      "training, acc: 0.640, loss: 0.912 (data_loss: 0.912, reg_loss: 0.000), lr: 0.0009770395701025891\n",
      "validation, acc: 0.778, loss: 0.617\n",
      "epoch: 3\n",
      "step: 0, acc: 0.691, loss: 0.824 (data_loss: 0.824, reg_loss: 0.000), lr: 0.0009769441187964048\n",
      "step: 100, acc: 0.693, loss: 0.854 (data_loss: 0.854, reg_loss: 0.000), lr: 0.0009674922600619195\n",
      "step: 117, acc: 0.698, loss: 0.844 (data_loss: 0.844, reg_loss: 0.000), lr: 0.0009659036028204385\n",
      "training, acc: 0.674, loss: 0.849 (data_loss: 0.849, reg_loss: 0.000), lr: 0.0009659036028204385\n",
      "validation, acc: 0.791, loss: 0.572\n",
      "epoch: 4\n",
      "step: 0, acc: 0.711, loss: 0.797 (data_loss: 0.797, reg_loss: 0.000), lr: 0.0009658103148541626\n",
      "step: 100, acc: 0.713, loss: 0.763 (data_loss: 0.763, reg_loss: 0.000), lr: 0.0009565716472163764\n",
      "step: 117, acc: 0.646, loss: 0.790 (data_loss: 0.790, reg_loss: 0.000), lr: 0.0009550186228631459\n",
      "training, acc: 0.685, loss: 0.820 (data_loss: 0.820, reg_loss: 0.000), lr: 0.0009550186228631459\n",
      "validation, acc: 0.794, loss: 0.563\n",
      "epoch: 5\n",
      "step: 0, acc: 0.721, loss: 0.770 (data_loss: 0.770, reg_loss: 0.000), lr: 0.000954927425515661\n",
      "step: 100, acc: 0.715, loss: 0.756 (data_loss: 0.756, reg_loss: 0.000), lr: 0.0009458948164964057\n",
      "step: 117, acc: 0.667, loss: 0.959 (data_loss: 0.959, reg_loss: 0.000), lr: 0.0009443762394938143\n",
      "training, acc: 0.697, loss: 0.795 (data_loss: 0.795, reg_loss: 0.000), lr: 0.0009443762394938143\n",
      "validation, acc: 0.807, loss: 0.537\n",
      "epoch: 6\n",
      "step: 0, acc: 0.707, loss: 0.820 (data_loss: 0.820, reg_loss: 0.000), lr: 0.0009442870632672334\n",
      "step: 100, acc: 0.711, loss: 0.780 (data_loss: 0.780, reg_loss: 0.000), lr: 0.0009354536950420954\n",
      "step: 117, acc: 0.729, loss: 0.847 (data_loss: 0.847, reg_loss: 0.000), lr: 0.000933968431867003\n",
      "training, acc: 0.705, loss: 0.782 (data_loss: 0.782, reg_loss: 0.000), lr: 0.000933968431867003\n",
      "validation, acc: 0.808, loss: 0.537\n",
      "epoch: 7\n",
      "step: 0, acc: 0.693, loss: 0.803 (data_loss: 0.803, reg_loss: 0.000), lr: 0.0009338812103100486\n",
      "step: 100, acc: 0.717, loss: 0.777 (data_loss: 0.777, reg_loss: 0.000), lr: 0.0009252405625462621\n",
      "step: 117, acc: 0.719, loss: 0.877 (data_loss: 0.877, reg_loss: 0.000), lr: 0.0009237875288683603\n",
      "training, acc: 0.710, loss: 0.773 (data_loss: 0.773, reg_loss: 0.000), lr: 0.0009237875288683603\n",
      "validation, acc: 0.809, loss: 0.521\n",
      "epoch: 8\n",
      "step: 0, acc: 0.709, loss: 0.764 (data_loss: 0.764, reg_loss: 0.000), lr: 0.0009237021984112323\n",
      "step: 100, acc: 0.684, loss: 0.798 (data_loss: 0.798, reg_loss: 0.000), lr: 0.0009152480322167307\n",
      "step: 117, acc: 0.698, loss: 0.691 (data_loss: 0.691, reg_loss: 0.000), lr: 0.0009138261902586128\n",
      "training, acc: 0.714, loss: 0.762 (data_loss: 0.762, reg_loss: 0.000), lr: 0.0009138261902586128\n",
      "validation, acc: 0.818, loss: 0.506\n",
      "epoch: 9\n",
      "step: 0, acc: 0.729, loss: 0.731 (data_loss: 0.731, reg_loss: 0.000), lr: 0.0009137426900584796\n",
      "step: 100, acc: 0.748, loss: 0.726 (data_loss: 0.726, reg_loss: 0.000), lr: 0.0009054690329590728\n",
      "step: 117, acc: 0.635, loss: 0.858 (data_loss: 0.858, reg_loss: 0.000), lr: 0.0009040773890245004\n",
      "training, acc: 0.716, loss: 0.756 (data_loss: 0.756, reg_loss: 0.000), lr: 0.0009040773890245004\n",
      "validation, acc: 0.814, loss: 0.510\n",
      "epoch: 10\n",
      "step: 0, acc: 0.719, loss: 0.764 (data_loss: 0.764, reg_loss: 0.000), lr: 0.0009039956608208281\n",
      "step: 100, acc: 0.738, loss: 0.724 (data_loss: 0.724, reg_loss: 0.000), lr: 0.0008958967926894821\n",
      "step: 117, acc: 0.688, loss: 0.874 (data_loss: 0.874, reg_loss: 0.000), lr: 0.0008945343948474817\n",
      "training, acc: 0.721, loss: 0.744 (data_loss: 0.744, reg_loss: 0.000), lr: 0.0008945343948474817\n",
      "validation, acc: 0.818, loss: 0.501\n"
     ]
    }
   ],
   "source": [
    "# Train models with different sizes of batch size\n",
    "test_batch_sizes = [16, 32, 64, 128, 256, 512]\n",
    "\n",
    "for batch_size in test_batch_sizes:\n",
    "   Train_Model(X=X, y=y, X_test=X_test, y_test=y_test, model_number=\"_batch_size\"+str(batch_size), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with batch size 16:\n",
      "validation, acc: 0.827, loss: 0.486\n",
      "Model with batch size 32:\n",
      "validation, acc: 0.824, loss: 0.487\n",
      "Model with batch size 64:\n",
      "validation, acc: 0.825, loss: 0.492\n",
      "Model with batch size 128:\n",
      "validation, acc: 0.825, loss: 0.487\n",
      "Model with batch size 256:\n",
      "validation, acc: 0.822, loss: 0.497\n",
      "Model with batch size 512:\n",
      "validation, acc: 0.818, loss: 0.501\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models\n",
    "print(\"Model with batch size 16:\")\n",
    "model = Model.load(\"fashion_mnist_batch_size16.model\")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Model with batch size 32:\")\n",
    "model = Model.load(\"fashion_mnist_batch_size32.model\")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Model with batch size 64:\")\n",
    "model = Model.load(\"fashion_mnist_batch_size64.model\")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Model with batch size 128:\")\n",
    "model = Model.load(\"fashion_mnist_batch_size128.model\")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Model with batch size 256:\")\n",
    "model = Model.load(\"fashion_mnist_batch_size256.model\")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Model with batch size 512:\")\n",
    "model = Model.load(\"fashion_mnist_batch_size512.model\")\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Model_Sigmoid(X, y, X_test, y_test, model_number=None, batch_size=128, epochs=10, number_of_neurons=128, learning_rate=0.001, decay=1e-4, dropout_rate=0.9, number_of_layers=1):\n",
    "    model = Model()\n",
    "    # Add layers\n",
    "    model.add(Layer_Dense(X.shape[1], number_of_neurons))\n",
    "\n",
    "    for i in range(number_of_layers):\n",
    "        model.add(Layer_Dense(number_of_neurons, number_of_neurons))\n",
    "        model.add(Activation_Sigmoid())\n",
    "        \n",
    "    model.add(Layer_Dropout(dropout_rate))\n",
    "    model.add(Layer_Dense(number_of_neurons, 10))\n",
    "    model.add(Activation_Softmax())\n",
    "\n",
    "    model.set(\n",
    "        loss=Loss_CategoricalCrossentropy(),\n",
    "        optimizer=Optimizer_Adam(decay=decay, learning_rate=learning_rate),\n",
    "        accuracy=Accuracy_Categorical(),\n",
    "    )\n",
    "\n",
    "    model.finalize()\n",
    "\n",
    "    model.train(\n",
    "        X, y, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size, print_every=100\n",
    "    )\n",
    "\n",
    "    model.save(\"fashion_mnist\"+ model_number +\".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0, acc: 0.086, loss: 2.315 (data_loss: 2.315, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.453, loss: 1.456 (data_loss: 1.456, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 200, acc: 0.430, loss: 1.260 (data_loss: 1.260, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 300, acc: 0.570, loss: 1.053 (data_loss: 1.053, reg_loss: 0.000), lr: 0.0009708737864077671\n",
      "step: 400, acc: 0.602, loss: 1.163 (data_loss: 1.163, reg_loss: 0.000), lr: 0.0009615384615384615\n",
      "step: 468, acc: 0.531, loss: 1.229 (data_loss: 1.229, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "training, acc: 0.492, loss: 1.309 (data_loss: 1.309, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "validation, acc: 0.729, loss: 0.774\n",
      "epoch: 2\n",
      "step: 0, acc: 0.641, loss: 0.948 (data_loss: 0.948, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.656, loss: 0.966 (data_loss: 0.966, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 200, acc: 0.578, loss: 1.022 (data_loss: 1.022, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 300, acc: 0.609, loss: 0.922 (data_loss: 0.922, reg_loss: 0.000), lr: 0.0009285913269570063\n",
      "step: 400, acc: 0.617, loss: 1.044 (data_loss: 1.044, reg_loss: 0.000), lr: 0.0009200478424878093\n",
      "step: 468, acc: 0.562, loss: 1.014 (data_loss: 1.014, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "training, acc: 0.602, loss: 0.974 (data_loss: 0.974, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "validation, acc: 0.767, loss: 0.655\n",
      "epoch: 3\n",
      "step: 0, acc: 0.656, loss: 0.911 (data_loss: 0.911, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.648, loss: 0.839 (data_loss: 0.839, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 200, acc: 0.656, loss: 0.824 (data_loss: 0.824, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 300, acc: 0.602, loss: 0.839 (data_loss: 0.839, reg_loss: 0.000), lr: 0.0008898380494749957\n",
      "step: 400, acc: 0.680, loss: 0.903 (data_loss: 0.903, reg_loss: 0.000), lr: 0.0008819897689186807\n",
      "step: 468, acc: 0.583, loss: 1.041 (data_loss: 1.041, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "training, acc: 0.631, loss: 0.909 (data_loss: 0.909, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "validation, acc: 0.781, loss: 0.600\n",
      "epoch: 4\n",
      "step: 0, acc: 0.680, loss: 0.814 (data_loss: 0.814, reg_loss: 0.000), lr: 0.0008766546857192952\n",
      "step: 100, acc: 0.703, loss: 0.917 (data_loss: 0.917, reg_loss: 0.000), lr: 0.0008690362388111585\n",
      "step: 200, acc: 0.727, loss: 0.839 (data_loss: 0.839, reg_loss: 0.000), lr: 0.0008615490652192642\n",
      "step: 300, acc: 0.719, loss: 0.789 (data_loss: 0.789, reg_loss: 0.000), lr: 0.0008541898009737763\n",
      "step: 400, acc: 0.727, loss: 0.816 (data_loss: 0.816, reg_loss: 0.000), lr: 0.0008469551960701278\n",
      "step: 468, acc: 0.615, loss: 0.977 (data_loss: 0.977, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "training, acc: 0.645, loss: 0.871 (data_loss: 0.871, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "validation, acc: 0.790, loss: 0.578\n",
      "epoch: 5\n",
      "step: 0, acc: 0.617, loss: 0.793 (data_loss: 0.793, reg_loss: 0.000), lr: 0.0008420343550016842\n",
      "step: 100, acc: 0.656, loss: 0.819 (data_loss: 0.819, reg_loss: 0.000), lr: 0.0008350033400133601\n",
      "step: 200, acc: 0.648, loss: 0.905 (data_loss: 0.905, reg_loss: 0.000), lr: 0.0008280887711162637\n",
      "step: 300, acc: 0.602, loss: 0.779 (data_loss: 0.779, reg_loss: 0.000), lr: 0.0008212877792378449\n",
      "step: 400, acc: 0.672, loss: 0.926 (data_loss: 0.926, reg_loss: 0.000), lr: 0.0008145975887911372\n",
      "step: 468, acc: 0.729, loss: 0.716 (data_loss: 0.716, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "training, acc: 0.650, loss: 0.855 (data_loss: 0.855, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "validation, acc: 0.795, loss: 0.555\n",
      "epoch: 6\n",
      "step: 0, acc: 0.609, loss: 0.801 (data_loss: 0.801, reg_loss: 0.000), lr: 0.0008100445524503849\n",
      "step: 100, acc: 0.641, loss: 0.865 (data_loss: 0.865, reg_loss: 0.000), lr: 0.0008035355564483729\n",
      "step: 200, acc: 0.633, loss: 0.892 (data_loss: 0.892, reg_loss: 0.000), lr: 0.0007971303308090873\n",
      "step: 300, acc: 0.609, loss: 0.811 (data_loss: 0.811, reg_loss: 0.000), lr: 0.0007908264136022144\n",
      "step: 400, acc: 0.656, loss: 0.844 (data_loss: 0.844, reg_loss: 0.000), lr: 0.0007846214201647706\n",
      "step: 468, acc: 0.635, loss: 0.872 (data_loss: 0.872, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "training, acc: 0.660, loss: 0.837 (data_loss: 0.837, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "validation, acc: 0.803, loss: 0.542\n",
      "epoch: 7\n",
      "step: 0, acc: 0.688, loss: 0.788 (data_loss: 0.788, reg_loss: 0.000), lr: 0.0007803964413922272\n",
      "step: 100, acc: 0.680, loss: 0.826 (data_loss: 0.826, reg_loss: 0.000), lr: 0.0007743534148985598\n",
      "step: 200, acc: 0.750, loss: 0.741 (data_loss: 0.741, reg_loss: 0.000), lr: 0.0007684032580298141\n",
      "step: 300, acc: 0.703, loss: 0.725 (data_loss: 0.725, reg_loss: 0.000), lr: 0.0007625438462711606\n",
      "step: 400, acc: 0.703, loss: 0.765 (data_loss: 0.765, reg_loss: 0.000), lr: 0.0007567731194187983\n",
      "step: 468, acc: 0.635, loss: 0.761 (data_loss: 0.761, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "training, acc: 0.665, loss: 0.821 (data_loss: 0.821, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "validation, acc: 0.815, loss: 0.521\n",
      "epoch: 8\n",
      "step: 0, acc: 0.656, loss: 0.709 (data_loss: 0.709, reg_loss: 0.000), lr: 0.0007528419784687194\n",
      "step: 100, acc: 0.656, loss: 0.806 (data_loss: 0.806, reg_loss: 0.000), lr: 0.0007472166180975864\n",
      "step: 200, acc: 0.648, loss: 0.794 (data_loss: 0.794, reg_loss: 0.000), lr: 0.0007416747014759327\n",
      "step: 300, acc: 0.695, loss: 0.737 (data_loss: 0.737, reg_loss: 0.000), lr: 0.0007362143856290952\n",
      "step: 400, acc: 0.625, loss: 0.778 (data_loss: 0.778, reg_loss: 0.000), lr: 0.0007308338814587444\n",
      "step: 468, acc: 0.635, loss: 0.957 (data_loss: 0.957, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "training, acc: 0.669, loss: 0.811 (data_loss: 0.811, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "validation, acc: 0.809, loss: 0.534\n",
      "epoch: 9\n",
      "step: 0, acc: 0.648, loss: 0.738 (data_loss: 0.738, reg_loss: 0.000), lr: 0.0007271669575334498\n",
      "step: 100, acc: 0.680, loss: 0.789 (data_loss: 0.789, reg_loss: 0.000), lr: 0.000721917412647993\n",
      "step: 200, acc: 0.641, loss: 0.828 (data_loss: 0.828, reg_loss: 0.000), lr: 0.0007167431192660551\n",
      "step: 300, acc: 0.703, loss: 0.756 (data_loss: 0.756, reg_loss: 0.000), lr: 0.0007116424708226587\n",
      "step: 400, acc: 0.648, loss: 0.839 (data_loss: 0.839, reg_loss: 0.000), lr: 0.0007066139061616733\n",
      "step: 468, acc: 0.646, loss: 0.877 (data_loss: 0.877, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "training, acc: 0.672, loss: 0.807 (data_loss: 0.807, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "validation, acc: 0.812, loss: 0.524\n",
      "epoch: 10\n",
      "step: 0, acc: 0.758, loss: 0.634 (data_loss: 0.634, reg_loss: 0.000), lr: 0.0007031854299978904\n",
      "step: 100, acc: 0.656, loss: 0.789 (data_loss: 0.789, reg_loss: 0.000), lr: 0.0006982752601075343\n",
      "step: 200, acc: 0.727, loss: 0.705 (data_loss: 0.705, reg_loss: 0.000), lr: 0.000693433187712364\n",
      "step: 300, acc: 0.703, loss: 0.741 (data_loss: 0.741, reg_loss: 0.000), lr: 0.0006886578059362303\n",
      "step: 400, acc: 0.695, loss: 0.865 (data_loss: 0.865, reg_loss: 0.000), lr: 0.0006839477463921757\n",
      "step: 468, acc: 0.677, loss: 0.940 (data_loss: 0.940, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "training, acc: 0.676, loss: 0.797 (data_loss: 0.797, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "validation, acc: 0.815, loss: 0.514\n",
      "epoch: 1\n",
      "step: 0, acc: 0.070, loss: 2.313 (data_loss: 2.313, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.234, loss: 1.847 (data_loss: 1.847, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 200, acc: 0.383, loss: 1.496 (data_loss: 1.496, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 300, acc: 0.453, loss: 1.269 (data_loss: 1.269, reg_loss: 0.000), lr: 0.0009708737864077671\n",
      "step: 400, acc: 0.531, loss: 1.272 (data_loss: 1.272, reg_loss: 0.000), lr: 0.0009615384615384615\n",
      "step: 468, acc: 0.448, loss: 1.268 (data_loss: 1.268, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "training, acc: 0.357, loss: 1.550 (data_loss: 1.550, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "validation, acc: 0.610, loss: 1.008\n",
      "epoch: 2\n",
      "step: 0, acc: 0.578, loss: 1.064 (data_loss: 1.064, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.562, loss: 1.102 (data_loss: 1.102, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 200, acc: 0.492, loss: 1.048 (data_loss: 1.048, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 300, acc: 0.555, loss: 1.020 (data_loss: 1.020, reg_loss: 0.000), lr: 0.0009285913269570063\n",
      "step: 400, acc: 0.555, loss: 1.237 (data_loss: 1.237, reg_loss: 0.000), lr: 0.0009200478424878093\n",
      "step: 468, acc: 0.604, loss: 1.090 (data_loss: 1.090, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "training, acc: 0.532, loss: 1.099 (data_loss: 1.099, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "validation, acc: 0.675, loss: 0.840\n",
      "epoch: 3\n",
      "step: 0, acc: 0.516, loss: 1.059 (data_loss: 1.059, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.570, loss: 0.934 (data_loss: 0.934, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 200, acc: 0.609, loss: 0.947 (data_loss: 0.947, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 300, acc: 0.609, loss: 0.877 (data_loss: 0.877, reg_loss: 0.000), lr: 0.0008898380494749957\n",
      "step: 400, acc: 0.617, loss: 1.043 (data_loss: 1.043, reg_loss: 0.000), lr: 0.0008819897689186807\n",
      "step: 468, acc: 0.573, loss: 0.951 (data_loss: 0.951, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "training, acc: 0.589, loss: 0.992 (data_loss: 0.992, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "validation, acc: 0.723, loss: 0.717\n",
      "epoch: 4\n",
      "step: 0, acc: 0.664, loss: 0.865 (data_loss: 0.865, reg_loss: 0.000), lr: 0.0008766546857192952\n",
      "step: 100, acc: 0.531, loss: 0.996 (data_loss: 0.996, reg_loss: 0.000), lr: 0.0008690362388111585\n",
      "step: 200, acc: 0.633, loss: 0.897 (data_loss: 0.897, reg_loss: 0.000), lr: 0.0008615490652192642\n",
      "step: 300, acc: 0.625, loss: 0.796 (data_loss: 0.796, reg_loss: 0.000), lr: 0.0008541898009737763\n",
      "step: 400, acc: 0.641, loss: 0.927 (data_loss: 0.927, reg_loss: 0.000), lr: 0.0008469551960701278\n",
      "step: 468, acc: 0.625, loss: 0.893 (data_loss: 0.893, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "training, acc: 0.611, loss: 0.933 (data_loss: 0.933, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "validation, acc: 0.744, loss: 0.668\n",
      "epoch: 5\n",
      "step: 0, acc: 0.758, loss: 0.776 (data_loss: 0.776, reg_loss: 0.000), lr: 0.0008420343550016842\n",
      "step: 100, acc: 0.609, loss: 0.962 (data_loss: 0.962, reg_loss: 0.000), lr: 0.0008350033400133601\n",
      "step: 200, acc: 0.609, loss: 0.982 (data_loss: 0.982, reg_loss: 0.000), lr: 0.0008280887711162637\n",
      "step: 300, acc: 0.625, loss: 0.842 (data_loss: 0.842, reg_loss: 0.000), lr: 0.0008212877792378449\n",
      "step: 400, acc: 0.648, loss: 0.860 (data_loss: 0.860, reg_loss: 0.000), lr: 0.0008145975887911372\n",
      "step: 468, acc: 0.604, loss: 1.003 (data_loss: 1.003, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "training, acc: 0.628, loss: 0.891 (data_loss: 0.891, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "validation, acc: 0.769, loss: 0.627\n",
      "epoch: 6\n",
      "step: 0, acc: 0.695, loss: 0.711 (data_loss: 0.711, reg_loss: 0.000), lr: 0.0008100445524503849\n",
      "step: 100, acc: 0.633, loss: 0.928 (data_loss: 0.928, reg_loss: 0.000), lr: 0.0008035355564483729\n",
      "step: 200, acc: 0.633, loss: 0.880 (data_loss: 0.880, reg_loss: 0.000), lr: 0.0007971303308090873\n",
      "step: 300, acc: 0.672, loss: 0.740 (data_loss: 0.740, reg_loss: 0.000), lr: 0.0007908264136022144\n",
      "step: 400, acc: 0.656, loss: 0.773 (data_loss: 0.773, reg_loss: 0.000), lr: 0.0007846214201647706\n",
      "step: 468, acc: 0.583, loss: 0.955 (data_loss: 0.955, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "training, acc: 0.640, loss: 0.864 (data_loss: 0.864, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "validation, acc: 0.775, loss: 0.610\n",
      "epoch: 7\n",
      "step: 0, acc: 0.648, loss: 0.765 (data_loss: 0.765, reg_loss: 0.000), lr: 0.0007803964413922272\n",
      "step: 100, acc: 0.703, loss: 0.850 (data_loss: 0.850, reg_loss: 0.000), lr: 0.0007743534148985598\n",
      "step: 200, acc: 0.641, loss: 0.931 (data_loss: 0.931, reg_loss: 0.000), lr: 0.0007684032580298141\n",
      "step: 300, acc: 0.742, loss: 0.751 (data_loss: 0.751, reg_loss: 0.000), lr: 0.0007625438462711606\n",
      "step: 400, acc: 0.719, loss: 0.776 (data_loss: 0.776, reg_loss: 0.000), lr: 0.0007567731194187983\n",
      "step: 468, acc: 0.615, loss: 0.958 (data_loss: 0.958, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "training, acc: 0.660, loss: 0.833 (data_loss: 0.833, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "validation, acc: 0.796, loss: 0.572\n",
      "epoch: 8\n",
      "step: 0, acc: 0.719, loss: 0.680 (data_loss: 0.680, reg_loss: 0.000), lr: 0.0007528419784687194\n",
      "step: 100, acc: 0.703, loss: 0.809 (data_loss: 0.809, reg_loss: 0.000), lr: 0.0007472166180975864\n",
      "step: 200, acc: 0.633, loss: 0.794 (data_loss: 0.794, reg_loss: 0.000), lr: 0.0007416747014759327\n",
      "step: 300, acc: 0.719, loss: 0.736 (data_loss: 0.736, reg_loss: 0.000), lr: 0.0007362143856290952\n",
      "step: 400, acc: 0.711, loss: 0.677 (data_loss: 0.677, reg_loss: 0.000), lr: 0.0007308338814587444\n",
      "step: 468, acc: 0.667, loss: 0.830 (data_loss: 0.830, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "training, acc: 0.673, loss: 0.805 (data_loss: 0.805, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "validation, acc: 0.809, loss: 0.559\n",
      "epoch: 9\n",
      "step: 0, acc: 0.742, loss: 0.669 (data_loss: 0.669, reg_loss: 0.000), lr: 0.0007271669575334498\n",
      "step: 100, acc: 0.727, loss: 0.701 (data_loss: 0.701, reg_loss: 0.000), lr: 0.000721917412647993\n",
      "step: 200, acc: 0.672, loss: 0.904 (data_loss: 0.904, reg_loss: 0.000), lr: 0.0007167431192660551\n",
      "step: 300, acc: 0.695, loss: 0.664 (data_loss: 0.664, reg_loss: 0.000), lr: 0.0007116424708226587\n",
      "step: 400, acc: 0.734, loss: 0.783 (data_loss: 0.783, reg_loss: 0.000), lr: 0.0007066139061616733\n",
      "step: 468, acc: 0.625, loss: 0.950 (data_loss: 0.950, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "training, acc: 0.688, loss: 0.787 (data_loss: 0.787, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "validation, acc: 0.814, loss: 0.542\n",
      "epoch: 10\n",
      "step: 0, acc: 0.680, loss: 0.693 (data_loss: 0.693, reg_loss: 0.000), lr: 0.0007031854299978904\n",
      "step: 100, acc: 0.672, loss: 0.850 (data_loss: 0.850, reg_loss: 0.000), lr: 0.0006982752601075343\n",
      "step: 200, acc: 0.672, loss: 0.834 (data_loss: 0.834, reg_loss: 0.000), lr: 0.000693433187712364\n",
      "step: 300, acc: 0.742, loss: 0.678 (data_loss: 0.678, reg_loss: 0.000), lr: 0.0006886578059362303\n",
      "step: 400, acc: 0.742, loss: 0.715 (data_loss: 0.715, reg_loss: 0.000), lr: 0.0006839477463921757\n",
      "step: 468, acc: 0.698, loss: 0.808 (data_loss: 0.808, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "training, acc: 0.700, loss: 0.764 (data_loss: 0.764, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "validation, acc: 0.815, loss: 0.525\n",
      "epoch: 1\n",
      "step: 0, acc: 0.117, loss: 2.310 (data_loss: 2.310, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.156, loss: 2.153 (data_loss: 2.153, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 200, acc: 0.195, loss: 1.742 (data_loss: 1.742, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 300, acc: 0.219, loss: 1.699 (data_loss: 1.699, reg_loss: 0.000), lr: 0.0009708737864077671\n",
      "step: 400, acc: 0.383, loss: 1.559 (data_loss: 1.559, reg_loss: 0.000), lr: 0.0009615384615384615\n",
      "step: 468, acc: 0.385, loss: 1.550 (data_loss: 1.550, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "training, acc: 0.221, loss: 1.841 (data_loss: 1.841, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "validation, acc: 0.362, loss: 1.427\n",
      "epoch: 2\n",
      "step: 0, acc: 0.234, loss: 1.591 (data_loss: 1.591, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.367, loss: 1.452 (data_loss: 1.452, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 200, acc: 0.344, loss: 1.414 (data_loss: 1.414, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 300, acc: 0.367, loss: 1.363 (data_loss: 1.363, reg_loss: 0.000), lr: 0.0009285913269570063\n",
      "step: 400, acc: 0.406, loss: 1.413 (data_loss: 1.413, reg_loss: 0.000), lr: 0.0009200478424878093\n",
      "step: 468, acc: 0.344, loss: 1.361 (data_loss: 1.361, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "training, acc: 0.359, loss: 1.398 (data_loss: 1.398, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "validation, acc: 0.408, loss: 1.218\n",
      "epoch: 3\n",
      "step: 0, acc: 0.398, loss: 1.252 (data_loss: 1.252, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.336, loss: 1.382 (data_loss: 1.382, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 200, acc: 0.414, loss: 1.340 (data_loss: 1.340, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 300, acc: 0.453, loss: 1.317 (data_loss: 1.317, reg_loss: 0.000), lr: 0.0008898380494749957\n",
      "step: 400, acc: 0.445, loss: 1.324 (data_loss: 1.324, reg_loss: 0.000), lr: 0.0008819897689186807\n",
      "step: 468, acc: 0.448, loss: 1.188 (data_loss: 1.188, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "training, acc: 0.402, loss: 1.300 (data_loss: 1.300, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "validation, acc: 0.503, loss: 1.108\n",
      "epoch: 4\n",
      "step: 0, acc: 0.500, loss: 1.116 (data_loss: 1.116, reg_loss: 0.000), lr: 0.0008766546857192952\n",
      "step: 100, acc: 0.453, loss: 1.216 (data_loss: 1.216, reg_loss: 0.000), lr: 0.0008690362388111585\n",
      "step: 200, acc: 0.461, loss: 1.190 (data_loss: 1.190, reg_loss: 0.000), lr: 0.0008615490652192642\n",
      "step: 300, acc: 0.523, loss: 1.226 (data_loss: 1.226, reg_loss: 0.000), lr: 0.0008541898009737763\n",
      "step: 400, acc: 0.531, loss: 1.225 (data_loss: 1.225, reg_loss: 0.000), lr: 0.0008469551960701278\n",
      "step: 468, acc: 0.427, loss: 1.130 (data_loss: 1.130, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "training, acc: 0.481, loss: 1.189 (data_loss: 1.189, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "validation, acc: 0.581, loss: 1.003\n",
      "epoch: 5\n",
      "step: 0, acc: 0.625, loss: 1.018 (data_loss: 1.018, reg_loss: 0.000), lr: 0.0008420343550016842\n",
      "step: 100, acc: 0.547, loss: 1.161 (data_loss: 1.161, reg_loss: 0.000), lr: 0.0008350033400133601\n",
      "step: 200, acc: 0.523, loss: 1.151 (data_loss: 1.151, reg_loss: 0.000), lr: 0.0008280887711162637\n",
      "step: 300, acc: 0.539, loss: 1.025 (data_loss: 1.025, reg_loss: 0.000), lr: 0.0008212877792378449\n",
      "step: 400, acc: 0.578, loss: 1.093 (data_loss: 1.093, reg_loss: 0.000), lr: 0.0008145975887911372\n",
      "step: 468, acc: 0.448, loss: 1.144 (data_loss: 1.144, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "training, acc: 0.544, loss: 1.087 (data_loss: 1.087, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "validation, acc: 0.653, loss: 0.849\n",
      "epoch: 6\n",
      "step: 0, acc: 0.570, loss: 0.936 (data_loss: 0.936, reg_loss: 0.000), lr: 0.0008100445524503849\n",
      "step: 100, acc: 0.570, loss: 1.111 (data_loss: 1.111, reg_loss: 0.000), lr: 0.0008035355564483729\n",
      "step: 200, acc: 0.570, loss: 1.037 (data_loss: 1.037, reg_loss: 0.000), lr: 0.0007971303308090873\n",
      "step: 300, acc: 0.578, loss: 0.927 (data_loss: 0.927, reg_loss: 0.000), lr: 0.0007908264136022144\n",
      "step: 400, acc: 0.648, loss: 0.969 (data_loss: 0.969, reg_loss: 0.000), lr: 0.0007846214201647706\n",
      "step: 468, acc: 0.531, loss: 1.152 (data_loss: 1.152, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "training, acc: 0.578, loss: 1.006 (data_loss: 1.006, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "validation, acc: 0.648, loss: 0.798\n",
      "epoch: 7\n",
      "step: 0, acc: 0.625, loss: 0.933 (data_loss: 0.933, reg_loss: 0.000), lr: 0.0007803964413922272\n",
      "step: 100, acc: 0.633, loss: 1.093 (data_loss: 1.093, reg_loss: 0.000), lr: 0.0007743534148985598\n",
      "step: 200, acc: 0.578, loss: 0.824 (data_loss: 0.824, reg_loss: 0.000), lr: 0.0007684032580298141\n",
      "step: 300, acc: 0.633, loss: 0.870 (data_loss: 0.870, reg_loss: 0.000), lr: 0.0007625438462711606\n",
      "step: 400, acc: 0.625, loss: 1.028 (data_loss: 1.028, reg_loss: 0.000), lr: 0.0007567731194187983\n",
      "step: 468, acc: 0.615, loss: 0.928 (data_loss: 0.928, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "training, acc: 0.598, loss: 0.968 (data_loss: 0.968, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "validation, acc: 0.711, loss: 0.763\n",
      "epoch: 8\n",
      "step: 0, acc: 0.688, loss: 0.801 (data_loss: 0.801, reg_loss: 0.000), lr: 0.0007528419784687194\n",
      "step: 100, acc: 0.703, loss: 0.976 (data_loss: 0.976, reg_loss: 0.000), lr: 0.0007472166180975864\n",
      "step: 200, acc: 0.586, loss: 0.932 (data_loss: 0.932, reg_loss: 0.000), lr: 0.0007416747014759327\n",
      "step: 300, acc: 0.641, loss: 0.948 (data_loss: 0.948, reg_loss: 0.000), lr: 0.0007362143856290952\n",
      "step: 400, acc: 0.680, loss: 0.989 (data_loss: 0.989, reg_loss: 0.000), lr: 0.0007308338814587444\n",
      "step: 468, acc: 0.594, loss: 0.953 (data_loss: 0.953, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "training, acc: 0.638, loss: 0.917 (data_loss: 0.917, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "validation, acc: 0.739, loss: 0.693\n",
      "epoch: 9\n",
      "step: 0, acc: 0.672, loss: 0.773 (data_loss: 0.773, reg_loss: 0.000), lr: 0.0007271669575334498\n",
      "step: 100, acc: 0.711, loss: 0.885 (data_loss: 0.885, reg_loss: 0.000), lr: 0.000721917412647993\n",
      "step: 200, acc: 0.719, loss: 0.796 (data_loss: 0.796, reg_loss: 0.000), lr: 0.0007167431192660551\n",
      "step: 300, acc: 0.648, loss: 0.802 (data_loss: 0.802, reg_loss: 0.000), lr: 0.0007116424708226587\n",
      "step: 400, acc: 0.742, loss: 0.981 (data_loss: 0.981, reg_loss: 0.000), lr: 0.0007066139061616733\n",
      "step: 468, acc: 0.615, loss: 0.838 (data_loss: 0.838, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "training, acc: 0.659, loss: 0.880 (data_loss: 0.880, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "validation, acc: 0.748, loss: 0.658\n",
      "epoch: 10\n",
      "step: 0, acc: 0.672, loss: 0.757 (data_loss: 0.757, reg_loss: 0.000), lr: 0.0007031854299978904\n",
      "step: 100, acc: 0.688, loss: 0.901 (data_loss: 0.901, reg_loss: 0.000), lr: 0.0006982752601075343\n",
      "step: 200, acc: 0.641, loss: 0.915 (data_loss: 0.915, reg_loss: 0.000), lr: 0.000693433187712364\n",
      "step: 300, acc: 0.625, loss: 0.793 (data_loss: 0.793, reg_loss: 0.000), lr: 0.0006886578059362303\n",
      "step: 400, acc: 0.711, loss: 0.859 (data_loss: 0.859, reg_loss: 0.000), lr: 0.0006839477463921757\n",
      "step: 468, acc: 0.698, loss: 0.842 (data_loss: 0.842, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "training, acc: 0.669, loss: 0.851 (data_loss: 0.851, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "validation, acc: 0.757, loss: 0.644\n"
     ]
    }
   ],
   "source": [
    "# Train models with sigmoid activation function\n",
    "Train_Model_Sigmoid(X=X, y=y, X_test=X_test, y_test=y_test, model_number=\"_sigmoid_layers1\", number_of_layers=1)\n",
    "Train_Model_Sigmoid(X=X, y=y, X_test=X_test, y_test=y_test, model_number=\"_sigmoid_layers2\", number_of_layers=2)\n",
    "Train_Model_Sigmoid(X=X, y=y, X_test=X_test, y_test=y_test, model_number=\"_sigmoid_layers3\", number_of_layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with 1 layer and sigmoid activation function:\n",
      "validation, acc: 0.815, loss: 0.514\n",
      "Model with 2 layers and sigmoid activation function:\n",
      "validation, acc: 0.815, loss: 0.525\n",
      "Model with 3 layers and sigmoid activation function:\n",
      "validation, acc: 0.757, loss: 0.644\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models\n",
    "print(\"Model with 1 layer and sigmoid activation function:\")\n",
    "model = Model.load(\"fashion_mnist_sigmoid_layers1.model\")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Model with 2 layers and sigmoid activation function:\")\n",
    "model = Model.load(\"fashion_mnist_sigmoid_layers2.model\")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Model with 3 layers and sigmoid activation function:\")\n",
    "model = Model.load(\"fashion_mnist_sigmoid_layers3.model\")\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0, acc: 0.125, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.5\n",
      "step: 100, acc: 0.094, loss: 2.396 (data_loss: 2.396, reg_loss: 0.000), lr: 0.49504950495049505\n",
      "step: 200, acc: 0.047, loss: 2.340 (data_loss: 2.340, reg_loss: 0.000), lr: 0.49019607843137253\n",
      "step: 300, acc: 0.078, loss: 2.311 (data_loss: 2.311, reg_loss: 0.000), lr: 0.4854368932038835\n",
      "step: 400, acc: 0.109, loss: 2.330 (data_loss: 2.330, reg_loss: 0.000), lr: 0.4807692307692307\n",
      "step: 468, acc: 0.104, loss: 2.337 (data_loss: 2.337, reg_loss: 0.000), lr: 0.47764615972487584\n",
      "training, acc: 0.101, loss: 2.438 (data_loss: 2.438, reg_loss: 0.000), lr: 0.47764615972487584\n",
      "validation, acc: 0.100, loss: 2.337\n",
      "epoch: 2\n",
      "step: 0, acc: 0.094, loss: 2.354 (data_loss: 2.354, reg_loss: 0.000), lr: 0.47760053491259913\n",
      "step: 100, acc: 0.094, loss: 2.390 (data_loss: 2.390, reg_loss: 0.000), lr: 0.473081653893462\n",
      "step: 200, acc: 0.047, loss: 2.340 (data_loss: 2.340, reg_loss: 0.000), lr: 0.46864748336301437\n",
      "step: 300, acc: 0.078, loss: 2.311 (data_loss: 2.311, reg_loss: 0.000), lr: 0.46429566347850315\n",
      "step: 400, acc: 0.109, loss: 2.329 (data_loss: 2.329, reg_loss: 0.000), lr: 0.4600239212439047\n",
      "step: 468, acc: 0.104, loss: 2.339 (data_loss: 2.339, reg_loss: 0.000), lr: 0.4571637560574197\n",
      "training, acc: 0.100, loss: 2.324 (data_loss: 2.324, reg_loss: 0.000), lr: 0.4571637560574197\n",
      "validation, acc: 0.100, loss: 2.340\n",
      "epoch: 3\n",
      "step: 0, acc: 0.070, loss: 2.353 (data_loss: 2.353, reg_loss: 0.000), lr: 0.45712196013896506\n",
      "step: 100, acc: 0.094, loss: 2.388 (data_loss: 2.388, reg_loss: 0.000), lr: 0.45298061242978793\n",
      "step: 200, acc: 0.047, loss: 2.339 (data_loss: 2.339, reg_loss: 0.000), lr: 0.44891362901777704\n",
      "step: 300, acc: 0.078, loss: 2.310 (data_loss: 2.310, reg_loss: 0.000), lr: 0.44491902473749784\n",
      "step: 400, acc: 0.109, loss: 2.327 (data_loss: 2.327, reg_loss: 0.000), lr: 0.4409948844593403\n",
      "step: 468, acc: 0.104, loss: 2.341 (data_loss: 2.341, reg_loss: 0.000), lr: 0.43836577240049096\n",
      "training, acc: 0.100, loss: 2.324 (data_loss: 2.324, reg_loss: 0.000), lr: 0.43836577240049096\n",
      "validation, acc: 0.100, loss: 2.337\n",
      "epoch: 4\n",
      "step: 0, acc: 0.070, loss: 2.354 (data_loss: 2.354, reg_loss: 0.000), lr: 0.4383273428596476\n",
      "step: 100, acc: 0.094, loss: 2.387 (data_loss: 2.387, reg_loss: 0.000), lr: 0.4345181194055792\n",
      "step: 200, acc: 0.047, loss: 2.339 (data_loss: 2.339, reg_loss: 0.000), lr: 0.4307745326096321\n",
      "step: 300, acc: 0.148, loss: 2.309 (data_loss: 2.309, reg_loss: 0.000), lr: 0.42709490048688814\n",
      "step: 400, acc: 0.109, loss: 2.325 (data_loss: 2.325, reg_loss: 0.000), lr: 0.4234775980350639\n",
      "step: 468, acc: 0.104, loss: 2.341 (data_loss: 2.341, reg_loss: 0.000), lr: 0.42105263157894735\n",
      "training, acc: 0.101, loss: 2.323 (data_loss: 2.323, reg_loss: 0.000), lr: 0.42105263157894735\n",
      "validation, acc: 0.100, loss: 2.338\n",
      "epoch: 5\n",
      "step: 0, acc: 0.070, loss: 2.354 (data_loss: 2.354, reg_loss: 0.000), lr: 0.42101717750084205\n",
      "step: 100, acc: 0.094, loss: 2.385 (data_loss: 2.385, reg_loss: 0.000), lr: 0.41750167000668004\n",
      "step: 200, acc: 0.047, loss: 2.339 (data_loss: 2.339, reg_loss: 0.000), lr: 0.41404438555813183\n",
      "step: 300, acc: 0.148, loss: 2.308 (data_loss: 2.308, reg_loss: 0.000), lr: 0.41064388961892245\n",
      "step: 400, acc: 0.109, loss: 2.324 (data_loss: 2.324, reg_loss: 0.000), lr: 0.4072987943955686\n",
      "step: 468, acc: 0.104, loss: 2.341 (data_loss: 2.341, reg_loss: 0.000), lr: 0.4050550874918989\n",
      "training, acc: 0.101, loss: 2.322 (data_loss: 2.322, reg_loss: 0.000), lr: 0.4050550874918989\n",
      "validation, acc: 0.100, loss: 2.339\n",
      "epoch: 6\n",
      "step: 0, acc: 0.070, loss: 2.355 (data_loss: 2.355, reg_loss: 0.000), lr: 0.40502227622519243\n",
      "step: 100, acc: 0.094, loss: 2.384 (data_loss: 2.384, reg_loss: 0.000), lr: 0.40176777822418647\n",
      "step: 200, acc: 0.047, loss: 2.338 (data_loss: 2.338, reg_loss: 0.000), lr: 0.3985651654045437\n",
      "step: 300, acc: 0.148, loss: 2.307 (data_loss: 2.307, reg_loss: 0.000), lr: 0.3954132068011072\n",
      "step: 400, acc: 0.109, loss: 2.323 (data_loss: 2.323, reg_loss: 0.000), lr: 0.39231071008238527\n",
      "step: 468, acc: 0.104, loss: 2.340 (data_loss: 2.340, reg_loss: 0.000), lr: 0.3902286740029658\n",
      "training, acc: 0.101, loss: 2.321 (data_loss: 2.321, reg_loss: 0.000), lr: 0.3902286740029658\n",
      "validation, acc: 0.100, loss: 2.339\n",
      "epoch: 7\n",
      "step: 0, acc: 0.070, loss: 2.355 (data_loss: 2.355, reg_loss: 0.000), lr: 0.3901982206961136\n",
      "step: 100, acc: 0.094, loss: 2.382 (data_loss: 2.382, reg_loss: 0.000), lr: 0.3871767074492799\n",
      "step: 200, acc: 0.047, loss: 2.338 (data_loss: 2.338, reg_loss: 0.000), lr: 0.384201629014907\n",
      "step: 300, acc: 0.148, loss: 2.306 (data_loss: 2.306, reg_loss: 0.000), lr: 0.38127192313558034\n",
      "step: 400, acc: 0.109, loss: 2.322 (data_loss: 2.322, reg_loss: 0.000), lr: 0.3783865597093991\n",
      "step: 468, acc: 0.104, loss: 2.339 (data_loss: 2.339, reg_loss: 0.000), lr: 0.37644932992019275\n",
      "training, acc: 0.101, loss: 2.321 (data_loss: 2.321, reg_loss: 0.000), lr: 0.37644932992019275\n",
      "validation, acc: 0.100, loss: 2.339\n",
      "epoch: 8\n",
      "step: 0, acc: 0.070, loss: 2.355 (data_loss: 2.355, reg_loss: 0.000), lr: 0.3764209892343597\n",
      "step: 100, acc: 0.094, loss: 2.380 (data_loss: 2.380, reg_loss: 0.000), lr: 0.3736083090487932\n",
      "step: 200, acc: 0.047, loss: 2.338 (data_loss: 2.338, reg_loss: 0.000), lr: 0.3708373507379663\n",
      "step: 300, acc: 0.148, loss: 2.306 (data_loss: 2.306, reg_loss: 0.000), lr: 0.36810719281454757\n",
      "step: 400, acc: 0.062, loss: 2.322 (data_loss: 2.322, reg_loss: 0.000), lr: 0.3654169407293722\n",
      "step: 468, acc: 0.104, loss: 2.337 (data_loss: 2.337, reg_loss: 0.000), lr: 0.3636099192785979\n",
      "training, acc: 0.101, loss: 2.320 (data_loss: 2.320, reg_loss: 0.000), lr: 0.3636099192785979\n",
      "validation, acc: 0.100, loss: 2.339\n",
      "epoch: 9\n",
      "step: 0, acc: 0.070, loss: 2.354 (data_loss: 2.354, reg_loss: 0.000), lr: 0.36358347876672487\n",
      "step: 100, acc: 0.094, loss: 2.378 (data_loss: 2.378, reg_loss: 0.000), lr: 0.3609587063239965\n",
      "step: 200, acc: 0.047, loss: 2.338 (data_loss: 2.338, reg_loss: 0.000), lr: 0.3583715596330275\n",
      "step: 300, acc: 0.148, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.3558212354113294\n",
      "step: 400, acc: 0.062, loss: 2.321 (data_loss: 2.321, reg_loss: 0.000), lr: 0.35330695308083665\n",
      "step: 468, acc: 0.104, loss: 2.334 (data_loss: 2.334, reg_loss: 0.000), lr: 0.3516174402250351\n",
      "training, acc: 0.101, loss: 2.320 (data_loss: 2.320, reg_loss: 0.000), lr: 0.3516174402250351\n",
      "validation, acc: 0.100, loss: 2.338\n",
      "epoch: 10\n",
      "step: 0, acc: 0.070, loss: 2.353 (data_loss: 2.353, reg_loss: 0.000), lr: 0.35159271499894523\n",
      "step: 100, acc: 0.094, loss: 2.375 (data_loss: 2.375, reg_loss: 0.000), lr: 0.34913763005376713\n",
      "step: 200, acc: 0.047, loss: 2.339 (data_loss: 2.339, reg_loss: 0.000), lr: 0.346716593856182\n",
      "step: 300, acc: 0.148, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.34432890296811514\n",
      "step: 400, acc: 0.062, loss: 2.321 (data_loss: 2.321, reg_loss: 0.000), lr: 0.34197387319608785\n",
      "step: 468, acc: 0.104, loss: 2.332 (data_loss: 2.332, reg_loss: 0.000), lr: 0.3403907686023555\n",
      "training, acc: 0.101, loss: 2.319 (data_loss: 2.319, reg_loss: 0.000), lr: 0.3403907686023555\n",
      "validation, acc: 0.100, loss: 2.337\n",
      "epoch: 1\n",
      "step: 0, acc: 0.062, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.1\n",
      "step: 100, acc: 0.148, loss: 2.323 (data_loss: 2.323, reg_loss: 0.000), lr: 0.09900990099009901\n",
      "step: 200, acc: 0.148, loss: 2.304 (data_loss: 2.304, reg_loss: 0.000), lr: 0.09803921568627451\n",
      "step: 300, acc: 0.125, loss: 2.296 (data_loss: 2.296, reg_loss: 0.000), lr: 0.09708737864077671\n",
      "step: 400, acc: 0.180, loss: 2.290 (data_loss: 2.290, reg_loss: 0.000), lr: 0.09615384615384615\n",
      "step: 468, acc: 0.062, loss: 2.308 (data_loss: 2.308, reg_loss: 0.000), lr: 0.09552923194497517\n",
      "training, acc: 0.101, loss: 2.411 (data_loss: 2.411, reg_loss: 0.000), lr: 0.09552923194497517\n",
      "validation, acc: 0.100, loss: 2.317\n",
      "epoch: 2\n",
      "step: 0, acc: 0.102, loss: 2.309 (data_loss: 2.309, reg_loss: 0.000), lr: 0.09552010698251984\n",
      "step: 100, acc: 0.148, loss: 2.321 (data_loss: 2.321, reg_loss: 0.000), lr: 0.09461633077869241\n",
      "step: 200, acc: 0.148, loss: 2.304 (data_loss: 2.304, reg_loss: 0.000), lr: 0.09372949667260289\n",
      "step: 300, acc: 0.125, loss: 2.295 (data_loss: 2.295, reg_loss: 0.000), lr: 0.09285913269570063\n",
      "step: 400, acc: 0.180, loss: 2.289 (data_loss: 2.289, reg_loss: 0.000), lr: 0.09200478424878095\n",
      "step: 468, acc: 0.062, loss: 2.308 (data_loss: 2.308, reg_loss: 0.000), lr: 0.09143275121148395\n",
      "training, acc: 0.101, loss: 2.312 (data_loss: 2.312, reg_loss: 0.000), lr: 0.09143275121148395\n",
      "validation, acc: 0.100, loss: 2.329\n",
      "epoch: 3\n",
      "step: 0, acc: 0.102, loss: 2.308 (data_loss: 2.308, reg_loss: 0.000), lr: 0.09142439202779301\n",
      "step: 100, acc: 0.148, loss: 2.321 (data_loss: 2.321, reg_loss: 0.000), lr: 0.0905961224859576\n",
      "step: 200, acc: 0.148, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.08978272580355541\n",
      "step: 300, acc: 0.125, loss: 2.295 (data_loss: 2.295, reg_loss: 0.000), lr: 0.08898380494749958\n",
      "step: 400, acc: 0.180, loss: 2.289 (data_loss: 2.289, reg_loss: 0.000), lr: 0.08819897689186806\n",
      "step: 468, acc: 0.062, loss: 2.308 (data_loss: 2.308, reg_loss: 0.000), lr: 0.0876731544800982\n",
      "training, acc: 0.101, loss: 2.308 (data_loss: 2.308, reg_loss: 0.000), lr: 0.0876731544800982\n",
      "validation, acc: 0.100, loss: 2.310\n",
      "epoch: 4\n",
      "step: 0, acc: 0.102, loss: 2.307 (data_loss: 2.307, reg_loss: 0.000), lr: 0.08766546857192953\n",
      "step: 100, acc: 0.148, loss: 2.320 (data_loss: 2.320, reg_loss: 0.000), lr: 0.08690362388111585\n",
      "step: 200, acc: 0.148, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.08615490652192642\n",
      "step: 300, acc: 0.125, loss: 2.294 (data_loss: 2.294, reg_loss: 0.000), lr: 0.08541898009737764\n",
      "step: 400, acc: 0.180, loss: 2.289 (data_loss: 2.289, reg_loss: 0.000), lr: 0.08469551960701278\n",
      "step: 468, acc: 0.062, loss: 2.308 (data_loss: 2.308, reg_loss: 0.000), lr: 0.08421052631578947\n",
      "training, acc: 0.101, loss: 2.308 (data_loss: 2.308, reg_loss: 0.000), lr: 0.08421052631578947\n",
      "validation, acc: 0.100, loss: 2.309\n",
      "epoch: 5\n",
      "step: 0, acc: 0.102, loss: 2.307 (data_loss: 2.307, reg_loss: 0.000), lr: 0.08420343550016841\n",
      "step: 100, acc: 0.148, loss: 2.319 (data_loss: 2.319, reg_loss: 0.000), lr: 0.08350033400133601\n",
      "step: 200, acc: 0.148, loss: 2.302 (data_loss: 2.302, reg_loss: 0.000), lr: 0.08280887711162638\n",
      "step: 300, acc: 0.125, loss: 2.293 (data_loss: 2.293, reg_loss: 0.000), lr: 0.0821287779237845\n",
      "step: 400, acc: 0.180, loss: 2.289 (data_loss: 2.289, reg_loss: 0.000), lr: 0.08145975887911372\n",
      "step: 468, acc: 0.062, loss: 2.309 (data_loss: 2.309, reg_loss: 0.000), lr: 0.08101101749837979\n",
      "training, acc: 0.101, loss: 2.307 (data_loss: 2.307, reg_loss: 0.000), lr: 0.08101101749837979\n",
      "validation, acc: 0.100, loss: 2.309\n",
      "epoch: 6\n",
      "step: 0, acc: 0.102, loss: 2.306 (data_loss: 2.306, reg_loss: 0.000), lr: 0.08100445524503849\n",
      "step: 100, acc: 0.148, loss: 2.319 (data_loss: 2.319, reg_loss: 0.000), lr: 0.0803535556448373\n",
      "step: 200, acc: 0.148, loss: 2.302 (data_loss: 2.302, reg_loss: 0.000), lr: 0.07971303308090874\n",
      "step: 300, acc: 0.125, loss: 2.293 (data_loss: 2.293, reg_loss: 0.000), lr: 0.07908264136022145\n",
      "step: 400, acc: 0.180, loss: 2.289 (data_loss: 2.289, reg_loss: 0.000), lr: 0.07846214201647705\n",
      "step: 468, acc: 0.062, loss: 2.309 (data_loss: 2.309, reg_loss: 0.000), lr: 0.07804573480059317\n",
      "training, acc: 0.101, loss: 2.307 (data_loss: 2.307, reg_loss: 0.000), lr: 0.07804573480059317\n",
      "validation, acc: 0.100, loss: 2.309\n",
      "epoch: 7\n",
      "step: 0, acc: 0.102, loss: 2.306 (data_loss: 2.306, reg_loss: 0.000), lr: 0.07803964413922272\n",
      "step: 100, acc: 0.148, loss: 2.318 (data_loss: 2.318, reg_loss: 0.000), lr: 0.07743534148985598\n",
      "step: 200, acc: 0.148, loss: 2.301 (data_loss: 2.301, reg_loss: 0.000), lr: 0.0768403258029814\n",
      "step: 300, acc: 0.125, loss: 2.292 (data_loss: 2.292, reg_loss: 0.000), lr: 0.07625438462711608\n",
      "step: 400, acc: 0.180, loss: 2.289 (data_loss: 2.289, reg_loss: 0.000), lr: 0.07567731194187982\n",
      "step: 468, acc: 0.062, loss: 2.309 (data_loss: 2.309, reg_loss: 0.000), lr: 0.07528986598403856\n",
      "training, acc: 0.101, loss: 2.307 (data_loss: 2.307, reg_loss: 0.000), lr: 0.07528986598403856\n",
      "validation, acc: 0.100, loss: 2.309\n",
      "epoch: 8\n",
      "step: 0, acc: 0.102, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.07528419784687194\n",
      "step: 100, acc: 0.148, loss: 2.317 (data_loss: 2.317, reg_loss: 0.000), lr: 0.07472166180975864\n",
      "step: 200, acc: 0.148, loss: 2.301 (data_loss: 2.301, reg_loss: 0.000), lr: 0.07416747014759327\n",
      "step: 300, acc: 0.125, loss: 2.292 (data_loss: 2.292, reg_loss: 0.000), lr: 0.07362143856290952\n",
      "step: 400, acc: 0.180, loss: 2.289 (data_loss: 2.289, reg_loss: 0.000), lr: 0.07308338814587444\n",
      "step: 468, acc: 0.062, loss: 2.309 (data_loss: 2.309, reg_loss: 0.000), lr: 0.07272198385571958\n",
      "training, acc: 0.100, loss: 2.307 (data_loss: 2.307, reg_loss: 0.000), lr: 0.07272198385571958\n",
      "validation, acc: 0.100, loss: 2.309\n",
      "epoch: 9\n",
      "step: 0, acc: 0.102, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.07271669575334498\n",
      "step: 100, acc: 0.148, loss: 2.317 (data_loss: 2.317, reg_loss: 0.000), lr: 0.07219174126479931\n",
      "step: 200, acc: 0.148, loss: 2.300 (data_loss: 2.300, reg_loss: 0.000), lr: 0.0716743119266055\n",
      "step: 300, acc: 0.125, loss: 2.292 (data_loss: 2.292, reg_loss: 0.000), lr: 0.07116424708226587\n",
      "step: 400, acc: 0.180, loss: 2.289 (data_loss: 2.289, reg_loss: 0.000), lr: 0.07066139061616733\n",
      "step: 468, acc: 0.062, loss: 2.309 (data_loss: 2.309, reg_loss: 0.000), lr: 0.07032348804500703\n",
      "training, acc: 0.101, loss: 2.307 (data_loss: 2.307, reg_loss: 0.000), lr: 0.07032348804500703\n",
      "validation, acc: 0.100, loss: 2.309\n",
      "epoch: 10\n",
      "step: 0, acc: 0.102, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.07031854299978905\n",
      "step: 100, acc: 0.148, loss: 2.316 (data_loss: 2.316, reg_loss: 0.000), lr: 0.06982752601075343\n",
      "step: 200, acc: 0.148, loss: 2.300 (data_loss: 2.300, reg_loss: 0.000), lr: 0.0693433187712364\n",
      "step: 300, acc: 0.125, loss: 2.291 (data_loss: 2.291, reg_loss: 0.000), lr: 0.06886578059362303\n",
      "step: 400, acc: 0.180, loss: 2.289 (data_loss: 2.289, reg_loss: 0.000), lr: 0.06839477463921757\n",
      "step: 468, acc: 0.062, loss: 2.309 (data_loss: 2.309, reg_loss: 0.000), lr: 0.0680781537204711\n",
      "training, acc: 0.101, loss: 2.307 (data_loss: 2.307, reg_loss: 0.000), lr: 0.0680781537204711\n",
      "validation, acc: 0.100, loss: 2.309\n",
      "epoch: 1\n",
      "step: 0, acc: 0.117, loss: 2.302 (data_loss: 2.302, reg_loss: 0.000), lr: 0.05\n",
      "step: 100, acc: 0.148, loss: 2.308 (data_loss: 2.308, reg_loss: 0.000), lr: 0.04950495049504951\n",
      "step: 200, acc: 0.094, loss: 2.300 (data_loss: 2.300, reg_loss: 0.000), lr: 0.049019607843137254\n",
      "step: 300, acc: 0.125, loss: 2.293 (data_loss: 2.293, reg_loss: 0.000), lr: 0.048543689320388356\n",
      "step: 400, acc: 0.180, loss: 2.289 (data_loss: 2.289, reg_loss: 0.000), lr: 0.04807692307692307\n",
      "step: 468, acc: 0.062, loss: 2.308 (data_loss: 2.308, reg_loss: 0.000), lr: 0.04776461597248759\n",
      "training, acc: 0.101, loss: 2.408 (data_loss: 2.408, reg_loss: 0.000), lr: 0.04776461597248759\n",
      "validation, acc: 0.100, loss: 2.310\n",
      "epoch: 2\n",
      "step: 0, acc: 0.102, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.04776005349125992\n",
      "step: 100, acc: 0.148, loss: 2.309 (data_loss: 2.309, reg_loss: 0.000), lr: 0.04730816538934621\n",
      "step: 200, acc: 0.148, loss: 2.300 (data_loss: 2.300, reg_loss: 0.000), lr: 0.04686474833630144\n",
      "step: 300, acc: 0.125, loss: 2.293 (data_loss: 2.293, reg_loss: 0.000), lr: 0.046429566347850316\n",
      "step: 400, acc: 0.180, loss: 2.289 (data_loss: 2.289, reg_loss: 0.000), lr: 0.04600239212439047\n",
      "step: 468, acc: 0.062, loss: 2.308 (data_loss: 2.308, reg_loss: 0.000), lr: 0.045716375605741974\n",
      "training, acc: 0.101, loss: 2.306 (data_loss: 2.306, reg_loss: 0.000), lr: 0.045716375605741974\n",
      "validation, acc: 0.100, loss: 2.310\n",
      "epoch: 3\n",
      "step: 0, acc: 0.102, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.045712196013896506\n",
      "step: 100, acc: 0.148, loss: 2.308 (data_loss: 2.308, reg_loss: 0.000), lr: 0.0452980612429788\n",
      "step: 200, acc: 0.094, loss: 2.300 (data_loss: 2.300, reg_loss: 0.000), lr: 0.044891362901777705\n",
      "step: 300, acc: 0.125, loss: 2.293 (data_loss: 2.293, reg_loss: 0.000), lr: 0.04449190247374979\n",
      "step: 400, acc: 0.180, loss: 2.289 (data_loss: 2.289, reg_loss: 0.000), lr: 0.04409948844593403\n",
      "step: 468, acc: 0.062, loss: 2.308 (data_loss: 2.308, reg_loss: 0.000), lr: 0.0438365772400491\n",
      "training, acc: 0.101, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.0438365772400491\n",
      "validation, acc: 0.100, loss: 2.310\n",
      "epoch: 4\n",
      "step: 0, acc: 0.102, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.043832734285964764\n",
      "step: 100, acc: 0.148, loss: 2.308 (data_loss: 2.308, reg_loss: 0.000), lr: 0.04345181194055792\n",
      "step: 200, acc: 0.094, loss: 2.301 (data_loss: 2.301, reg_loss: 0.000), lr: 0.04307745326096321\n",
      "step: 300, acc: 0.148, loss: 2.294 (data_loss: 2.294, reg_loss: 0.000), lr: 0.04270949004868882\n",
      "step: 400, acc: 0.180, loss: 2.289 (data_loss: 2.289, reg_loss: 0.000), lr: 0.04234775980350639\n",
      "step: 468, acc: 0.062, loss: 2.308 (data_loss: 2.308, reg_loss: 0.000), lr: 0.042105263157894736\n",
      "training, acc: 0.101, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.042105263157894736\n",
      "validation, acc: 0.100, loss: 2.310\n",
      "epoch: 5\n",
      "step: 0, acc: 0.102, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.04210171775008421\n",
      "step: 100, acc: 0.148, loss: 2.308 (data_loss: 2.308, reg_loss: 0.000), lr: 0.041750167000668005\n",
      "step: 200, acc: 0.094, loss: 2.301 (data_loss: 2.301, reg_loss: 0.000), lr: 0.04140443855581319\n",
      "step: 300, acc: 0.148, loss: 2.294 (data_loss: 2.294, reg_loss: 0.000), lr: 0.04106438896189225\n",
      "step: 400, acc: 0.180, loss: 2.289 (data_loss: 2.289, reg_loss: 0.000), lr: 0.04072987943955686\n",
      "step: 468, acc: 0.062, loss: 2.308 (data_loss: 2.308, reg_loss: 0.000), lr: 0.04050550874918989\n",
      "training, acc: 0.101, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.04050550874918989\n",
      "validation, acc: 0.100, loss: 2.309\n",
      "epoch: 6\n",
      "step: 0, acc: 0.102, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.040502227622519246\n",
      "step: 100, acc: 0.148, loss: 2.307 (data_loss: 2.307, reg_loss: 0.000), lr: 0.04017677782241865\n",
      "step: 200, acc: 0.094, loss: 2.302 (data_loss: 2.302, reg_loss: 0.000), lr: 0.03985651654045437\n",
      "step: 300, acc: 0.148, loss: 2.295 (data_loss: 2.295, reg_loss: 0.000), lr: 0.03954132068011072\n",
      "step: 400, acc: 0.180, loss: 2.290 (data_loss: 2.290, reg_loss: 0.000), lr: 0.03923107100823853\n",
      "step: 468, acc: 0.062, loss: 2.307 (data_loss: 2.307, reg_loss: 0.000), lr: 0.03902286740029658\n",
      "training, acc: 0.101, loss: 2.307 (data_loss: 2.307, reg_loss: 0.000), lr: 0.03902286740029658\n",
      "validation, acc: 0.100, loss: 2.326\n",
      "epoch: 7\n",
      "step: 0, acc: 0.102, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.03901982206961136\n",
      "step: 100, acc: 0.148, loss: 2.307 (data_loss: 2.307, reg_loss: 0.000), lr: 0.03871767074492799\n",
      "step: 200, acc: 0.094, loss: 2.302 (data_loss: 2.302, reg_loss: 0.000), lr: 0.0384201629014907\n",
      "step: 300, acc: 0.148, loss: 2.295 (data_loss: 2.295, reg_loss: 0.000), lr: 0.03812719231355804\n",
      "step: 400, acc: 0.180, loss: 2.289 (data_loss: 2.289, reg_loss: 0.000), lr: 0.03783865597093991\n",
      "step: 468, acc: 0.062, loss: 2.307 (data_loss: 2.307, reg_loss: 0.000), lr: 0.03764493299201928\n",
      "training, acc: 0.101, loss: 2.306 (data_loss: 2.306, reg_loss: 0.000), lr: 0.03764493299201928\n",
      "validation, acc: 0.100, loss: 2.310\n",
      "epoch: 8\n",
      "step: 0, acc: 0.102, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.03764209892343597\n",
      "step: 100, acc: 0.148, loss: 2.307 (data_loss: 2.307, reg_loss: 0.000), lr: 0.03736083090487932\n",
      "step: 200, acc: 0.094, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.037083735073796635\n",
      "step: 300, acc: 0.148, loss: 2.296 (data_loss: 2.296, reg_loss: 0.000), lr: 0.03681071928145476\n",
      "step: 400, acc: 0.180, loss: 2.289 (data_loss: 2.289, reg_loss: 0.000), lr: 0.03654169407293722\n",
      "step: 468, acc: 0.062, loss: 2.307 (data_loss: 2.307, reg_loss: 0.000), lr: 0.03636099192785979\n",
      "training, acc: 0.100, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.03636099192785979\n",
      "validation, acc: 0.100, loss: 2.309\n",
      "epoch: 9\n",
      "step: 0, acc: 0.102, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.03635834787667249\n",
      "step: 100, acc: 0.148, loss: 2.306 (data_loss: 2.306, reg_loss: 0.000), lr: 0.036095870632399656\n",
      "step: 200, acc: 0.094, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.03583715596330275\n",
      "step: 300, acc: 0.148, loss: 2.296 (data_loss: 2.296, reg_loss: 0.000), lr: 0.03558212354113294\n",
      "step: 400, acc: 0.180, loss: 2.289 (data_loss: 2.289, reg_loss: 0.000), lr: 0.035330695308083665\n",
      "step: 468, acc: 0.062, loss: 2.307 (data_loss: 2.307, reg_loss: 0.000), lr: 0.035161744022503515\n",
      "training, acc: 0.101, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.035161744022503515\n",
      "validation, acc: 0.100, loss: 2.309\n",
      "epoch: 10\n",
      "step: 0, acc: 0.102, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.03515927149989453\n",
      "step: 100, acc: 0.148, loss: 2.306 (data_loss: 2.306, reg_loss: 0.000), lr: 0.03491376300537671\n",
      "step: 200, acc: 0.094, loss: 2.304 (data_loss: 2.304, reg_loss: 0.000), lr: 0.0346716593856182\n",
      "step: 300, acc: 0.148, loss: 2.296 (data_loss: 2.296, reg_loss: 0.000), lr: 0.034432890296811514\n",
      "step: 400, acc: 0.180, loss: 2.289 (data_loss: 2.289, reg_loss: 0.000), lr: 0.03419738731960879\n",
      "step: 468, acc: 0.062, loss: 2.307 (data_loss: 2.307, reg_loss: 0.000), lr: 0.03403907686023555\n",
      "training, acc: 0.101, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.03403907686023555\n",
      "validation, acc: 0.100, loss: 2.309\n",
      "epoch: 1\n",
      "step: 0, acc: 0.156, loss: 2.302 (data_loss: 2.302, reg_loss: 0.000), lr: 0.01\n",
      "step: 100, acc: 0.211, loss: 2.059 (data_loss: 2.059, reg_loss: 0.000), lr: 0.009900990099009901\n",
      "step: 200, acc: 0.141, loss: 2.114 (data_loss: 2.114, reg_loss: 0.000), lr: 0.00980392156862745\n",
      "step: 300, acc: 0.141, loss: 2.154 (data_loss: 2.154, reg_loss: 0.000), lr: 0.00970873786407767\n",
      "step: 400, acc: 0.227, loss: 2.195 (data_loss: 2.195, reg_loss: 0.000), lr: 0.009615384615384614\n",
      "step: 468, acc: 0.135, loss: 2.291 (data_loss: 2.291, reg_loss: 0.000), lr: 0.009552923194497517\n",
      "training, acc: 0.189, loss: 2.124 (data_loss: 2.124, reg_loss: 0.000), lr: 0.009552923194497517\n",
      "validation, acc: 0.101, loss: 2.331\n",
      "epoch: 2\n",
      "step: 0, acc: 0.055, loss: 2.362 (data_loss: 2.362, reg_loss: 0.000), lr: 0.009552010698251982\n",
      "step: 100, acc: 0.070, loss: 2.302 (data_loss: 2.302, reg_loss: 0.000), lr: 0.009461633077869241\n",
      "step: 200, acc: 0.055, loss: 2.310 (data_loss: 2.310, reg_loss: 0.000), lr: 0.009372949667260287\n",
      "step: 300, acc: 0.102, loss: 2.304 (data_loss: 2.304, reg_loss: 0.000), lr: 0.009285913269570064\n",
      "step: 400, acc: 0.180, loss: 2.294 (data_loss: 2.294, reg_loss: 0.000), lr: 0.009200478424878094\n",
      "step: 468, acc: 0.135, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.009143275121148394\n",
      "training, acc: 0.101, loss: 2.309 (data_loss: 2.309, reg_loss: 0.000), lr: 0.009143275121148394\n",
      "validation, acc: 0.100, loss: 2.305\n",
      "epoch: 3\n",
      "step: 0, acc: 0.055, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.0091424392027793\n",
      "step: 100, acc: 0.070, loss: 2.302 (data_loss: 2.302, reg_loss: 0.000), lr: 0.009059612248595758\n",
      "step: 200, acc: 0.055, loss: 2.310 (data_loss: 2.310, reg_loss: 0.000), lr: 0.00897827258035554\n",
      "step: 300, acc: 0.102, loss: 2.304 (data_loss: 2.304, reg_loss: 0.000), lr: 0.008898380494749957\n",
      "step: 400, acc: 0.180, loss: 2.295 (data_loss: 2.295, reg_loss: 0.000), lr: 0.008819897689186807\n",
      "step: 468, acc: 0.135, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.00876731544800982\n",
      "training, acc: 0.101, loss: 2.307 (data_loss: 2.307, reg_loss: 0.000), lr: 0.00876731544800982\n",
      "validation, acc: 0.100, loss: 2.305\n",
      "epoch: 4\n",
      "step: 0, acc: 0.055, loss: 2.304 (data_loss: 2.304, reg_loss: 0.000), lr: 0.008766546857192952\n",
      "step: 100, acc: 0.070, loss: 2.302 (data_loss: 2.302, reg_loss: 0.000), lr: 0.008690362388111585\n",
      "step: 200, acc: 0.055, loss: 2.310 (data_loss: 2.310, reg_loss: 0.000), lr: 0.008615490652192642\n",
      "step: 300, acc: 0.102, loss: 2.304 (data_loss: 2.304, reg_loss: 0.000), lr: 0.008541898009737764\n",
      "step: 400, acc: 0.180, loss: 2.295 (data_loss: 2.295, reg_loss: 0.000), lr: 0.008469551960701278\n",
      "step: 468, acc: 0.135, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.008421052631578947\n",
      "training, acc: 0.101, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.008421052631578947\n",
      "validation, acc: 0.101, loss: 2.306\n",
      "epoch: 5\n",
      "step: 0, acc: 0.055, loss: 2.304 (data_loss: 2.304, reg_loss: 0.000), lr: 0.008420343550016841\n",
      "step: 100, acc: 0.070, loss: 2.302 (data_loss: 2.302, reg_loss: 0.000), lr: 0.008350033400133601\n",
      "step: 200, acc: 0.055, loss: 2.310 (data_loss: 2.310, reg_loss: 0.000), lr: 0.008280887711162636\n",
      "step: 300, acc: 0.102, loss: 2.304 (data_loss: 2.304, reg_loss: 0.000), lr: 0.00821287779237845\n",
      "step: 400, acc: 0.180, loss: 2.295 (data_loss: 2.295, reg_loss: 0.000), lr: 0.008145975887911373\n",
      "step: 468, acc: 0.135, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.008101101749837978\n",
      "training, acc: 0.101, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.008101101749837978\n",
      "validation, acc: 0.100, loss: 2.304\n",
      "epoch: 6\n",
      "step: 0, acc: 0.055, loss: 2.304 (data_loss: 2.304, reg_loss: 0.000), lr: 0.00810044552450385\n",
      "step: 100, acc: 0.070, loss: 2.302 (data_loss: 2.302, reg_loss: 0.000), lr: 0.00803535556448373\n",
      "step: 200, acc: 0.055, loss: 2.310 (data_loss: 2.310, reg_loss: 0.000), lr: 0.007971303308090873\n",
      "step: 300, acc: 0.102, loss: 2.304 (data_loss: 2.304, reg_loss: 0.000), lr: 0.007908264136022145\n",
      "step: 400, acc: 0.180, loss: 2.295 (data_loss: 2.295, reg_loss: 0.000), lr: 0.007846214201647706\n",
      "step: 468, acc: 0.135, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.007804573480059316\n",
      "training, acc: 0.101, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.007804573480059316\n",
      "validation, acc: 0.100, loss: 2.304\n",
      "epoch: 7\n",
      "step: 0, acc: 0.055, loss: 2.304 (data_loss: 2.304, reg_loss: 0.000), lr: 0.007803964413922272\n",
      "step: 100, acc: 0.070, loss: 2.302 (data_loss: 2.302, reg_loss: 0.000), lr: 0.007743534148985598\n",
      "step: 200, acc: 0.055, loss: 2.309 (data_loss: 2.309, reg_loss: 0.000), lr: 0.007684032580298141\n",
      "step: 300, acc: 0.102, loss: 2.304 (data_loss: 2.304, reg_loss: 0.000), lr: 0.007625438462711607\n",
      "step: 400, acc: 0.180, loss: 2.296 (data_loss: 2.296, reg_loss: 0.000), lr: 0.007567731194187982\n",
      "step: 468, acc: 0.135, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.007528986598403856\n",
      "training, acc: 0.101, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.007528986598403856\n",
      "validation, acc: 0.100, loss: 2.303\n",
      "epoch: 8\n",
      "step: 0, acc: 0.055, loss: 2.304 (data_loss: 2.304, reg_loss: 0.000), lr: 0.007528419784687193\n",
      "step: 100, acc: 0.070, loss: 2.302 (data_loss: 2.302, reg_loss: 0.000), lr: 0.007472166180975864\n",
      "step: 200, acc: 0.055, loss: 2.309 (data_loss: 2.309, reg_loss: 0.000), lr: 0.007416747014759326\n",
      "step: 300, acc: 0.102, loss: 2.304 (data_loss: 2.304, reg_loss: 0.000), lr: 0.007362143856290951\n",
      "step: 400, acc: 0.180, loss: 2.296 (data_loss: 2.296, reg_loss: 0.000), lr: 0.007308338814587444\n",
      "step: 468, acc: 0.135, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.007272198385571959\n",
      "training, acc: 0.101, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.007272198385571959\n",
      "validation, acc: 0.100, loss: 2.303\n",
      "epoch: 9\n",
      "step: 0, acc: 0.055, loss: 2.304 (data_loss: 2.304, reg_loss: 0.000), lr: 0.007271669575334498\n",
      "step: 100, acc: 0.070, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.007219174126479931\n",
      "step: 200, acc: 0.055, loss: 2.309 (data_loss: 2.309, reg_loss: 0.000), lr: 0.007167431192660551\n",
      "step: 300, acc: 0.102, loss: 2.304 (data_loss: 2.304, reg_loss: 0.000), lr: 0.007116424708226587\n",
      "step: 400, acc: 0.180, loss: 2.296 (data_loss: 2.296, reg_loss: 0.000), lr: 0.007066139061616733\n",
      "step: 468, acc: 0.135, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.007032348804500702\n",
      "training, acc: 0.101, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.007032348804500702\n",
      "validation, acc: 0.100, loss: 2.303\n",
      "epoch: 10\n",
      "step: 0, acc: 0.055, loss: 2.304 (data_loss: 2.304, reg_loss: 0.000), lr: 0.007031854299978905\n",
      "step: 100, acc: 0.070, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.006982752601075343\n",
      "step: 200, acc: 0.055, loss: 2.309 (data_loss: 2.309, reg_loss: 0.000), lr: 0.0069343318771236395\n",
      "step: 300, acc: 0.109, loss: 2.304 (data_loss: 2.304, reg_loss: 0.000), lr: 0.006886578059362303\n",
      "step: 400, acc: 0.180, loss: 2.296 (data_loss: 2.296, reg_loss: 0.000), lr: 0.006839477463921757\n",
      "step: 468, acc: 0.135, loss: 2.305 (data_loss: 2.305, reg_loss: 0.000), lr: 0.00680781537204711\n",
      "training, acc: 0.101, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.00680781537204711\n",
      "validation, acc: 0.100, loss: 2.303\n",
      "epoch: 1\n",
      "step: 0, acc: 0.094, loss: 2.302 (data_loss: 2.302, reg_loss: 0.000), lr: 0.005\n",
      "step: 100, acc: 0.547, loss: 1.095 (data_loss: 1.095, reg_loss: 0.000), lr: 0.0049504950495049506\n",
      "step: 200, acc: 0.547, loss: 1.300 (data_loss: 1.300, reg_loss: 0.000), lr: 0.004901960784313725\n",
      "step: 300, acc: 0.570, loss: 1.068 (data_loss: 1.068, reg_loss: 0.000), lr: 0.004854368932038835\n",
      "step: 400, acc: 0.539, loss: 1.365 (data_loss: 1.365, reg_loss: 0.000), lr: 0.004807692307692307\n",
      "step: 468, acc: 0.458, loss: 1.700 (data_loss: 1.700, reg_loss: 0.000), lr: 0.004776461597248758\n",
      "training, acc: 0.500, loss: 1.346 (data_loss: 1.346, reg_loss: 0.000), lr: 0.004776461597248758\n",
      "validation, acc: 0.689, loss: 0.826\n",
      "epoch: 2\n",
      "step: 0, acc: 0.500, loss: 1.293 (data_loss: 1.293, reg_loss: 0.000), lr: 0.004776005349125991\n",
      "step: 100, acc: 0.477, loss: 1.456 (data_loss: 1.456, reg_loss: 0.000), lr: 0.0047308165389346206\n",
      "step: 200, acc: 0.539, loss: 1.240 (data_loss: 1.240, reg_loss: 0.000), lr: 0.004686474833630144\n",
      "step: 300, acc: 0.492, loss: 1.466 (data_loss: 1.466, reg_loss: 0.000), lr: 0.004642956634785032\n",
      "step: 400, acc: 0.531, loss: 1.352 (data_loss: 1.352, reg_loss: 0.000), lr: 0.004600239212439047\n",
      "step: 468, acc: 0.490, loss: 1.331 (data_loss: 1.331, reg_loss: 0.000), lr: 0.004571637560574197\n",
      "training, acc: 0.493, loss: 1.401 (data_loss: 1.401, reg_loss: 0.000), lr: 0.004571637560574197\n",
      "validation, acc: 0.696, loss: 0.866\n",
      "epoch: 3\n",
      "step: 0, acc: 0.531, loss: 1.471 (data_loss: 1.471, reg_loss: 0.000), lr: 0.00457121960138965\n",
      "step: 100, acc: 0.430, loss: 1.475 (data_loss: 1.475, reg_loss: 0.000), lr: 0.004529806124297879\n",
      "step: 200, acc: 0.391, loss: 1.421 (data_loss: 1.421, reg_loss: 0.000), lr: 0.00448913629017777\n",
      "step: 300, acc: 0.422, loss: 1.582 (data_loss: 1.582, reg_loss: 0.000), lr: 0.004449190247374979\n",
      "step: 400, acc: 0.547, loss: 1.264 (data_loss: 1.264, reg_loss: 0.000), lr: 0.004409948844593403\n",
      "step: 468, acc: 0.469, loss: 1.364 (data_loss: 1.364, reg_loss: 0.000), lr: 0.00438365772400491\n",
      "training, acc: 0.455, loss: 1.500 (data_loss: 1.500, reg_loss: 0.000), lr: 0.00438365772400491\n",
      "validation, acc: 0.705, loss: 0.821\n",
      "epoch: 4\n",
      "step: 0, acc: 0.523, loss: 1.360 (data_loss: 1.360, reg_loss: 0.000), lr: 0.004383273428596476\n",
      "step: 100, acc: 0.484, loss: 1.492 (data_loss: 1.492, reg_loss: 0.000), lr: 0.004345181194055792\n",
      "step: 200, acc: 0.484, loss: 1.292 (data_loss: 1.292, reg_loss: 0.000), lr: 0.004307745326096321\n",
      "step: 300, acc: 0.430, loss: 1.435 (data_loss: 1.435, reg_loss: 0.000), lr: 0.004270949004868882\n",
      "step: 400, acc: 0.477, loss: 1.326 (data_loss: 1.326, reg_loss: 0.000), lr: 0.004234775980350639\n",
      "step: 468, acc: 0.438, loss: 1.602 (data_loss: 1.602, reg_loss: 0.000), lr: 0.004210526315789474\n",
      "training, acc: 0.459, loss: 1.474 (data_loss: 1.474, reg_loss: 0.000), lr: 0.004210526315789474\n",
      "validation, acc: 0.679, loss: 0.937\n",
      "epoch: 5\n",
      "step: 0, acc: 0.508, loss: 1.347 (data_loss: 1.347, reg_loss: 0.000), lr: 0.004210171775008421\n",
      "step: 100, acc: 0.461, loss: 1.564 (data_loss: 1.564, reg_loss: 0.000), lr: 0.004175016700066801\n",
      "step: 200, acc: 0.383, loss: 1.619 (data_loss: 1.619, reg_loss: 0.000), lr: 0.004140443855581318\n",
      "step: 300, acc: 0.414, loss: 1.737 (data_loss: 1.737, reg_loss: 0.000), lr: 0.004106438896189225\n",
      "step: 400, acc: 0.516, loss: 1.189 (data_loss: 1.189, reg_loss: 0.000), lr: 0.004072987943955686\n",
      "step: 468, acc: 0.448, loss: 1.461 (data_loss: 1.461, reg_loss: 0.000), lr: 0.004050550874918989\n",
      "training, acc: 0.452, loss: 1.490 (data_loss: 1.490, reg_loss: 0.000), lr: 0.004050550874918989\n",
      "validation, acc: 0.677, loss: 0.968\n",
      "epoch: 6\n",
      "step: 0, acc: 0.617, loss: 1.192 (data_loss: 1.192, reg_loss: 0.000), lr: 0.004050222762251925\n",
      "step: 100, acc: 0.430, loss: 1.504 (data_loss: 1.504, reg_loss: 0.000), lr: 0.004017677782241865\n",
      "step: 200, acc: 0.383, loss: 1.520 (data_loss: 1.520, reg_loss: 0.000), lr: 0.0039856516540454365\n",
      "step: 300, acc: 0.383, loss: 1.592 (data_loss: 1.592, reg_loss: 0.000), lr: 0.003954132068011072\n",
      "step: 400, acc: 0.492, loss: 1.382 (data_loss: 1.382, reg_loss: 0.000), lr: 0.003923107100823853\n",
      "step: 468, acc: 0.438, loss: 1.393 (data_loss: 1.393, reg_loss: 0.000), lr: 0.003902286740029658\n",
      "training, acc: 0.442, loss: 1.510 (data_loss: 1.510, reg_loss: 0.000), lr: 0.003902286740029658\n",
      "validation, acc: 0.597, loss: 1.053\n",
      "epoch: 7\n",
      "step: 0, acc: 0.516, loss: 1.285 (data_loss: 1.285, reg_loss: 0.000), lr: 0.003901982206961136\n",
      "step: 100, acc: 0.406, loss: 1.592 (data_loss: 1.592, reg_loss: 0.000), lr: 0.003871767074492799\n",
      "step: 200, acc: 0.406, loss: 1.699 (data_loss: 1.699, reg_loss: 0.000), lr: 0.0038420162901490703\n",
      "step: 300, acc: 0.461, loss: 1.320 (data_loss: 1.320, reg_loss: 0.000), lr: 0.0038127192313558034\n",
      "step: 400, acc: 0.531, loss: 1.227 (data_loss: 1.227, reg_loss: 0.000), lr: 0.003783865597093991\n",
      "step: 468, acc: 0.438, loss: 1.503 (data_loss: 1.503, reg_loss: 0.000), lr: 0.003764493299201928\n",
      "training, acc: 0.463, loss: 1.461 (data_loss: 1.461, reg_loss: 0.000), lr: 0.003764493299201928\n",
      "validation, acc: 0.666, loss: 0.905\n",
      "epoch: 8\n",
      "step: 0, acc: 0.523, loss: 1.372 (data_loss: 1.372, reg_loss: 0.000), lr: 0.0037642098923435967\n",
      "step: 100, acc: 0.469, loss: 1.384 (data_loss: 1.384, reg_loss: 0.000), lr: 0.003736083090487932\n",
      "step: 200, acc: 0.406, loss: 1.595 (data_loss: 1.595, reg_loss: 0.000), lr: 0.003708373507379663\n",
      "step: 300, acc: 0.461, loss: 1.360 (data_loss: 1.360, reg_loss: 0.000), lr: 0.0036810719281454756\n",
      "step: 400, acc: 0.555, loss: 1.302 (data_loss: 1.302, reg_loss: 0.000), lr: 0.003654169407293722\n",
      "step: 468, acc: 0.406, loss: 1.546 (data_loss: 1.546, reg_loss: 0.000), lr: 0.0036360991927859793\n",
      "training, acc: 0.464, loss: 1.434 (data_loss: 1.434, reg_loss: 0.000), lr: 0.0036360991927859793\n",
      "validation, acc: 0.682, loss: 0.904\n",
      "epoch: 9\n",
      "step: 0, acc: 0.570, loss: 1.312 (data_loss: 1.312, reg_loss: 0.000), lr: 0.003635834787667249\n",
      "step: 100, acc: 0.477, loss: 1.365 (data_loss: 1.365, reg_loss: 0.000), lr: 0.0036095870632399654\n",
      "step: 200, acc: 0.438, loss: 1.332 (data_loss: 1.332, reg_loss: 0.000), lr: 0.0035837155963302754\n",
      "step: 300, acc: 0.477, loss: 1.293 (data_loss: 1.293, reg_loss: 0.000), lr: 0.0035582123541132936\n",
      "step: 400, acc: 0.477, loss: 1.609 (data_loss: 1.609, reg_loss: 0.000), lr: 0.0035330695308083667\n",
      "step: 468, acc: 0.458, loss: 1.368 (data_loss: 1.368, reg_loss: 0.000), lr: 0.003516174402250351\n",
      "training, acc: 0.454, loss: 1.453 (data_loss: 1.453, reg_loss: 0.000), lr: 0.003516174402250351\n",
      "validation, acc: 0.665, loss: 0.909\n",
      "epoch: 10\n",
      "step: 0, acc: 0.555, loss: 1.257 (data_loss: 1.257, reg_loss: 0.000), lr: 0.0035159271499894526\n",
      "step: 100, acc: 0.492, loss: 1.263 (data_loss: 1.263, reg_loss: 0.000), lr: 0.0034913763005376716\n",
      "step: 200, acc: 0.430, loss: 1.404 (data_loss: 1.404, reg_loss: 0.000), lr: 0.0034671659385618198\n",
      "step: 300, acc: 0.484, loss: 1.360 (data_loss: 1.360, reg_loss: 0.000), lr: 0.0034432890296811516\n",
      "step: 400, acc: 0.453, loss: 1.318 (data_loss: 1.318, reg_loss: 0.000), lr: 0.0034197387319608785\n",
      "step: 468, acc: 0.469, loss: 1.469 (data_loss: 1.469, reg_loss: 0.000), lr: 0.003403907686023555\n",
      "training, acc: 0.469, loss: 1.410 (data_loss: 1.410, reg_loss: 0.000), lr: 0.003403907686023555\n",
      "validation, acc: 0.612, loss: 0.991\n",
      "epoch: 1\n",
      "step: 0, acc: 0.086, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.453, loss: 1.192 (data_loss: 1.192, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 200, acc: 0.641, loss: 0.929 (data_loss: 0.929, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 300, acc: 0.688, loss: 0.867 (data_loss: 0.867, reg_loss: 0.000), lr: 0.0009708737864077671\n",
      "step: 400, acc: 0.688, loss: 0.880 (data_loss: 0.880, reg_loss: 0.000), lr: 0.0009615384615384615\n",
      "step: 468, acc: 0.604, loss: 0.991 (data_loss: 0.991, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "training, acc: 0.562, loss: 1.093 (data_loss: 1.093, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "validation, acc: 0.766, loss: 0.617\n",
      "epoch: 2\n",
      "step: 0, acc: 0.648, loss: 0.811 (data_loss: 0.811, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.703, loss: 0.883 (data_loss: 0.883, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 200, acc: 0.672, loss: 0.799 (data_loss: 0.799, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 300, acc: 0.711, loss: 0.773 (data_loss: 0.773, reg_loss: 0.000), lr: 0.0009285913269570063\n",
      "step: 400, acc: 0.664, loss: 0.874 (data_loss: 0.874, reg_loss: 0.000), lr: 0.0009200478424878093\n",
      "step: 468, acc: 0.646, loss: 0.856 (data_loss: 0.856, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "training, acc: 0.665, loss: 0.867 (data_loss: 0.867, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "validation, acc: 0.795, loss: 0.557\n",
      "epoch: 3\n",
      "step: 0, acc: 0.680, loss: 0.788 (data_loss: 0.788, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.719, loss: 0.827 (data_loss: 0.827, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 200, acc: 0.688, loss: 0.782 (data_loss: 0.782, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 300, acc: 0.703, loss: 0.743 (data_loss: 0.743, reg_loss: 0.000), lr: 0.0008898380494749957\n",
      "step: 400, acc: 0.727, loss: 0.898 (data_loss: 0.898, reg_loss: 0.000), lr: 0.0008819897689186807\n",
      "step: 468, acc: 0.615, loss: 0.922 (data_loss: 0.922, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "training, acc: 0.687, loss: 0.825 (data_loss: 0.825, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "validation, acc: 0.809, loss: 0.531\n",
      "epoch: 4\n",
      "step: 0, acc: 0.703, loss: 0.735 (data_loss: 0.735, reg_loss: 0.000), lr: 0.0008766546857192952\n",
      "step: 100, acc: 0.703, loss: 0.873 (data_loss: 0.873, reg_loss: 0.000), lr: 0.0008690362388111585\n",
      "step: 200, acc: 0.664, loss: 0.798 (data_loss: 0.798, reg_loss: 0.000), lr: 0.0008615490652192642\n",
      "step: 300, acc: 0.750, loss: 0.711 (data_loss: 0.711, reg_loss: 0.000), lr: 0.0008541898009737763\n",
      "step: 400, acc: 0.711, loss: 0.672 (data_loss: 0.672, reg_loss: 0.000), lr: 0.0008469551960701278\n",
      "step: 468, acc: 0.635, loss: 0.872 (data_loss: 0.872, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "training, acc: 0.694, loss: 0.808 (data_loss: 0.808, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "validation, acc: 0.808, loss: 0.526\n",
      "epoch: 5\n",
      "step: 0, acc: 0.703, loss: 0.648 (data_loss: 0.648, reg_loss: 0.000), lr: 0.0008420343550016842\n",
      "step: 100, acc: 0.711, loss: 0.842 (data_loss: 0.842, reg_loss: 0.000), lr: 0.0008350033400133601\n",
      "step: 200, acc: 0.672, loss: 0.801 (data_loss: 0.801, reg_loss: 0.000), lr: 0.0008280887711162637\n",
      "step: 300, acc: 0.727, loss: 0.821 (data_loss: 0.821, reg_loss: 0.000), lr: 0.0008212877792378449\n",
      "step: 400, acc: 0.766, loss: 0.690 (data_loss: 0.690, reg_loss: 0.000), lr: 0.0008145975887911372\n",
      "step: 468, acc: 0.615, loss: 0.973 (data_loss: 0.973, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "training, acc: 0.704, loss: 0.785 (data_loss: 0.785, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "validation, acc: 0.817, loss: 0.511\n",
      "epoch: 6\n",
      "step: 0, acc: 0.734, loss: 0.787 (data_loss: 0.787, reg_loss: 0.000), lr: 0.0008100445524503849\n",
      "step: 100, acc: 0.727, loss: 0.804 (data_loss: 0.804, reg_loss: 0.000), lr: 0.0008035355564483729\n",
      "step: 200, acc: 0.664, loss: 0.769 (data_loss: 0.769, reg_loss: 0.000), lr: 0.0007971303308090873\n",
      "step: 300, acc: 0.734, loss: 0.667 (data_loss: 0.667, reg_loss: 0.000), lr: 0.0007908264136022144\n",
      "step: 400, acc: 0.703, loss: 0.772 (data_loss: 0.772, reg_loss: 0.000), lr: 0.0007846214201647706\n",
      "step: 468, acc: 0.729, loss: 0.758 (data_loss: 0.758, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "training, acc: 0.707, loss: 0.778 (data_loss: 0.778, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "validation, acc: 0.816, loss: 0.511\n",
      "epoch: 7\n",
      "step: 0, acc: 0.773, loss: 0.617 (data_loss: 0.617, reg_loss: 0.000), lr: 0.0007803964413922272\n",
      "step: 100, acc: 0.734, loss: 0.794 (data_loss: 0.794, reg_loss: 0.000), lr: 0.0007743534148985598\n",
      "step: 200, acc: 0.695, loss: 0.741 (data_loss: 0.741, reg_loss: 0.000), lr: 0.0007684032580298141\n",
      "step: 300, acc: 0.703, loss: 0.724 (data_loss: 0.724, reg_loss: 0.000), lr: 0.0007625438462711606\n",
      "step: 400, acc: 0.719, loss: 0.737 (data_loss: 0.737, reg_loss: 0.000), lr: 0.0007567731194187983\n",
      "step: 468, acc: 0.656, loss: 0.922 (data_loss: 0.922, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "training, acc: 0.712, loss: 0.768 (data_loss: 0.768, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "validation, acc: 0.820, loss: 0.513\n",
      "epoch: 8\n",
      "step: 0, acc: 0.719, loss: 0.816 (data_loss: 0.816, reg_loss: 0.000), lr: 0.0007528419784687194\n",
      "step: 100, acc: 0.742, loss: 0.735 (data_loss: 0.735, reg_loss: 0.000), lr: 0.0007472166180975864\n",
      "step: 200, acc: 0.773, loss: 0.627 (data_loss: 0.627, reg_loss: 0.000), lr: 0.0007416747014759327\n",
      "step: 300, acc: 0.727, loss: 0.663 (data_loss: 0.663, reg_loss: 0.000), lr: 0.0007362143856290952\n",
      "step: 400, acc: 0.734, loss: 0.921 (data_loss: 0.921, reg_loss: 0.000), lr: 0.0007308338814587444\n",
      "step: 468, acc: 0.698, loss: 0.876 (data_loss: 0.876, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "training, acc: 0.714, loss: 0.761 (data_loss: 0.761, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "validation, acc: 0.824, loss: 0.513\n",
      "epoch: 9\n",
      "step: 0, acc: 0.695, loss: 0.751 (data_loss: 0.751, reg_loss: 0.000), lr: 0.0007271669575334498\n",
      "step: 100, acc: 0.781, loss: 0.630 (data_loss: 0.630, reg_loss: 0.000), lr: 0.000721917412647993\n",
      "step: 200, acc: 0.703, loss: 0.743 (data_loss: 0.743, reg_loss: 0.000), lr: 0.0007167431192660551\n",
      "step: 300, acc: 0.773, loss: 0.634 (data_loss: 0.634, reg_loss: 0.000), lr: 0.0007116424708226587\n",
      "step: 400, acc: 0.695, loss: 0.803 (data_loss: 0.803, reg_loss: 0.000), lr: 0.0007066139061616733\n",
      "step: 468, acc: 0.646, loss: 0.928 (data_loss: 0.928, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "training, acc: 0.719, loss: 0.753 (data_loss: 0.753, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "validation, acc: 0.823, loss: 0.496\n",
      "epoch: 10\n",
      "step: 0, acc: 0.750, loss: 0.729 (data_loss: 0.729, reg_loss: 0.000), lr: 0.0007031854299978904\n",
      "step: 100, acc: 0.695, loss: 0.860 (data_loss: 0.860, reg_loss: 0.000), lr: 0.0006982752601075343\n",
      "step: 200, acc: 0.672, loss: 0.751 (data_loss: 0.751, reg_loss: 0.000), lr: 0.000693433187712364\n",
      "step: 300, acc: 0.727, loss: 0.691 (data_loss: 0.691, reg_loss: 0.000), lr: 0.0006886578059362303\n",
      "step: 400, acc: 0.742, loss: 0.644 (data_loss: 0.644, reg_loss: 0.000), lr: 0.0006839477463921757\n",
      "step: 468, acc: 0.667, loss: 0.854 (data_loss: 0.854, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "training, acc: 0.722, loss: 0.743 (data_loss: 0.743, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "validation, acc: 0.822, loss: 0.499\n",
      "epoch: 1\n",
      "step: 0, acc: 0.125, loss: 2.302 (data_loss: 2.302, reg_loss: 0.000), lr: 0.0005\n",
      "step: 100, acc: 0.492, loss: 1.268 (data_loss: 1.268, reg_loss: 0.000), lr: 0.0004950495049504951\n",
      "step: 200, acc: 0.602, loss: 1.006 (data_loss: 1.006, reg_loss: 0.000), lr: 0.0004901960784313725\n",
      "step: 300, acc: 0.617, loss: 0.943 (data_loss: 0.943, reg_loss: 0.000), lr: 0.00048543689320388353\n",
      "step: 400, acc: 0.617, loss: 1.014 (data_loss: 1.014, reg_loss: 0.000), lr: 0.00048076923076923074\n",
      "step: 468, acc: 0.583, loss: 1.023 (data_loss: 1.023, reg_loss: 0.000), lr: 0.0004776461597248759\n",
      "training, acc: 0.533, loss: 1.174 (data_loss: 1.174, reg_loss: 0.000), lr: 0.0004776461597248759\n",
      "validation, acc: 0.759, loss: 0.645\n",
      "epoch: 2\n",
      "step: 0, acc: 0.664, loss: 0.844 (data_loss: 0.844, reg_loss: 0.000), lr: 0.00047760053491259914\n",
      "step: 100, acc: 0.586, loss: 1.029 (data_loss: 1.029, reg_loss: 0.000), lr: 0.00047308165389346203\n",
      "step: 200, acc: 0.656, loss: 0.991 (data_loss: 0.991, reg_loss: 0.000), lr: 0.00046864748336301437\n",
      "step: 300, acc: 0.672, loss: 0.799 (data_loss: 0.799, reg_loss: 0.000), lr: 0.00046429566347850313\n",
      "step: 400, acc: 0.695, loss: 0.889 (data_loss: 0.889, reg_loss: 0.000), lr: 0.00046002392124390467\n",
      "step: 468, acc: 0.708, loss: 0.988 (data_loss: 0.988, reg_loss: 0.000), lr: 0.0004571637560574197\n",
      "training, acc: 0.661, loss: 0.875 (data_loss: 0.875, reg_loss: 0.000), lr: 0.0004571637560574197\n",
      "validation, acc: 0.783, loss: 0.577\n",
      "epoch: 3\n",
      "step: 0, acc: 0.664, loss: 0.835 (data_loss: 0.835, reg_loss: 0.000), lr: 0.0004571219601389651\n",
      "step: 100, acc: 0.672, loss: 0.917 (data_loss: 0.917, reg_loss: 0.000), lr: 0.00045298061242978794\n",
      "step: 200, acc: 0.656, loss: 0.815 (data_loss: 0.815, reg_loss: 0.000), lr: 0.00044891362901777705\n",
      "step: 300, acc: 0.703, loss: 0.702 (data_loss: 0.702, reg_loss: 0.000), lr: 0.00044491902473749786\n",
      "step: 400, acc: 0.703, loss: 0.904 (data_loss: 0.904, reg_loss: 0.000), lr: 0.00044099488445934035\n",
      "step: 468, acc: 0.656, loss: 0.898 (data_loss: 0.898, reg_loss: 0.000), lr: 0.000438365772400491\n",
      "training, acc: 0.688, loss: 0.822 (data_loss: 0.822, reg_loss: 0.000), lr: 0.000438365772400491\n",
      "validation, acc: 0.796, loss: 0.551\n",
      "epoch: 4\n",
      "step: 0, acc: 0.734, loss: 0.666 (data_loss: 0.666, reg_loss: 0.000), lr: 0.0004383273428596476\n",
      "step: 100, acc: 0.727, loss: 0.900 (data_loss: 0.900, reg_loss: 0.000), lr: 0.00043451811940557923\n",
      "step: 200, acc: 0.727, loss: 0.845 (data_loss: 0.845, reg_loss: 0.000), lr: 0.0004307745326096321\n",
      "step: 300, acc: 0.688, loss: 0.651 (data_loss: 0.651, reg_loss: 0.000), lr: 0.00042709490048688817\n",
      "step: 400, acc: 0.758, loss: 0.651 (data_loss: 0.651, reg_loss: 0.000), lr: 0.0004234775980350639\n",
      "step: 468, acc: 0.656, loss: 0.881 (data_loss: 0.881, reg_loss: 0.000), lr: 0.00042105263157894734\n",
      "training, acc: 0.698, loss: 0.799 (data_loss: 0.799, reg_loss: 0.000), lr: 0.00042105263157894734\n",
      "validation, acc: 0.802, loss: 0.537\n",
      "epoch: 5\n",
      "step: 0, acc: 0.695, loss: 0.760 (data_loss: 0.760, reg_loss: 0.000), lr: 0.0004210171775008421\n",
      "step: 100, acc: 0.758, loss: 0.813 (data_loss: 0.813, reg_loss: 0.000), lr: 0.00041750167000668005\n",
      "step: 200, acc: 0.680, loss: 0.816 (data_loss: 0.816, reg_loss: 0.000), lr: 0.00041404438555813183\n",
      "step: 300, acc: 0.727, loss: 0.684 (data_loss: 0.684, reg_loss: 0.000), lr: 0.00041064388961892245\n",
      "step: 400, acc: 0.734, loss: 0.716 (data_loss: 0.716, reg_loss: 0.000), lr: 0.0004072987943955686\n",
      "step: 468, acc: 0.594, loss: 1.025 (data_loss: 1.025, reg_loss: 0.000), lr: 0.0004050550874918989\n",
      "training, acc: 0.706, loss: 0.785 (data_loss: 0.785, reg_loss: 0.000), lr: 0.0004050550874918989\n",
      "validation, acc: 0.806, loss: 0.526\n",
      "epoch: 6\n",
      "step: 0, acc: 0.734, loss: 0.609 (data_loss: 0.609, reg_loss: 0.000), lr: 0.00040502227622519246\n",
      "step: 100, acc: 0.727, loss: 0.816 (data_loss: 0.816, reg_loss: 0.000), lr: 0.00040176777822418646\n",
      "step: 200, acc: 0.711, loss: 0.695 (data_loss: 0.695, reg_loss: 0.000), lr: 0.00039856516540454366\n",
      "step: 300, acc: 0.781, loss: 0.651 (data_loss: 0.651, reg_loss: 0.000), lr: 0.0003954132068011072\n",
      "step: 400, acc: 0.727, loss: 0.736 (data_loss: 0.736, reg_loss: 0.000), lr: 0.0003923107100823853\n",
      "step: 468, acc: 0.708, loss: 0.697 (data_loss: 0.697, reg_loss: 0.000), lr: 0.0003902286740029658\n",
      "training, acc: 0.713, loss: 0.767 (data_loss: 0.767, reg_loss: 0.000), lr: 0.0003902286740029658\n",
      "validation, acc: 0.812, loss: 0.517\n",
      "epoch: 7\n",
      "step: 0, acc: 0.711, loss: 0.725 (data_loss: 0.725, reg_loss: 0.000), lr: 0.0003901982206961136\n",
      "step: 100, acc: 0.719, loss: 0.875 (data_loss: 0.875, reg_loss: 0.000), lr: 0.0003871767074492799\n",
      "step: 200, acc: 0.742, loss: 0.807 (data_loss: 0.807, reg_loss: 0.000), lr: 0.00038420162901490703\n",
      "step: 300, acc: 0.711, loss: 0.640 (data_loss: 0.640, reg_loss: 0.000), lr: 0.0003812719231355803\n",
      "step: 400, acc: 0.766, loss: 0.640 (data_loss: 0.640, reg_loss: 0.000), lr: 0.00037838655970939914\n",
      "step: 468, acc: 0.729, loss: 0.833 (data_loss: 0.833, reg_loss: 0.000), lr: 0.0003764493299201928\n",
      "training, acc: 0.717, loss: 0.758 (data_loss: 0.758, reg_loss: 0.000), lr: 0.0003764493299201928\n",
      "validation, acc: 0.808, loss: 0.515\n",
      "epoch: 8\n",
      "step: 0, acc: 0.789, loss: 0.677 (data_loss: 0.677, reg_loss: 0.000), lr: 0.0003764209892343597\n",
      "step: 100, acc: 0.734, loss: 0.733 (data_loss: 0.733, reg_loss: 0.000), lr: 0.0003736083090487932\n",
      "step: 200, acc: 0.742, loss: 0.687 (data_loss: 0.687, reg_loss: 0.000), lr: 0.00037083735073796634\n",
      "step: 300, acc: 0.695, loss: 0.718 (data_loss: 0.718, reg_loss: 0.000), lr: 0.0003681071928145476\n",
      "step: 400, acc: 0.766, loss: 0.682 (data_loss: 0.682, reg_loss: 0.000), lr: 0.0003654169407293722\n",
      "step: 468, acc: 0.729, loss: 0.845 (data_loss: 0.845, reg_loss: 0.000), lr: 0.00036360991927859794\n",
      "training, acc: 0.718, loss: 0.756 (data_loss: 0.756, reg_loss: 0.000), lr: 0.00036360991927859794\n",
      "validation, acc: 0.816, loss: 0.502\n",
      "epoch: 9\n",
      "step: 0, acc: 0.680, loss: 0.745 (data_loss: 0.745, reg_loss: 0.000), lr: 0.0003635834787667249\n",
      "step: 100, acc: 0.711, loss: 0.852 (data_loss: 0.852, reg_loss: 0.000), lr: 0.0003609587063239965\n",
      "step: 200, acc: 0.719, loss: 0.772 (data_loss: 0.772, reg_loss: 0.000), lr: 0.00035837155963302754\n",
      "step: 300, acc: 0.766, loss: 0.643 (data_loss: 0.643, reg_loss: 0.000), lr: 0.00035582123541132935\n",
      "step: 400, acc: 0.711, loss: 0.655 (data_loss: 0.655, reg_loss: 0.000), lr: 0.00035330695308083665\n",
      "step: 468, acc: 0.708, loss: 0.971 (data_loss: 0.971, reg_loss: 0.000), lr: 0.0003516174402250351\n",
      "training, acc: 0.722, loss: 0.745 (data_loss: 0.745, reg_loss: 0.000), lr: 0.0003516174402250351\n",
      "validation, acc: 0.818, loss: 0.500\n",
      "epoch: 10\n",
      "step: 0, acc: 0.742, loss: 0.734 (data_loss: 0.734, reg_loss: 0.000), lr: 0.0003515927149989452\n",
      "step: 100, acc: 0.742, loss: 0.757 (data_loss: 0.757, reg_loss: 0.000), lr: 0.00034913763005376715\n",
      "step: 200, acc: 0.688, loss: 0.783 (data_loss: 0.783, reg_loss: 0.000), lr: 0.000346716593856182\n",
      "step: 300, acc: 0.766, loss: 0.579 (data_loss: 0.579, reg_loss: 0.000), lr: 0.00034432890296811513\n",
      "step: 400, acc: 0.719, loss: 0.863 (data_loss: 0.863, reg_loss: 0.000), lr: 0.00034197387319608784\n",
      "step: 468, acc: 0.688, loss: 0.717 (data_loss: 0.717, reg_loss: 0.000), lr: 0.0003403907686023555\n",
      "training, acc: 0.725, loss: 0.743 (data_loss: 0.743, reg_loss: 0.000), lr: 0.0003403907686023555\n",
      "validation, acc: 0.818, loss: 0.492\n",
      "epoch: 1\n",
      "step: 0, acc: 0.078, loss: 2.304 (data_loss: 2.304, reg_loss: 0.000), lr: 0.0001\n",
      "step: 100, acc: 0.273, loss: 1.928 (data_loss: 1.928, reg_loss: 0.000), lr: 9.900990099009902e-05\n",
      "step: 200, acc: 0.344, loss: 1.495 (data_loss: 1.495, reg_loss: 0.000), lr: 9.80392156862745e-05\n",
      "step: 300, acc: 0.438, loss: 1.333 (data_loss: 1.333, reg_loss: 0.000), lr: 9.70873786407767e-05\n",
      "step: 400, acc: 0.523, loss: 1.272 (data_loss: 1.272, reg_loss: 0.000), lr: 9.615384615384615e-05\n",
      "step: 468, acc: 0.562, loss: 1.140 (data_loss: 1.140, reg_loss: 0.000), lr: 9.552923194497518e-05\n",
      "training, acc: 0.381, loss: 1.587 (data_loss: 1.587, reg_loss: 0.000), lr: 9.552923194497518e-05\n",
      "validation, acc: 0.639, loss: 0.988\n",
      "epoch: 2\n",
      "step: 0, acc: 0.531, loss: 1.127 (data_loss: 1.127, reg_loss: 0.000), lr: 9.552010698251983e-05\n",
      "step: 100, acc: 0.562, loss: 1.081 (data_loss: 1.081, reg_loss: 0.000), lr: 9.46163307786924e-05\n",
      "step: 200, acc: 0.578, loss: 1.090 (data_loss: 1.090, reg_loss: 0.000), lr: 9.372949667260288e-05\n",
      "step: 300, acc: 0.602, loss: 1.059 (data_loss: 1.059, reg_loss: 0.000), lr: 9.285913269570063e-05\n",
      "step: 400, acc: 0.594, loss: 1.160 (data_loss: 1.160, reg_loss: 0.000), lr: 9.200478424878094e-05\n",
      "step: 468, acc: 0.510, loss: 1.169 (data_loss: 1.169, reg_loss: 0.000), lr: 9.143275121148394e-05\n",
      "training, acc: 0.543, loss: 1.126 (data_loss: 1.126, reg_loss: 0.000), lr: 9.143275121148394e-05\n",
      "validation, acc: 0.722, loss: 0.826\n",
      "epoch: 3\n",
      "step: 0, acc: 0.547, loss: 1.069 (data_loss: 1.069, reg_loss: 0.000), lr: 9.142439202779301e-05\n",
      "step: 100, acc: 0.602, loss: 1.080 (data_loss: 1.080, reg_loss: 0.000), lr: 9.059612248595759e-05\n",
      "step: 200, acc: 0.617, loss: 1.076 (data_loss: 1.076, reg_loss: 0.000), lr: 8.978272580355542e-05\n",
      "step: 300, acc: 0.625, loss: 0.900 (data_loss: 0.900, reg_loss: 0.000), lr: 8.898380494749958e-05\n",
      "step: 400, acc: 0.625, loss: 1.040 (data_loss: 1.040, reg_loss: 0.000), lr: 8.819897689186807e-05\n",
      "step: 468, acc: 0.531, loss: 1.223 (data_loss: 1.223, reg_loss: 0.000), lr: 8.76731544800982e-05\n",
      "training, acc: 0.590, loss: 1.028 (data_loss: 1.028, reg_loss: 0.000), lr: 8.76731544800982e-05\n",
      "validation, acc: 0.745, loss: 0.743\n",
      "epoch: 4\n",
      "step: 0, acc: 0.641, loss: 0.811 (data_loss: 0.811, reg_loss: 0.000), lr: 8.766546857192952e-05\n",
      "step: 100, acc: 0.617, loss: 1.037 (data_loss: 1.037, reg_loss: 0.000), lr: 8.690362388111585e-05\n",
      "step: 200, acc: 0.648, loss: 0.913 (data_loss: 0.913, reg_loss: 0.000), lr: 8.615490652192643e-05\n",
      "step: 300, acc: 0.609, loss: 0.880 (data_loss: 0.880, reg_loss: 0.000), lr: 8.541898009737763e-05\n",
      "step: 400, acc: 0.703, loss: 0.949 (data_loss: 0.949, reg_loss: 0.000), lr: 8.469551960701279e-05\n",
      "step: 468, acc: 0.604, loss: 1.008 (data_loss: 1.008, reg_loss: 0.000), lr: 8.421052631578948e-05\n",
      "training, acc: 0.619, loss: 0.964 (data_loss: 0.964, reg_loss: 0.000), lr: 8.421052631578948e-05\n",
      "validation, acc: 0.754, loss: 0.698\n",
      "epoch: 5\n",
      "step: 0, acc: 0.633, loss: 0.869 (data_loss: 0.869, reg_loss: 0.000), lr: 8.420343550016842e-05\n",
      "step: 100, acc: 0.578, loss: 1.049 (data_loss: 1.049, reg_loss: 0.000), lr: 8.350033400133601e-05\n",
      "step: 200, acc: 0.641, loss: 0.939 (data_loss: 0.939, reg_loss: 0.000), lr: 8.280887711162637e-05\n",
      "step: 300, acc: 0.656, loss: 0.867 (data_loss: 0.867, reg_loss: 0.000), lr: 8.21287779237845e-05\n",
      "step: 400, acc: 0.688, loss: 0.928 (data_loss: 0.928, reg_loss: 0.000), lr: 8.145975887911372e-05\n",
      "step: 468, acc: 0.667, loss: 0.987 (data_loss: 0.987, reg_loss: 0.000), lr: 8.101101749837978e-05\n",
      "training, acc: 0.638, loss: 0.926 (data_loss: 0.926, reg_loss: 0.000), lr: 8.101101749837978e-05\n",
      "validation, acc: 0.761, loss: 0.667\n",
      "epoch: 6\n",
      "step: 0, acc: 0.695, loss: 0.822 (data_loss: 0.822, reg_loss: 0.000), lr: 8.10044552450385e-05\n",
      "step: 100, acc: 0.633, loss: 0.968 (data_loss: 0.968, reg_loss: 0.000), lr: 8.035355564483729e-05\n",
      "step: 200, acc: 0.695, loss: 0.962 (data_loss: 0.962, reg_loss: 0.000), lr: 7.971303308090874e-05\n",
      "step: 300, acc: 0.680, loss: 0.822 (data_loss: 0.822, reg_loss: 0.000), lr: 7.908264136022144e-05\n",
      "step: 400, acc: 0.695, loss: 0.844 (data_loss: 0.844, reg_loss: 0.000), lr: 7.846214201647706e-05\n",
      "step: 468, acc: 0.677, loss: 0.923 (data_loss: 0.923, reg_loss: 0.000), lr: 7.804573480059316e-05\n",
      "training, acc: 0.651, loss: 0.893 (data_loss: 0.893, reg_loss: 0.000), lr: 7.804573480059316e-05\n",
      "validation, acc: 0.767, loss: 0.642\n",
      "epoch: 7\n",
      "step: 0, acc: 0.680, loss: 0.809 (data_loss: 0.809, reg_loss: 0.000), lr: 7.803964413922272e-05\n",
      "step: 100, acc: 0.633, loss: 0.926 (data_loss: 0.926, reg_loss: 0.000), lr: 7.743534148985598e-05\n",
      "step: 200, acc: 0.695, loss: 0.921 (data_loss: 0.921, reg_loss: 0.000), lr: 7.684032580298141e-05\n",
      "step: 300, acc: 0.758, loss: 0.707 (data_loss: 0.707, reg_loss: 0.000), lr: 7.625438462711606e-05\n",
      "step: 400, acc: 0.648, loss: 0.799 (data_loss: 0.799, reg_loss: 0.000), lr: 7.567731194187983e-05\n",
      "step: 468, acc: 0.594, loss: 1.033 (data_loss: 1.033, reg_loss: 0.000), lr: 7.528986598403855e-05\n",
      "training, acc: 0.660, loss: 0.872 (data_loss: 0.872, reg_loss: 0.000), lr: 7.528986598403855e-05\n",
      "validation, acc: 0.772, loss: 0.627\n",
      "epoch: 8\n",
      "step: 0, acc: 0.703, loss: 0.734 (data_loss: 0.734, reg_loss: 0.000), lr: 7.528419784687193e-05\n",
      "step: 100, acc: 0.695, loss: 0.945 (data_loss: 0.945, reg_loss: 0.000), lr: 7.472166180975865e-05\n",
      "step: 200, acc: 0.664, loss: 0.826 (data_loss: 0.826, reg_loss: 0.000), lr: 7.416747014759327e-05\n",
      "step: 300, acc: 0.664, loss: 0.783 (data_loss: 0.783, reg_loss: 0.000), lr: 7.362143856290952e-05\n",
      "step: 400, acc: 0.703, loss: 0.756 (data_loss: 0.756, reg_loss: 0.000), lr: 7.308338814587444e-05\n",
      "step: 468, acc: 0.667, loss: 0.926 (data_loss: 0.926, reg_loss: 0.000), lr: 7.272198385571959e-05\n",
      "training, acc: 0.670, loss: 0.853 (data_loss: 0.853, reg_loss: 0.000), lr: 7.272198385571959e-05\n",
      "validation, acc: 0.775, loss: 0.613\n",
      "epoch: 9\n",
      "step: 0, acc: 0.727, loss: 0.724 (data_loss: 0.724, reg_loss: 0.000), lr: 7.271669575334498e-05\n",
      "step: 100, acc: 0.625, loss: 0.976 (data_loss: 0.976, reg_loss: 0.000), lr: 7.219174126479931e-05\n",
      "step: 200, acc: 0.672, loss: 0.864 (data_loss: 0.864, reg_loss: 0.000), lr: 7.167431192660551e-05\n",
      "step: 300, acc: 0.727, loss: 0.766 (data_loss: 0.766, reg_loss: 0.000), lr: 7.116424708226587e-05\n",
      "step: 400, acc: 0.695, loss: 0.914 (data_loss: 0.914, reg_loss: 0.000), lr: 7.066139061616733e-05\n",
      "step: 468, acc: 0.635, loss: 1.016 (data_loss: 1.016, reg_loss: 0.000), lr: 7.032348804500702e-05\n",
      "training, acc: 0.671, loss: 0.845 (data_loss: 0.845, reg_loss: 0.000), lr: 7.032348804500702e-05\n",
      "validation, acc: 0.777, loss: 0.605\n",
      "epoch: 10\n",
      "step: 0, acc: 0.688, loss: 0.787 (data_loss: 0.787, reg_loss: 0.000), lr: 7.031854299978905e-05\n",
      "step: 100, acc: 0.750, loss: 0.966 (data_loss: 0.966, reg_loss: 0.000), lr: 6.982752601075344e-05\n",
      "step: 200, acc: 0.641, loss: 0.779 (data_loss: 0.779, reg_loss: 0.000), lr: 6.93433187712364e-05\n",
      "step: 300, acc: 0.703, loss: 0.789 (data_loss: 0.789, reg_loss: 0.000), lr: 6.886578059362304e-05\n",
      "step: 400, acc: 0.766, loss: 0.775 (data_loss: 0.775, reg_loss: 0.000), lr: 6.839477463921757e-05\n",
      "step: 468, acc: 0.677, loss: 1.099 (data_loss: 1.099, reg_loss: 0.000), lr: 6.80781537204711e-05\n",
      "training, acc: 0.676, loss: 0.834 (data_loss: 0.834, reg_loss: 0.000), lr: 6.80781537204711e-05\n",
      "validation, acc: 0.779, loss: 0.592\n"
     ]
    }
   ],
   "source": [
    "# Train models with different learning rates\n",
    "test_learning_rates = [0.5, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
    "\n",
    "for learning_rate in test_learning_rates:\n",
    "   Train_Model(X=X, y=y, X_test=X_test, y_test=y_test, model_number=\"_learning_rate\"+str(learning_rate), learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with learning rate 0.5:\n",
      "validation, acc: 0.100, loss: 2.337\n",
      "Model with learning rate 0.1:\n",
      "validation, acc: 0.100, loss: 2.309\n",
      "Model with learning rate 0.05:\n",
      "validation, acc: 0.100, loss: 2.309\n",
      "Model with learning rate 0.01:\n",
      "validation, acc: 0.100, loss: 2.303\n",
      "Model with learning rate 0.005:\n",
      "validation, acc: 0.612, loss: 0.991\n",
      "Model with learning rate 0.001:\n",
      "validation, acc: 0.822, loss: 0.499\n",
      "Model with learning rate 0.0005:\n",
      "validation, acc: 0.818, loss: 0.492\n",
      "Model with learning rate 0.0001:\n",
      "validation, acc: 0.779, loss: 0.592\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models\n",
    "print(\"Model with learning rate 0.5:\")\n",
    "model = Model.load(\"fashion_mnist_learning_rate0.5.model\")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Model with learning rate 0.1:\")\n",
    "model = Model.load(\"fashion_mnist_learning_rate0.1.model\")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Model with learning rate 0.05:\")\n",
    "model = Model.load(\"fashion_mnist_learning_rate0.05.model\")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Model with learning rate 0.01:\")\n",
    "model = Model.load(\"fashion_mnist_learning_rate0.01.model\")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Model with learning rate 0.005:\")\n",
    "model = Model.load(\"fashion_mnist_learning_rate0.005.model\")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Model with learning rate 0.001:\")\n",
    "model = Model.load(\"fashion_mnist_learning_rate0.001.model\")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Model with learning rate 0.0005:\")\n",
    "model = Model.load(\"fashion_mnist_learning_rate0.0005.model\")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Model with learning rate 0.0001:\")\n",
    "model = Model.load(\"fashion_mnist_learning_rate0.0001.model\")\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0, acc: 0.125, loss: 2.302 (data_loss: 2.302, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.742, loss: 0.751 (data_loss: 0.751, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 200, acc: 0.836, loss: 0.542 (data_loss: 0.542, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 300, acc: 0.836, loss: 0.459 (data_loss: 0.459, reg_loss: 0.000), lr: 0.0009708737864077671\n",
      "step: 400, acc: 0.820, loss: 0.533 (data_loss: 0.533, reg_loss: 0.000), lr: 0.0009615384615384615\n",
      "step: 468, acc: 0.802, loss: 0.633 (data_loss: 0.633, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "training, acc: 0.745, loss: 0.700 (data_loss: 0.700, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "validation, acc: 0.820, loss: 0.494\n",
      "epoch: 2\n",
      "step: 0, acc: 0.773, loss: 0.508 (data_loss: 0.508, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.844, loss: 0.558 (data_loss: 0.558, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 200, acc: 0.836, loss: 0.489 (data_loss: 0.489, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 300, acc: 0.836, loss: 0.383 (data_loss: 0.383, reg_loss: 0.000), lr: 0.0009285913269570063\n",
      "step: 400, acc: 0.836, loss: 0.556 (data_loss: 0.556, reg_loss: 0.000), lr: 0.0009200478424878093\n",
      "step: 468, acc: 0.812, loss: 0.617 (data_loss: 0.617, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "training, acc: 0.826, loss: 0.487 (data_loss: 0.487, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "validation, acc: 0.836, loss: 0.456\n",
      "epoch: 3\n",
      "step: 0, acc: 0.820, loss: 0.444 (data_loss: 0.444, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.828, loss: 0.547 (data_loss: 0.547, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 200, acc: 0.836, loss: 0.417 (data_loss: 0.417, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 300, acc: 0.836, loss: 0.382 (data_loss: 0.382, reg_loss: 0.000), lr: 0.0008898380494749957\n",
      "step: 400, acc: 0.844, loss: 0.528 (data_loss: 0.528, reg_loss: 0.000), lr: 0.0008819897689186807\n",
      "step: 468, acc: 0.802, loss: 0.608 (data_loss: 0.608, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "training, acc: 0.838, loss: 0.449 (data_loss: 0.449, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "validation, acc: 0.836, loss: 0.447\n",
      "epoch: 4\n",
      "step: 0, acc: 0.844, loss: 0.425 (data_loss: 0.425, reg_loss: 0.000), lr: 0.0008766546857192952\n",
      "step: 100, acc: 0.797, loss: 0.570 (data_loss: 0.570, reg_loss: 0.000), lr: 0.0008690362388111585\n",
      "step: 200, acc: 0.844, loss: 0.454 (data_loss: 0.454, reg_loss: 0.000), lr: 0.0008615490652192642\n",
      "step: 300, acc: 0.859, loss: 0.363 (data_loss: 0.363, reg_loss: 0.000), lr: 0.0008541898009737763\n",
      "step: 400, acc: 0.852, loss: 0.447 (data_loss: 0.447, reg_loss: 0.000), lr: 0.0008469551960701278\n",
      "step: 468, acc: 0.833, loss: 0.527 (data_loss: 0.527, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "training, acc: 0.845, loss: 0.430 (data_loss: 0.430, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "validation, acc: 0.844, loss: 0.432\n",
      "epoch: 5\n",
      "step: 0, acc: 0.836, loss: 0.463 (data_loss: 0.463, reg_loss: 0.000), lr: 0.0008420343550016842\n",
      "step: 100, acc: 0.820, loss: 0.548 (data_loss: 0.548, reg_loss: 0.000), lr: 0.0008350033400133601\n",
      "step: 200, acc: 0.875, loss: 0.360 (data_loss: 0.360, reg_loss: 0.000), lr: 0.0008280887711162637\n",
      "step: 300, acc: 0.828, loss: 0.366 (data_loss: 0.366, reg_loss: 0.000), lr: 0.0008212877792378449\n",
      "step: 400, acc: 0.891, loss: 0.400 (data_loss: 0.400, reg_loss: 0.000), lr: 0.0008145975887911372\n",
      "step: 468, acc: 0.865, loss: 0.450 (data_loss: 0.450, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "training, acc: 0.850, loss: 0.415 (data_loss: 0.415, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "validation, acc: 0.844, loss: 0.421\n",
      "epoch: 6\n",
      "step: 0, acc: 0.844, loss: 0.415 (data_loss: 0.415, reg_loss: 0.000), lr: 0.0008100445524503849\n",
      "step: 100, acc: 0.828, loss: 0.526 (data_loss: 0.526, reg_loss: 0.000), lr: 0.0008035355564483729\n",
      "step: 200, acc: 0.875, loss: 0.371 (data_loss: 0.371, reg_loss: 0.000), lr: 0.0007971303308090873\n",
      "step: 300, acc: 0.859, loss: 0.337 (data_loss: 0.337, reg_loss: 0.000), lr: 0.0007908264136022144\n",
      "step: 400, acc: 0.844, loss: 0.481 (data_loss: 0.481, reg_loss: 0.000), lr: 0.0007846214201647706\n",
      "step: 468, acc: 0.844, loss: 0.529 (data_loss: 0.529, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "training, acc: 0.854, loss: 0.406 (data_loss: 0.406, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "validation, acc: 0.851, loss: 0.412\n",
      "epoch: 7\n",
      "step: 0, acc: 0.828, loss: 0.405 (data_loss: 0.405, reg_loss: 0.000), lr: 0.0007803964413922272\n",
      "step: 100, acc: 0.820, loss: 0.548 (data_loss: 0.548, reg_loss: 0.000), lr: 0.0007743534148985598\n",
      "step: 200, acc: 0.875, loss: 0.377 (data_loss: 0.377, reg_loss: 0.000), lr: 0.0007684032580298141\n",
      "step: 300, acc: 0.883, loss: 0.358 (data_loss: 0.358, reg_loss: 0.000), lr: 0.0007625438462711606\n",
      "step: 400, acc: 0.852, loss: 0.386 (data_loss: 0.386, reg_loss: 0.000), lr: 0.0007567731194187983\n",
      "step: 468, acc: 0.854, loss: 0.450 (data_loss: 0.450, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "training, acc: 0.858, loss: 0.395 (data_loss: 0.395, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "validation, acc: 0.855, loss: 0.406\n",
      "epoch: 8\n",
      "step: 0, acc: 0.867, loss: 0.401 (data_loss: 0.401, reg_loss: 0.000), lr: 0.0007528419784687194\n",
      "step: 100, acc: 0.812, loss: 0.586 (data_loss: 0.586, reg_loss: 0.000), lr: 0.0007472166180975864\n",
      "step: 200, acc: 0.852, loss: 0.381 (data_loss: 0.381, reg_loss: 0.000), lr: 0.0007416747014759327\n",
      "step: 300, acc: 0.875, loss: 0.315 (data_loss: 0.315, reg_loss: 0.000), lr: 0.0007362143856290952\n",
      "step: 400, acc: 0.867, loss: 0.407 (data_loss: 0.407, reg_loss: 0.000), lr: 0.0007308338814587444\n",
      "step: 468, acc: 0.854, loss: 0.491 (data_loss: 0.491, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "training, acc: 0.860, loss: 0.386 (data_loss: 0.386, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "validation, acc: 0.854, loss: 0.400\n",
      "epoch: 9\n",
      "step: 0, acc: 0.875, loss: 0.387 (data_loss: 0.387, reg_loss: 0.000), lr: 0.0007271669575334498\n",
      "step: 100, acc: 0.859, loss: 0.484 (data_loss: 0.484, reg_loss: 0.000), lr: 0.000721917412647993\n",
      "step: 200, acc: 0.859, loss: 0.369 (data_loss: 0.369, reg_loss: 0.000), lr: 0.0007167431192660551\n",
      "step: 300, acc: 0.867, loss: 0.337 (data_loss: 0.337, reg_loss: 0.000), lr: 0.0007116424708226587\n",
      "step: 400, acc: 0.852, loss: 0.444 (data_loss: 0.444, reg_loss: 0.000), lr: 0.0007066139061616733\n",
      "step: 468, acc: 0.823, loss: 0.577 (data_loss: 0.577, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "training, acc: 0.861, loss: 0.381 (data_loss: 0.381, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "validation, acc: 0.858, loss: 0.397\n",
      "epoch: 10\n",
      "step: 0, acc: 0.867, loss: 0.391 (data_loss: 0.391, reg_loss: 0.000), lr: 0.0007031854299978904\n",
      "step: 100, acc: 0.844, loss: 0.512 (data_loss: 0.512, reg_loss: 0.000), lr: 0.0006982752601075343\n",
      "step: 200, acc: 0.859, loss: 0.366 (data_loss: 0.366, reg_loss: 0.000), lr: 0.000693433187712364\n",
      "step: 300, acc: 0.883, loss: 0.340 (data_loss: 0.340, reg_loss: 0.000), lr: 0.0006886578059362303\n",
      "step: 400, acc: 0.836, loss: 0.431 (data_loss: 0.431, reg_loss: 0.000), lr: 0.0006839477463921757\n",
      "step: 468, acc: 0.833, loss: 0.534 (data_loss: 0.534, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "training, acc: 0.864, loss: 0.374 (data_loss: 0.374, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "validation, acc: 0.859, loss: 0.396\n",
      "epoch: 1\n",
      "step: 0, acc: 0.070, loss: 2.302 (data_loss: 2.302, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.688, loss: 0.848 (data_loss: 0.848, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 200, acc: 0.805, loss: 0.569 (data_loss: 0.569, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 300, acc: 0.820, loss: 0.499 (data_loss: 0.499, reg_loss: 0.000), lr: 0.0009708737864077671\n",
      "step: 400, acc: 0.828, loss: 0.545 (data_loss: 0.545, reg_loss: 0.000), lr: 0.0009615384615384615\n",
      "step: 468, acc: 0.750, loss: 0.649 (data_loss: 0.649, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "training, acc: 0.728, loss: 0.734 (data_loss: 0.734, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "validation, acc: 0.817, loss: 0.505\n",
      "epoch: 2\n",
      "step: 0, acc: 0.750, loss: 0.569 (data_loss: 0.569, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.828, loss: 0.640 (data_loss: 0.640, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 200, acc: 0.852, loss: 0.466 (data_loss: 0.466, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 300, acc: 0.844, loss: 0.413 (data_loss: 0.413, reg_loss: 0.000), lr: 0.0009285913269570063\n",
      "step: 400, acc: 0.844, loss: 0.498 (data_loss: 0.498, reg_loss: 0.000), lr: 0.0009200478424878093\n",
      "step: 468, acc: 0.802, loss: 0.600 (data_loss: 0.600, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "training, acc: 0.817, loss: 0.515 (data_loss: 0.515, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "validation, acc: 0.831, loss: 0.471\n",
      "epoch: 3\n",
      "step: 0, acc: 0.797, loss: 0.494 (data_loss: 0.494, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.805, loss: 0.557 (data_loss: 0.557, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 200, acc: 0.859, loss: 0.458 (data_loss: 0.458, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 300, acc: 0.820, loss: 0.434 (data_loss: 0.434, reg_loss: 0.000), lr: 0.0008898380494749957\n",
      "step: 400, acc: 0.828, loss: 0.547 (data_loss: 0.547, reg_loss: 0.000), lr: 0.0008819897689186807\n",
      "step: 468, acc: 0.833, loss: 0.546 (data_loss: 0.546, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "training, acc: 0.830, loss: 0.479 (data_loss: 0.479, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "validation, acc: 0.840, loss: 0.443\n",
      "epoch: 4\n",
      "step: 0, acc: 0.781, loss: 0.490 (data_loss: 0.490, reg_loss: 0.000), lr: 0.0008766546857192952\n",
      "step: 100, acc: 0.805, loss: 0.560 (data_loss: 0.560, reg_loss: 0.000), lr: 0.0008690362388111585\n",
      "step: 200, acc: 0.852, loss: 0.460 (data_loss: 0.460, reg_loss: 0.000), lr: 0.0008615490652192642\n",
      "step: 300, acc: 0.867, loss: 0.385 (data_loss: 0.385, reg_loss: 0.000), lr: 0.0008541898009737763\n",
      "step: 400, acc: 0.859, loss: 0.517 (data_loss: 0.517, reg_loss: 0.000), lr: 0.0008469551960701278\n",
      "step: 468, acc: 0.812, loss: 0.627 (data_loss: 0.627, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "training, acc: 0.838, loss: 0.455 (data_loss: 0.455, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "validation, acc: 0.839, loss: 0.437\n",
      "epoch: 5\n",
      "step: 0, acc: 0.844, loss: 0.429 (data_loss: 0.429, reg_loss: 0.000), lr: 0.0008420343550016842\n",
      "step: 100, acc: 0.812, loss: 0.591 (data_loss: 0.591, reg_loss: 0.000), lr: 0.0008350033400133601\n",
      "step: 200, acc: 0.875, loss: 0.368 (data_loss: 0.368, reg_loss: 0.000), lr: 0.0008280887711162637\n",
      "step: 300, acc: 0.875, loss: 0.379 (data_loss: 0.379, reg_loss: 0.000), lr: 0.0008212877792378449\n",
      "step: 400, acc: 0.844, loss: 0.447 (data_loss: 0.447, reg_loss: 0.000), lr: 0.0008145975887911372\n",
      "step: 468, acc: 0.833, loss: 0.573 (data_loss: 0.573, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "training, acc: 0.843, loss: 0.442 (data_loss: 0.442, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "validation, acc: 0.842, loss: 0.426\n",
      "epoch: 6\n",
      "step: 0, acc: 0.875, loss: 0.410 (data_loss: 0.410, reg_loss: 0.000), lr: 0.0008100445524503849\n",
      "step: 100, acc: 0.797, loss: 0.646 (data_loss: 0.646, reg_loss: 0.000), lr: 0.0008035355564483729\n",
      "step: 200, acc: 0.844, loss: 0.385 (data_loss: 0.385, reg_loss: 0.000), lr: 0.0007971303308090873\n",
      "step: 300, acc: 0.844, loss: 0.365 (data_loss: 0.365, reg_loss: 0.000), lr: 0.0007908264136022144\n",
      "step: 400, acc: 0.859, loss: 0.458 (data_loss: 0.458, reg_loss: 0.000), lr: 0.0007846214201647706\n",
      "step: 468, acc: 0.771, loss: 0.586 (data_loss: 0.586, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "training, acc: 0.846, loss: 0.432 (data_loss: 0.432, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "validation, acc: 0.845, loss: 0.426\n",
      "epoch: 7\n",
      "step: 0, acc: 0.836, loss: 0.436 (data_loss: 0.436, reg_loss: 0.000), lr: 0.0007803964413922272\n",
      "step: 100, acc: 0.812, loss: 0.522 (data_loss: 0.522, reg_loss: 0.000), lr: 0.0007743534148985598\n",
      "step: 200, acc: 0.867, loss: 0.411 (data_loss: 0.411, reg_loss: 0.000), lr: 0.0007684032580298141\n",
      "step: 300, acc: 0.836, loss: 0.377 (data_loss: 0.377, reg_loss: 0.000), lr: 0.0007625438462711606\n",
      "step: 400, acc: 0.789, loss: 0.479 (data_loss: 0.479, reg_loss: 0.000), lr: 0.0007567731194187983\n",
      "step: 468, acc: 0.802, loss: 0.591 (data_loss: 0.591, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "training, acc: 0.849, loss: 0.422 (data_loss: 0.422, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "validation, acc: 0.846, loss: 0.419\n",
      "epoch: 8\n",
      "step: 0, acc: 0.836, loss: 0.461 (data_loss: 0.461, reg_loss: 0.000), lr: 0.0007528419784687194\n",
      "step: 100, acc: 0.820, loss: 0.503 (data_loss: 0.503, reg_loss: 0.000), lr: 0.0007472166180975864\n",
      "step: 200, acc: 0.852, loss: 0.366 (data_loss: 0.366, reg_loss: 0.000), lr: 0.0007416747014759327\n",
      "step: 300, acc: 0.867, loss: 0.396 (data_loss: 0.396, reg_loss: 0.000), lr: 0.0007362143856290952\n",
      "step: 400, acc: 0.828, loss: 0.424 (data_loss: 0.424, reg_loss: 0.000), lr: 0.0007308338814587444\n",
      "step: 468, acc: 0.865, loss: 0.546 (data_loss: 0.546, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "training, acc: 0.852, loss: 0.413 (data_loss: 0.413, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "validation, acc: 0.843, loss: 0.425\n",
      "epoch: 9\n",
      "step: 0, acc: 0.828, loss: 0.480 (data_loss: 0.480, reg_loss: 0.000), lr: 0.0007271669575334498\n",
      "step: 100, acc: 0.820, loss: 0.488 (data_loss: 0.488, reg_loss: 0.000), lr: 0.000721917412647993\n",
      "step: 200, acc: 0.852, loss: 0.424 (data_loss: 0.424, reg_loss: 0.000), lr: 0.0007167431192660551\n",
      "step: 300, acc: 0.875, loss: 0.388 (data_loss: 0.388, reg_loss: 0.000), lr: 0.0007116424708226587\n",
      "step: 400, acc: 0.852, loss: 0.363 (data_loss: 0.363, reg_loss: 0.000), lr: 0.0007066139061616733\n",
      "step: 468, acc: 0.854, loss: 0.523 (data_loss: 0.523, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "training, acc: 0.853, loss: 0.410 (data_loss: 0.410, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "validation, acc: 0.850, loss: 0.412\n",
      "epoch: 10\n",
      "step: 0, acc: 0.859, loss: 0.411 (data_loss: 0.411, reg_loss: 0.000), lr: 0.0007031854299978904\n",
      "step: 100, acc: 0.828, loss: 0.523 (data_loss: 0.523, reg_loss: 0.000), lr: 0.0006982752601075343\n",
      "step: 200, acc: 0.891, loss: 0.320 (data_loss: 0.320, reg_loss: 0.000), lr: 0.000693433187712364\n",
      "step: 300, acc: 0.836, loss: 0.376 (data_loss: 0.376, reg_loss: 0.000), lr: 0.0006886578059362303\n",
      "step: 400, acc: 0.852, loss: 0.393 (data_loss: 0.393, reg_loss: 0.000), lr: 0.0006839477463921757\n",
      "step: 468, acc: 0.823, loss: 0.588 (data_loss: 0.588, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "training, acc: 0.854, loss: 0.403 (data_loss: 0.403, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "validation, acc: 0.849, loss: 0.417\n",
      "epoch: 1\n",
      "step: 0, acc: 0.055, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.688, loss: 0.847 (data_loss: 0.847, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 200, acc: 0.758, loss: 0.622 (data_loss: 0.622, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 300, acc: 0.797, loss: 0.558 (data_loss: 0.558, reg_loss: 0.000), lr: 0.0009708737864077671\n",
      "step: 400, acc: 0.820, loss: 0.579 (data_loss: 0.579, reg_loss: 0.000), lr: 0.0009615384615384615\n",
      "step: 468, acc: 0.802, loss: 0.759 (data_loss: 0.759, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "training, acc: 0.711, loss: 0.788 (data_loss: 0.788, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "validation, acc: 0.807, loss: 0.522\n",
      "epoch: 2\n",
      "step: 0, acc: 0.773, loss: 0.576 (data_loss: 0.576, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.766, loss: 0.605 (data_loss: 0.605, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 200, acc: 0.820, loss: 0.518 (data_loss: 0.518, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 300, acc: 0.852, loss: 0.423 (data_loss: 0.423, reg_loss: 0.000), lr: 0.0009285913269570063\n",
      "step: 400, acc: 0.828, loss: 0.525 (data_loss: 0.525, reg_loss: 0.000), lr: 0.0009200478424878093\n",
      "step: 468, acc: 0.750, loss: 0.700 (data_loss: 0.700, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "training, acc: 0.804, loss: 0.556 (data_loss: 0.556, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "validation, acc: 0.825, loss: 0.480\n",
      "epoch: 3\n",
      "step: 0, acc: 0.781, loss: 0.620 (data_loss: 0.620, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.805, loss: 0.680 (data_loss: 0.680, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 200, acc: 0.844, loss: 0.466 (data_loss: 0.466, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 300, acc: 0.828, loss: 0.425 (data_loss: 0.425, reg_loss: 0.000), lr: 0.0008898380494749957\n",
      "step: 400, acc: 0.820, loss: 0.502 (data_loss: 0.502, reg_loss: 0.000), lr: 0.0008819897689186807\n",
      "step: 468, acc: 0.823, loss: 0.579 (data_loss: 0.579, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "training, acc: 0.817, loss: 0.516 (data_loss: 0.516, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "validation, acc: 0.831, loss: 0.460\n",
      "epoch: 4\n",
      "step: 0, acc: 0.797, loss: 0.484 (data_loss: 0.484, reg_loss: 0.000), lr: 0.0008766546857192952\n",
      "step: 100, acc: 0.805, loss: 0.564 (data_loss: 0.564, reg_loss: 0.000), lr: 0.0008690362388111585\n",
      "step: 200, acc: 0.844, loss: 0.464 (data_loss: 0.464, reg_loss: 0.000), lr: 0.0008615490652192642\n",
      "step: 300, acc: 0.820, loss: 0.473 (data_loss: 0.473, reg_loss: 0.000), lr: 0.0008541898009737763\n",
      "step: 400, acc: 0.883, loss: 0.506 (data_loss: 0.506, reg_loss: 0.000), lr: 0.0008469551960701278\n",
      "step: 468, acc: 0.844, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "training, acc: 0.826, loss: 0.493 (data_loss: 0.493, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "validation, acc: 0.833, loss: 0.456\n",
      "epoch: 5\n",
      "step: 0, acc: 0.828, loss: 0.545 (data_loss: 0.545, reg_loss: 0.000), lr: 0.0008420343550016842\n",
      "step: 100, acc: 0.820, loss: 0.538 (data_loss: 0.538, reg_loss: 0.000), lr: 0.0008350033400133601\n",
      "step: 200, acc: 0.852, loss: 0.438 (data_loss: 0.438, reg_loss: 0.000), lr: 0.0008280887711162637\n",
      "step: 300, acc: 0.844, loss: 0.391 (data_loss: 0.391, reg_loss: 0.000), lr: 0.0008212877792378449\n",
      "step: 400, acc: 0.789, loss: 0.522 (data_loss: 0.522, reg_loss: 0.000), lr: 0.0008145975887911372\n",
      "step: 468, acc: 0.823, loss: 0.614 (data_loss: 0.614, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "training, acc: 0.829, loss: 0.481 (data_loss: 0.481, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "validation, acc: 0.841, loss: 0.440\n",
      "epoch: 6\n",
      "step: 0, acc: 0.828, loss: 0.471 (data_loss: 0.471, reg_loss: 0.000), lr: 0.0008100445524503849\n",
      "step: 100, acc: 0.828, loss: 0.470 (data_loss: 0.470, reg_loss: 0.000), lr: 0.0008035355564483729\n",
      "step: 200, acc: 0.828, loss: 0.460 (data_loss: 0.460, reg_loss: 0.000), lr: 0.0007971303308090873\n",
      "step: 300, acc: 0.836, loss: 0.365 (data_loss: 0.365, reg_loss: 0.000), lr: 0.0007908264136022144\n",
      "step: 400, acc: 0.836, loss: 0.503 (data_loss: 0.503, reg_loss: 0.000), lr: 0.0007846214201647706\n",
      "step: 468, acc: 0.823, loss: 0.663 (data_loss: 0.663, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "training, acc: 0.833, loss: 0.474 (data_loss: 0.474, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "validation, acc: 0.846, loss: 0.424\n",
      "epoch: 7\n",
      "step: 0, acc: 0.836, loss: 0.412 (data_loss: 0.412, reg_loss: 0.000), lr: 0.0007803964413922272\n",
      "step: 100, acc: 0.852, loss: 0.505 (data_loss: 0.505, reg_loss: 0.000), lr: 0.0007743534148985598\n",
      "step: 200, acc: 0.836, loss: 0.399 (data_loss: 0.399, reg_loss: 0.000), lr: 0.0007684032580298141\n",
      "step: 300, acc: 0.844, loss: 0.405 (data_loss: 0.405, reg_loss: 0.000), lr: 0.0007625438462711606\n",
      "step: 400, acc: 0.852, loss: 0.465 (data_loss: 0.465, reg_loss: 0.000), lr: 0.0007567731194187983\n",
      "step: 468, acc: 0.844, loss: 0.582 (data_loss: 0.582, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "training, acc: 0.837, loss: 0.458 (data_loss: 0.458, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "validation, acc: 0.848, loss: 0.422\n",
      "epoch: 8\n",
      "step: 0, acc: 0.836, loss: 0.405 (data_loss: 0.405, reg_loss: 0.000), lr: 0.0007528419784687194\n",
      "step: 100, acc: 0.797, loss: 0.594 (data_loss: 0.594, reg_loss: 0.000), lr: 0.0007472166180975864\n",
      "step: 200, acc: 0.867, loss: 0.365 (data_loss: 0.365, reg_loss: 0.000), lr: 0.0007416747014759327\n",
      "step: 300, acc: 0.875, loss: 0.406 (data_loss: 0.406, reg_loss: 0.000), lr: 0.0007362143856290952\n",
      "step: 400, acc: 0.859, loss: 0.493 (data_loss: 0.493, reg_loss: 0.000), lr: 0.0007308338814587444\n",
      "step: 468, acc: 0.812, loss: 0.589 (data_loss: 0.589, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "training, acc: 0.837, loss: 0.454 (data_loss: 0.454, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "validation, acc: 0.847, loss: 0.422\n",
      "epoch: 9\n",
      "step: 0, acc: 0.836, loss: 0.418 (data_loss: 0.418, reg_loss: 0.000), lr: 0.0007271669575334498\n",
      "step: 100, acc: 0.812, loss: 0.583 (data_loss: 0.583, reg_loss: 0.000), lr: 0.000721917412647993\n",
      "step: 200, acc: 0.867, loss: 0.381 (data_loss: 0.381, reg_loss: 0.000), lr: 0.0007167431192660551\n",
      "step: 300, acc: 0.867, loss: 0.389 (data_loss: 0.389, reg_loss: 0.000), lr: 0.0007116424708226587\n",
      "step: 400, acc: 0.867, loss: 0.402 (data_loss: 0.402, reg_loss: 0.000), lr: 0.0007066139061616733\n",
      "step: 468, acc: 0.833, loss: 0.587 (data_loss: 0.587, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "training, acc: 0.840, loss: 0.450 (data_loss: 0.450, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "validation, acc: 0.850, loss: 0.414\n",
      "epoch: 10\n",
      "step: 0, acc: 0.852, loss: 0.393 (data_loss: 0.393, reg_loss: 0.000), lr: 0.0007031854299978904\n",
      "step: 100, acc: 0.820, loss: 0.479 (data_loss: 0.479, reg_loss: 0.000), lr: 0.0006982752601075343\n",
      "step: 200, acc: 0.844, loss: 0.407 (data_loss: 0.407, reg_loss: 0.000), lr: 0.000693433187712364\n",
      "step: 300, acc: 0.898, loss: 0.364 (data_loss: 0.364, reg_loss: 0.000), lr: 0.0006886578059362303\n",
      "step: 400, acc: 0.828, loss: 0.469 (data_loss: 0.469, reg_loss: 0.000), lr: 0.0006839477463921757\n",
      "step: 468, acc: 0.802, loss: 0.572 (data_loss: 0.572, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "training, acc: 0.842, loss: 0.445 (data_loss: 0.445, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "validation, acc: 0.846, loss: 0.414\n",
      "epoch: 1\n",
      "step: 0, acc: 0.094, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.672, loss: 0.923 (data_loss: 0.923, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 200, acc: 0.805, loss: 0.634 (data_loss: 0.634, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 300, acc: 0.852, loss: 0.540 (data_loss: 0.540, reg_loss: 0.000), lr: 0.0009708737864077671\n",
      "step: 400, acc: 0.781, loss: 0.743 (data_loss: 0.743, reg_loss: 0.000), lr: 0.0009615384615384615\n",
      "step: 468, acc: 0.719, loss: 0.878 (data_loss: 0.878, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "training, acc: 0.682, loss: 0.849 (data_loss: 0.849, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "validation, acc: 0.796, loss: 0.547\n",
      "epoch: 2\n",
      "step: 0, acc: 0.758, loss: 0.623 (data_loss: 0.623, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.781, loss: 0.755 (data_loss: 0.755, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 200, acc: 0.766, loss: 0.582 (data_loss: 0.582, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 300, acc: 0.789, loss: 0.528 (data_loss: 0.528, reg_loss: 0.000), lr: 0.0009285913269570063\n",
      "step: 400, acc: 0.781, loss: 0.621 (data_loss: 0.621, reg_loss: 0.000), lr: 0.0009200478424878093\n",
      "step: 468, acc: 0.760, loss: 0.729 (data_loss: 0.729, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "training, acc: 0.776, loss: 0.631 (data_loss: 0.631, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "validation, acc: 0.814, loss: 0.513\n",
      "epoch: 3\n",
      "step: 0, acc: 0.797, loss: 0.556 (data_loss: 0.556, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.797, loss: 0.553 (data_loss: 0.553, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 200, acc: 0.852, loss: 0.564 (data_loss: 0.564, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 300, acc: 0.797, loss: 0.514 (data_loss: 0.514, reg_loss: 0.000), lr: 0.0008898380494749957\n",
      "step: 400, acc: 0.812, loss: 0.550 (data_loss: 0.550, reg_loss: 0.000), lr: 0.0008819897689186807\n",
      "step: 468, acc: 0.802, loss: 0.727 (data_loss: 0.727, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "training, acc: 0.792, loss: 0.589 (data_loss: 0.589, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "validation, acc: 0.825, loss: 0.490\n",
      "epoch: 4\n",
      "step: 0, acc: 0.773, loss: 0.572 (data_loss: 0.572, reg_loss: 0.000), lr: 0.0008766546857192952\n",
      "step: 100, acc: 0.812, loss: 0.669 (data_loss: 0.669, reg_loss: 0.000), lr: 0.0008690362388111585\n",
      "step: 200, acc: 0.812, loss: 0.514 (data_loss: 0.514, reg_loss: 0.000), lr: 0.0008615490652192642\n",
      "step: 300, acc: 0.805, loss: 0.494 (data_loss: 0.494, reg_loss: 0.000), lr: 0.0008541898009737763\n",
      "step: 400, acc: 0.844, loss: 0.484 (data_loss: 0.484, reg_loss: 0.000), lr: 0.0008469551960701278\n",
      "step: 468, acc: 0.802, loss: 0.704 (data_loss: 0.704, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "training, acc: 0.798, loss: 0.572 (data_loss: 0.572, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "validation, acc: 0.830, loss: 0.471\n",
      "epoch: 5\n",
      "step: 0, acc: 0.797, loss: 0.491 (data_loss: 0.491, reg_loss: 0.000), lr: 0.0008420343550016842\n",
      "step: 100, acc: 0.820, loss: 0.675 (data_loss: 0.675, reg_loss: 0.000), lr: 0.0008350033400133601\n",
      "step: 200, acc: 0.805, loss: 0.529 (data_loss: 0.529, reg_loss: 0.000), lr: 0.0008280887711162637\n",
      "step: 300, acc: 0.852, loss: 0.477 (data_loss: 0.477, reg_loss: 0.000), lr: 0.0008212877792378449\n",
      "step: 400, acc: 0.805, loss: 0.538 (data_loss: 0.538, reg_loss: 0.000), lr: 0.0008145975887911372\n",
      "step: 468, acc: 0.750, loss: 0.586 (data_loss: 0.586, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "training, acc: 0.802, loss: 0.560 (data_loss: 0.560, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "validation, acc: 0.833, loss: 0.465\n",
      "epoch: 6\n",
      "step: 0, acc: 0.766, loss: 0.519 (data_loss: 0.519, reg_loss: 0.000), lr: 0.0008100445524503849\n",
      "step: 100, acc: 0.812, loss: 0.640 (data_loss: 0.640, reg_loss: 0.000), lr: 0.0008035355564483729\n",
      "step: 200, acc: 0.797, loss: 0.538 (data_loss: 0.538, reg_loss: 0.000), lr: 0.0007971303308090873\n",
      "step: 300, acc: 0.828, loss: 0.492 (data_loss: 0.492, reg_loss: 0.000), lr: 0.0007908264136022144\n",
      "step: 400, acc: 0.844, loss: 0.503 (data_loss: 0.503, reg_loss: 0.000), lr: 0.0007846214201647706\n",
      "step: 468, acc: 0.740, loss: 0.678 (data_loss: 0.678, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "training, acc: 0.808, loss: 0.544 (data_loss: 0.544, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "validation, acc: 0.829, loss: 0.470\n",
      "epoch: 7\n",
      "step: 0, acc: 0.766, loss: 0.569 (data_loss: 0.569, reg_loss: 0.000), lr: 0.0007803964413922272\n",
      "step: 100, acc: 0.805, loss: 0.681 (data_loss: 0.681, reg_loss: 0.000), lr: 0.0007743534148985598\n",
      "step: 200, acc: 0.797, loss: 0.457 (data_loss: 0.457, reg_loss: 0.000), lr: 0.0007684032580298141\n",
      "step: 300, acc: 0.805, loss: 0.458 (data_loss: 0.458, reg_loss: 0.000), lr: 0.0007625438462711606\n",
      "step: 400, acc: 0.875, loss: 0.490 (data_loss: 0.490, reg_loss: 0.000), lr: 0.0007567731194187983\n",
      "step: 468, acc: 0.781, loss: 0.714 (data_loss: 0.714, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "training, acc: 0.808, loss: 0.542 (data_loss: 0.542, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "validation, acc: 0.834, loss: 0.459\n",
      "epoch: 8\n",
      "step: 0, acc: 0.750, loss: 0.527 (data_loss: 0.527, reg_loss: 0.000), lr: 0.0007528419784687194\n",
      "step: 100, acc: 0.781, loss: 0.599 (data_loss: 0.599, reg_loss: 0.000), lr: 0.0007472166180975864\n",
      "step: 200, acc: 0.820, loss: 0.483 (data_loss: 0.483, reg_loss: 0.000), lr: 0.0007416747014759327\n",
      "step: 300, acc: 0.828, loss: 0.471 (data_loss: 0.471, reg_loss: 0.000), lr: 0.0007362143856290952\n",
      "step: 400, acc: 0.844, loss: 0.565 (data_loss: 0.565, reg_loss: 0.000), lr: 0.0007308338814587444\n",
      "step: 468, acc: 0.781, loss: 0.697 (data_loss: 0.697, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "training, acc: 0.813, loss: 0.525 (data_loss: 0.525, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "validation, acc: 0.838, loss: 0.444\n",
      "epoch: 9\n",
      "step: 0, acc: 0.773, loss: 0.509 (data_loss: 0.509, reg_loss: 0.000), lr: 0.0007271669575334498\n",
      "step: 100, acc: 0.820, loss: 0.563 (data_loss: 0.563, reg_loss: 0.000), lr: 0.000721917412647993\n",
      "step: 200, acc: 0.844, loss: 0.510 (data_loss: 0.510, reg_loss: 0.000), lr: 0.0007167431192660551\n",
      "step: 300, acc: 0.844, loss: 0.418 (data_loss: 0.418, reg_loss: 0.000), lr: 0.0007116424708226587\n",
      "step: 400, acc: 0.844, loss: 0.420 (data_loss: 0.420, reg_loss: 0.000), lr: 0.0007066139061616733\n",
      "step: 468, acc: 0.792, loss: 0.701 (data_loss: 0.701, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "training, acc: 0.814, loss: 0.521 (data_loss: 0.521, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "validation, acc: 0.840, loss: 0.448\n",
      "epoch: 10\n",
      "step: 0, acc: 0.836, loss: 0.528 (data_loss: 0.528, reg_loss: 0.000), lr: 0.0007031854299978904\n",
      "step: 100, acc: 0.773, loss: 0.661 (data_loss: 0.661, reg_loss: 0.000), lr: 0.0006982752601075343\n",
      "step: 200, acc: 0.805, loss: 0.502 (data_loss: 0.502, reg_loss: 0.000), lr: 0.000693433187712364\n",
      "step: 300, acc: 0.852, loss: 0.405 (data_loss: 0.405, reg_loss: 0.000), lr: 0.0006886578059362303\n",
      "step: 400, acc: 0.836, loss: 0.547 (data_loss: 0.547, reg_loss: 0.000), lr: 0.0006839477463921757\n",
      "step: 468, acc: 0.792, loss: 0.672 (data_loss: 0.672, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "training, acc: 0.817, loss: 0.517 (data_loss: 0.517, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "validation, acc: 0.838, loss: 0.443\n",
      "epoch: 1\n",
      "step: 0, acc: 0.117, loss: 2.302 (data_loss: 2.302, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.562, loss: 1.179 (data_loss: 1.179, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 200, acc: 0.586, loss: 1.001 (data_loss: 1.001, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 300, acc: 0.672, loss: 0.904 (data_loss: 0.904, reg_loss: 0.000), lr: 0.0009708737864077671\n",
      "step: 400, acc: 0.688, loss: 0.974 (data_loss: 0.974, reg_loss: 0.000), lr: 0.0009615384615384615\n",
      "step: 468, acc: 0.573, loss: 1.126 (data_loss: 1.126, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "training, acc: 0.575, loss: 1.070 (data_loss: 1.070, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "validation, acc: 0.776, loss: 0.594\n",
      "epoch: 2\n",
      "step: 0, acc: 0.656, loss: 0.874 (data_loss: 0.874, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.625, loss: 0.931 (data_loss: 0.931, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 200, acc: 0.750, loss: 0.843 (data_loss: 0.843, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 300, acc: 0.711, loss: 0.729 (data_loss: 0.729, reg_loss: 0.000), lr: 0.0009285913269570063\n",
      "step: 400, acc: 0.727, loss: 0.721 (data_loss: 0.721, reg_loss: 0.000), lr: 0.0009200478424878093\n",
      "step: 468, acc: 0.615, loss: 0.955 (data_loss: 0.955, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "training, acc: 0.669, loss: 0.850 (data_loss: 0.850, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "validation, acc: 0.798, loss: 0.550\n",
      "epoch: 3\n",
      "step: 0, acc: 0.703, loss: 0.768 (data_loss: 0.768, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.641, loss: 1.076 (data_loss: 1.076, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 200, acc: 0.781, loss: 0.701 (data_loss: 0.701, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 300, acc: 0.727, loss: 0.707 (data_loss: 0.707, reg_loss: 0.000), lr: 0.0008898380494749957\n",
      "step: 400, acc: 0.766, loss: 0.908 (data_loss: 0.908, reg_loss: 0.000), lr: 0.0008819897689186807\n",
      "step: 468, acc: 0.677, loss: 0.928 (data_loss: 0.928, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "training, acc: 0.685, loss: 0.814 (data_loss: 0.814, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "validation, acc: 0.806, loss: 0.530\n",
      "epoch: 4\n",
      "step: 0, acc: 0.711, loss: 0.791 (data_loss: 0.791, reg_loss: 0.000), lr: 0.0008766546857192952\n",
      "step: 100, acc: 0.719, loss: 0.843 (data_loss: 0.843, reg_loss: 0.000), lr: 0.0008690362388111585\n",
      "step: 200, acc: 0.727, loss: 0.774 (data_loss: 0.774, reg_loss: 0.000), lr: 0.0008615490652192642\n",
      "step: 300, acc: 0.695, loss: 0.739 (data_loss: 0.739, reg_loss: 0.000), lr: 0.0008541898009737763\n",
      "step: 400, acc: 0.781, loss: 0.706 (data_loss: 0.706, reg_loss: 0.000), lr: 0.0008469551960701278\n",
      "step: 468, acc: 0.667, loss: 0.867 (data_loss: 0.867, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "training, acc: 0.695, loss: 0.800 (data_loss: 0.800, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "validation, acc: 0.810, loss: 0.522\n",
      "epoch: 5\n",
      "step: 0, acc: 0.695, loss: 0.747 (data_loss: 0.747, reg_loss: 0.000), lr: 0.0008420343550016842\n",
      "step: 100, acc: 0.719, loss: 0.873 (data_loss: 0.873, reg_loss: 0.000), lr: 0.0008350033400133601\n",
      "step: 200, acc: 0.758, loss: 0.724 (data_loss: 0.724, reg_loss: 0.000), lr: 0.0008280887711162637\n",
      "step: 300, acc: 0.695, loss: 0.698 (data_loss: 0.698, reg_loss: 0.000), lr: 0.0008212877792378449\n",
      "step: 400, acc: 0.789, loss: 0.689 (data_loss: 0.689, reg_loss: 0.000), lr: 0.0008145975887911372\n",
      "step: 468, acc: 0.719, loss: 0.885 (data_loss: 0.885, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "training, acc: 0.701, loss: 0.786 (data_loss: 0.786, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "validation, acc: 0.816, loss: 0.532\n",
      "epoch: 6\n",
      "step: 0, acc: 0.680, loss: 0.825 (data_loss: 0.825, reg_loss: 0.000), lr: 0.0008100445524503849\n",
      "step: 100, acc: 0.734, loss: 0.816 (data_loss: 0.816, reg_loss: 0.000), lr: 0.0008035355564483729\n",
      "step: 200, acc: 0.734, loss: 0.776 (data_loss: 0.776, reg_loss: 0.000), lr: 0.0007971303308090873\n",
      "step: 300, acc: 0.711, loss: 0.702 (data_loss: 0.702, reg_loss: 0.000), lr: 0.0007908264136022144\n",
      "step: 400, acc: 0.727, loss: 0.787 (data_loss: 0.787, reg_loss: 0.000), lr: 0.0007846214201647706\n",
      "step: 468, acc: 0.719, loss: 0.779 (data_loss: 0.779, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "training, acc: 0.703, loss: 0.779 (data_loss: 0.779, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "validation, acc: 0.820, loss: 0.509\n",
      "epoch: 7\n",
      "step: 0, acc: 0.719, loss: 0.639 (data_loss: 0.639, reg_loss: 0.000), lr: 0.0007803964413922272\n",
      "step: 100, acc: 0.672, loss: 0.873 (data_loss: 0.873, reg_loss: 0.000), lr: 0.0007743534148985598\n",
      "step: 200, acc: 0.727, loss: 0.747 (data_loss: 0.747, reg_loss: 0.000), lr: 0.0007684032580298141\n",
      "step: 300, acc: 0.664, loss: 0.759 (data_loss: 0.759, reg_loss: 0.000), lr: 0.0007625438462711606\n",
      "step: 400, acc: 0.766, loss: 0.618 (data_loss: 0.618, reg_loss: 0.000), lr: 0.0007567731194187983\n",
      "step: 468, acc: 0.698, loss: 0.669 (data_loss: 0.669, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "training, acc: 0.710, loss: 0.768 (data_loss: 0.768, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "validation, acc: 0.826, loss: 0.499\n",
      "epoch: 8\n",
      "step: 0, acc: 0.727, loss: 0.654 (data_loss: 0.654, reg_loss: 0.000), lr: 0.0007528419784687194\n",
      "step: 100, acc: 0.781, loss: 0.713 (data_loss: 0.713, reg_loss: 0.000), lr: 0.0007472166180975864\n",
      "step: 200, acc: 0.711, loss: 0.739 (data_loss: 0.739, reg_loss: 0.000), lr: 0.0007416747014759327\n",
      "step: 300, acc: 0.719, loss: 0.658 (data_loss: 0.658, reg_loss: 0.000), lr: 0.0007362143856290952\n",
      "step: 400, acc: 0.781, loss: 0.629 (data_loss: 0.629, reg_loss: 0.000), lr: 0.0007308338814587444\n",
      "step: 468, acc: 0.677, loss: 0.869 (data_loss: 0.869, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "training, acc: 0.715, loss: 0.757 (data_loss: 0.757, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "validation, acc: 0.823, loss: 0.504\n",
      "epoch: 9\n",
      "step: 0, acc: 0.758, loss: 0.591 (data_loss: 0.591, reg_loss: 0.000), lr: 0.0007271669575334498\n",
      "step: 100, acc: 0.742, loss: 0.904 (data_loss: 0.904, reg_loss: 0.000), lr: 0.000721917412647993\n",
      "step: 200, acc: 0.750, loss: 0.681 (data_loss: 0.681, reg_loss: 0.000), lr: 0.0007167431192660551\n",
      "step: 300, acc: 0.711, loss: 0.687 (data_loss: 0.687, reg_loss: 0.000), lr: 0.0007116424708226587\n",
      "step: 400, acc: 0.773, loss: 0.565 (data_loss: 0.565, reg_loss: 0.000), lr: 0.0007066139061616733\n",
      "step: 468, acc: 0.646, loss: 0.840 (data_loss: 0.840, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "training, acc: 0.716, loss: 0.748 (data_loss: 0.748, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "validation, acc: 0.826, loss: 0.503\n",
      "epoch: 10\n",
      "step: 0, acc: 0.656, loss: 0.736 (data_loss: 0.736, reg_loss: 0.000), lr: 0.0007031854299978904\n",
      "step: 100, acc: 0.766, loss: 0.666 (data_loss: 0.666, reg_loss: 0.000), lr: 0.0006982752601075343\n",
      "step: 200, acc: 0.734, loss: 0.803 (data_loss: 0.803, reg_loss: 0.000), lr: 0.000693433187712364\n",
      "step: 300, acc: 0.703, loss: 0.672 (data_loss: 0.672, reg_loss: 0.000), lr: 0.0006886578059362303\n",
      "step: 400, acc: 0.758, loss: 0.749 (data_loss: 0.749, reg_loss: 0.000), lr: 0.0006839477463921757\n",
      "step: 468, acc: 0.656, loss: 0.939 (data_loss: 0.939, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "training, acc: 0.718, loss: 0.743 (data_loss: 0.743, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "validation, acc: 0.828, loss: 0.486\n"
     ]
    }
   ],
   "source": [
    "# Train models with different dropout rates\n",
    "test_dropout_rates = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "for dropout_rate in test_dropout_rates:\n",
    "   Train_Model(X=X, y=y, X_test=X_test, y_test=y_test, model_number=\"_dropout_rate\"+str(dropout_rate), dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with dropout rate 0.5:\n",
      "validation, acc: 0.859, loss: 0.396\n",
      "Model with dropout rate 0.6:\n",
      "validation, acc: 0.849, loss: 0.417\n",
      "Model with dropout rate 0.7:\n",
      "validation, acc: 0.846, loss: 0.414\n",
      "Model with dropout rate 0.8:\n",
      "validation, acc: 0.838, loss: 0.443\n",
      "Model with dropout rate 0.9:\n",
      "validation, acc: 0.828, loss: 0.486\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models\n",
    "print(\"Model with dropout rate 0.5:\")\n",
    "model = Model.load(\"fashion_mnist_dropout_rate0.5.model\")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Model with dropout rate 0.6:\")\n",
    "model = Model.load(\"fashion_mnist_dropout_rate0.6.model\")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Model with dropout rate 0.7:\")\n",
    "model = Model.load(\"fashion_mnist_dropout_rate0.7.model\")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Model with dropout rate 0.8:\")\n",
    "model = Model.load(\"fashion_mnist_dropout_rate0.8.model\")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Model with dropout rate 0.9:\")\n",
    "model = Model.load(\"fashion_mnist_dropout_rate0.9.model\")\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0, acc: 0.000, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.688, loss: 0.846 (data_loss: 0.846, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 200, acc: 0.773, loss: 0.664 (data_loss: 0.664, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 300, acc: 0.812, loss: 0.475 (data_loss: 0.475, reg_loss: 0.000), lr: 0.0009708737864077671\n",
      "step: 400, acc: 0.867, loss: 0.483 (data_loss: 0.483, reg_loss: 0.000), lr: 0.0009615384615384615\n",
      "step: 468, acc: 0.823, loss: 0.629 (data_loss: 0.629, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "training, acc: 0.715, loss: 0.754 (data_loss: 0.754, reg_loss: 0.000), lr: 0.0009552923194497518\n",
      "validation, acc: 0.808, loss: 0.528\n",
      "epoch: 2\n",
      "step: 0, acc: 0.836, loss: 0.463 (data_loss: 0.463, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.812, loss: 0.588 (data_loss: 0.588, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 200, acc: 0.852, loss: 0.441 (data_loss: 0.441, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 300, acc: 0.844, loss: 0.385 (data_loss: 0.385, reg_loss: 0.000), lr: 0.0009285913269570063\n",
      "step: 400, acc: 0.859, loss: 0.415 (data_loss: 0.415, reg_loss: 0.000), lr: 0.0009200478424878093\n",
      "step: 468, acc: 0.823, loss: 0.562 (data_loss: 0.562, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "training, acc: 0.825, loss: 0.485 (data_loss: 0.485, reg_loss: 0.000), lr: 0.0009143275121148394\n",
      "validation, acc: 0.830, loss: 0.468\n",
      "epoch: 3\n",
      "step: 0, acc: 0.844, loss: 0.403 (data_loss: 0.403, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.812, loss: 0.533 (data_loss: 0.533, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 200, acc: 0.828, loss: 0.417 (data_loss: 0.417, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 300, acc: 0.836, loss: 0.374 (data_loss: 0.374, reg_loss: 0.000), lr: 0.0008898380494749957\n",
      "step: 400, acc: 0.867, loss: 0.386 (data_loss: 0.386, reg_loss: 0.000), lr: 0.0008819897689186807\n",
      "step: 468, acc: 0.823, loss: 0.568 (data_loss: 0.568, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "training, acc: 0.844, loss: 0.432 (data_loss: 0.432, reg_loss: 0.000), lr: 0.000876731544800982\n",
      "validation, acc: 0.836, loss: 0.454\n",
      "epoch: 4\n",
      "step: 0, acc: 0.836, loss: 0.390 (data_loss: 0.390, reg_loss: 0.000), lr: 0.0008766546857192952\n",
      "step: 100, acc: 0.836, loss: 0.505 (data_loss: 0.505, reg_loss: 0.000), lr: 0.0008690362388111585\n",
      "step: 200, acc: 0.859, loss: 0.370 (data_loss: 0.370, reg_loss: 0.000), lr: 0.0008615490652192642\n",
      "step: 300, acc: 0.820, loss: 0.381 (data_loss: 0.381, reg_loss: 0.000), lr: 0.0008541898009737763\n",
      "step: 400, acc: 0.859, loss: 0.379 (data_loss: 0.379, reg_loss: 0.000), lr: 0.0008469551960701278\n",
      "step: 468, acc: 0.844, loss: 0.486 (data_loss: 0.486, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "training, acc: 0.851, loss: 0.403 (data_loss: 0.403, reg_loss: 0.000), lr: 0.0008421052631578947\n",
      "validation, acc: 0.849, loss: 0.413\n",
      "epoch: 5\n",
      "step: 0, acc: 0.852, loss: 0.350 (data_loss: 0.350, reg_loss: 0.000), lr: 0.0008420343550016842\n",
      "step: 100, acc: 0.836, loss: 0.510 (data_loss: 0.510, reg_loss: 0.000), lr: 0.0008350033400133601\n",
      "step: 200, acc: 0.875, loss: 0.359 (data_loss: 0.359, reg_loss: 0.000), lr: 0.0008280887711162637\n",
      "step: 300, acc: 0.836, loss: 0.388 (data_loss: 0.388, reg_loss: 0.000), lr: 0.0008212877792378449\n",
      "step: 400, acc: 0.883, loss: 0.368 (data_loss: 0.368, reg_loss: 0.000), lr: 0.0008145975887911372\n",
      "step: 468, acc: 0.844, loss: 0.483 (data_loss: 0.483, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "training, acc: 0.861, loss: 0.377 (data_loss: 0.377, reg_loss: 0.000), lr: 0.0008101101749837978\n",
      "validation, acc: 0.858, loss: 0.394\n",
      "epoch: 6\n",
      "step: 0, acc: 0.859, loss: 0.342 (data_loss: 0.342, reg_loss: 0.000), lr: 0.0008100445524503849\n",
      "step: 100, acc: 0.820, loss: 0.457 (data_loss: 0.457, reg_loss: 0.000), lr: 0.0008035355564483729\n",
      "step: 200, acc: 0.891, loss: 0.359 (data_loss: 0.359, reg_loss: 0.000), lr: 0.0007971303308090873\n",
      "step: 300, acc: 0.867, loss: 0.354 (data_loss: 0.354, reg_loss: 0.000), lr: 0.0007908264136022144\n",
      "step: 400, acc: 0.906, loss: 0.346 (data_loss: 0.346, reg_loss: 0.000), lr: 0.0007846214201647706\n",
      "step: 468, acc: 0.865, loss: 0.454 (data_loss: 0.454, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "training, acc: 0.869, loss: 0.354 (data_loss: 0.354, reg_loss: 0.000), lr: 0.0007804573480059316\n",
      "validation, acc: 0.860, loss: 0.383\n",
      "epoch: 7\n",
      "step: 0, acc: 0.859, loss: 0.366 (data_loss: 0.366, reg_loss: 0.000), lr: 0.0007803964413922272\n",
      "step: 100, acc: 0.812, loss: 0.465 (data_loss: 0.465, reg_loss: 0.000), lr: 0.0007743534148985598\n",
      "step: 200, acc: 0.891, loss: 0.337 (data_loss: 0.337, reg_loss: 0.000), lr: 0.0007684032580298141\n",
      "step: 300, acc: 0.867, loss: 0.318 (data_loss: 0.318, reg_loss: 0.000), lr: 0.0007625438462711606\n",
      "step: 400, acc: 0.898, loss: 0.336 (data_loss: 0.336, reg_loss: 0.000), lr: 0.0007567731194187983\n",
      "step: 468, acc: 0.854, loss: 0.413 (data_loss: 0.413, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "training, acc: 0.877, loss: 0.333 (data_loss: 0.333, reg_loss: 0.000), lr: 0.0007528986598403856\n",
      "validation, acc: 0.862, loss: 0.380\n",
      "epoch: 8\n",
      "step: 0, acc: 0.859, loss: 0.379 (data_loss: 0.379, reg_loss: 0.000), lr: 0.0007528419784687194\n",
      "step: 100, acc: 0.812, loss: 0.447 (data_loss: 0.447, reg_loss: 0.000), lr: 0.0007472166180975864\n",
      "step: 200, acc: 0.898, loss: 0.323 (data_loss: 0.323, reg_loss: 0.000), lr: 0.0007416747014759327\n",
      "step: 300, acc: 0.891, loss: 0.300 (data_loss: 0.300, reg_loss: 0.000), lr: 0.0007362143856290952\n",
      "step: 400, acc: 0.891, loss: 0.311 (data_loss: 0.311, reg_loss: 0.000), lr: 0.0007308338814587444\n",
      "step: 468, acc: 0.875, loss: 0.410 (data_loss: 0.410, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "training, acc: 0.882, loss: 0.319 (data_loss: 0.319, reg_loss: 0.000), lr: 0.0007272198385571959\n",
      "validation, acc: 0.863, loss: 0.373\n",
      "epoch: 9\n",
      "step: 0, acc: 0.852, loss: 0.335 (data_loss: 0.335, reg_loss: 0.000), lr: 0.0007271669575334498\n",
      "step: 100, acc: 0.820, loss: 0.417 (data_loss: 0.417, reg_loss: 0.000), lr: 0.000721917412647993\n",
      "step: 200, acc: 0.914, loss: 0.293 (data_loss: 0.293, reg_loss: 0.000), lr: 0.0007167431192660551\n",
      "step: 300, acc: 0.875, loss: 0.300 (data_loss: 0.300, reg_loss: 0.000), lr: 0.0007116424708226587\n",
      "step: 400, acc: 0.914, loss: 0.280 (data_loss: 0.280, reg_loss: 0.000), lr: 0.0007066139061616733\n",
      "step: 468, acc: 0.875, loss: 0.392 (data_loss: 0.392, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "training, acc: 0.887, loss: 0.304 (data_loss: 0.304, reg_loss: 0.000), lr: 0.0007032348804500702\n",
      "validation, acc: 0.869, loss: 0.362\n",
      "epoch: 10\n",
      "step: 0, acc: 0.883, loss: 0.331 (data_loss: 0.331, reg_loss: 0.000), lr: 0.0007031854299978904\n",
      "step: 100, acc: 0.859, loss: 0.375 (data_loss: 0.375, reg_loss: 0.000), lr: 0.0006982752601075343\n",
      "step: 200, acc: 0.891, loss: 0.294 (data_loss: 0.294, reg_loss: 0.000), lr: 0.000693433187712364\n",
      "step: 300, acc: 0.898, loss: 0.280 (data_loss: 0.280, reg_loss: 0.000), lr: 0.0006886578059362303\n",
      "step: 400, acc: 0.914, loss: 0.251 (data_loss: 0.251, reg_loss: 0.000), lr: 0.0006839477463921757\n",
      "step: 468, acc: 0.885, loss: 0.376 (data_loss: 0.376, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "training, acc: 0.891, loss: 0.293 (data_loss: 0.293, reg_loss: 0.000), lr: 0.000680781537204711\n",
      "validation, acc: 0.869, loss: 0.364\n",
      "epoch: 11\n",
      "step: 0, acc: 0.867, loss: 0.332 (data_loss: 0.332, reg_loss: 0.000), lr: 0.0006807351940095304\n",
      "step: 100, acc: 0.844, loss: 0.392 (data_loss: 0.392, reg_loss: 0.000), lr: 0.000676132521974307\n",
      "step: 200, acc: 0.898, loss: 0.283 (data_loss: 0.283, reg_loss: 0.000), lr: 0.0006715916722632639\n",
      "step: 300, acc: 0.891, loss: 0.291 (data_loss: 0.291, reg_loss: 0.000), lr: 0.0006671114076050701\n",
      "step: 400, acc: 0.906, loss: 0.289 (data_loss: 0.289, reg_loss: 0.000), lr: 0.0006626905235255136\n",
      "step: 468, acc: 0.875, loss: 0.348 (data_loss: 0.348, reg_loss: 0.000), lr: 0.0006597176408497163\n",
      "training, acc: 0.894, loss: 0.283 (data_loss: 0.283, reg_loss: 0.000), lr: 0.0006597176408497163\n",
      "validation, acc: 0.871, loss: 0.364\n",
      "epoch: 12\n",
      "step: 0, acc: 0.867, loss: 0.318 (data_loss: 0.318, reg_loss: 0.000), lr: 0.0006596741209842338\n",
      "step: 100, acc: 0.836, loss: 0.367 (data_loss: 0.367, reg_loss: 0.000), lr: 0.0006553509404285995\n",
      "step: 200, acc: 0.914, loss: 0.274 (data_loss: 0.274, reg_loss: 0.000), lr: 0.0006510840549514942\n",
      "step: 300, acc: 0.898, loss: 0.253 (data_loss: 0.253, reg_loss: 0.000), lr: 0.0006468723720809884\n",
      "step: 400, acc: 0.906, loss: 0.277 (data_loss: 0.277, reg_loss: 0.000), lr: 0.0006427148274310688\n",
      "step: 468, acc: 0.875, loss: 0.326 (data_loss: 0.326, reg_loss: 0.000), lr: 0.0006399180904844181\n",
      "training, acc: 0.898, loss: 0.273 (data_loss: 0.273, reg_loss: 0.000), lr: 0.0006399180904844181\n",
      "validation, acc: 0.871, loss: 0.364\n",
      "epoch: 13\n",
      "step: 0, acc: 0.867, loss: 0.323 (data_loss: 0.323, reg_loss: 0.000), lr: 0.0006398771435884309\n",
      "step: 100, acc: 0.859, loss: 0.384 (data_loss: 0.384, reg_loss: 0.000), lr: 0.0006358087487283825\n",
      "step: 200, acc: 0.914, loss: 0.255 (data_loss: 0.255, reg_loss: 0.000), lr: 0.0006317917614354309\n",
      "step: 300, acc: 0.898, loss: 0.274 (data_loss: 0.274, reg_loss: 0.000), lr: 0.0006278252134605726\n",
      "step: 400, acc: 0.914, loss: 0.253 (data_loss: 0.253, reg_loss: 0.000), lr: 0.0006239081607187422\n",
      "step: 468, acc: 0.885, loss: 0.300 (data_loss: 0.300, reg_loss: 0.000), lr: 0.000621272365805169\n",
      "training, acc: 0.900, loss: 0.263 (data_loss: 0.263, reg_loss: 0.000), lr: 0.000621272365805169\n",
      "validation, acc: 0.875, loss: 0.357\n",
      "epoch: 14\n",
      "step: 0, acc: 0.867, loss: 0.321 (data_loss: 0.321, reg_loss: 0.000), lr: 0.0006212337702677517\n",
      "step: 100, acc: 0.875, loss: 0.365 (data_loss: 0.365, reg_loss: 0.000), lr: 0.0006173982836327715\n",
      "step: 200, acc: 0.922, loss: 0.254 (data_loss: 0.254, reg_loss: 0.000), lr: 0.0006136098668466589\n",
      "step: 300, acc: 0.906, loss: 0.267 (data_loss: 0.267, reg_loss: 0.000), lr: 0.0006098676587180582\n",
      "step: 400, acc: 0.914, loss: 0.228 (data_loss: 0.228, reg_loss: 0.000), lr: 0.0006061708189367763\n",
      "step: 468, acc: 0.927, loss: 0.268 (data_loss: 0.268, reg_loss: 0.000), lr: 0.0006036824630244491\n",
      "training, acc: 0.905, loss: 0.252 (data_loss: 0.252, reg_loss: 0.000), lr: 0.0006036824630244491\n",
      "validation, acc: 0.873, loss: 0.353\n",
      "epoch: 15\n",
      "step: 0, acc: 0.906, loss: 0.263 (data_loss: 0.263, reg_loss: 0.000), lr: 0.0006036460219727152\n",
      "step: 100, acc: 0.859, loss: 0.370 (data_loss: 0.370, reg_loss: 0.000), lr: 0.0006000240009600384\n",
      "step: 200, acc: 0.898, loss: 0.292 (data_loss: 0.292, reg_loss: 0.000), lr: 0.0005964451866873433\n",
      "step: 300, acc: 0.898, loss: 0.265 (data_loss: 0.265, reg_loss: 0.000), lr: 0.000592908810624926\n",
      "step: 400, acc: 0.930, loss: 0.221 (data_loss: 0.221, reg_loss: 0.000), lr: 0.0005894141223623718\n",
      "step: 468, acc: 0.906, loss: 0.242 (data_loss: 0.242, reg_loss: 0.000), lr: 0.0005870611717740989\n",
      "training, acc: 0.907, loss: 0.242 (data_loss: 0.242, reg_loss: 0.000), lr: 0.0005870611717740989\n",
      "validation, acc: 0.872, loss: 0.363\n",
      "epoch: 16\n",
      "step: 0, acc: 0.859, loss: 0.287 (data_loss: 0.287, reg_loss: 0.000), lr: 0.000587026709715292\n",
      "step: 100, acc: 0.844, loss: 0.336 (data_loss: 0.336, reg_loss: 0.000), lr: 0.0005836008170411439\n",
      "step: 200, acc: 0.898, loss: 0.281 (data_loss: 0.281, reg_loss: 0.000), lr: 0.0005802146794313896\n",
      "step: 300, acc: 0.898, loss: 0.291 (data_loss: 0.291, reg_loss: 0.000), lr: 0.0005768676088837612\n",
      "step: 400, acc: 0.914, loss: 0.210 (data_loss: 0.210, reg_loss: 0.000), lr: 0.0005735589331803844\n",
      "step: 468, acc: 0.917, loss: 0.288 (data_loss: 0.288, reg_loss: 0.000), lr: 0.0005713306290350225\n",
      "training, acc: 0.910, loss: 0.237 (data_loss: 0.237, reg_loss: 0.000), lr: 0.0005713306290350225\n",
      "validation, acc: 0.877, loss: 0.355\n",
      "epoch: 17\n",
      "step: 0, acc: 0.891, loss: 0.271 (data_loss: 0.271, reg_loss: 0.000), lr: 0.0005712979890310786\n",
      "step: 100, acc: 0.875, loss: 0.310 (data_loss: 0.310, reg_loss: 0.000), lr: 0.000568052715291979\n",
      "step: 200, acc: 0.914, loss: 0.249 (data_loss: 0.249, reg_loss: 0.000), lr: 0.0005648441030275643\n",
      "step: 300, acc: 0.898, loss: 0.251 (data_loss: 0.251, reg_loss: 0.000), lr: 0.0005616715344866322\n",
      "step: 400, acc: 0.906, loss: 0.254 (data_loss: 0.254, reg_loss: 0.000), lr: 0.0005585344057193923\n",
      "step: 468, acc: 0.927, loss: 0.245 (data_loss: 0.245, reg_loss: 0.000), lr: 0.0005564210994880927\n",
      "training, acc: 0.913, loss: 0.226 (data_loss: 0.226, reg_loss: 0.000), lr: 0.0005564210994880927\n",
      "validation, acc: 0.875, loss: 0.362\n",
      "epoch: 18\n",
      "step: 0, acc: 0.875, loss: 0.279 (data_loss: 0.279, reg_loss: 0.000), lr: 0.0005563901407667057\n",
      "step: 100, acc: 0.875, loss: 0.322 (data_loss: 0.322, reg_loss: 0.000), lr: 0.0005533115697449233\n",
      "step: 200, acc: 0.914, loss: 0.242 (data_loss: 0.242, reg_loss: 0.000), lr: 0.0005502668794365267\n",
      "step: 300, acc: 0.883, loss: 0.252 (data_loss: 0.252, reg_loss: 0.000), lr: 0.0005472555135992994\n",
      "step: 400, acc: 0.922, loss: 0.207 (data_loss: 0.207, reg_loss: 0.000), lr: 0.0005442769281010178\n",
      "step: 468, acc: 0.927, loss: 0.230 (data_loss: 0.230, reg_loss: 0.000), lr: 0.0005422699419771162\n",
      "training, acc: 0.917, loss: 0.217 (data_loss: 0.217, reg_loss: 0.000), lr: 0.0005422699419771162\n",
      "validation, acc: 0.878, loss: 0.365\n",
      "epoch: 19\n",
      "step: 0, acc: 0.883, loss: 0.238 (data_loss: 0.238, reg_loss: 0.000), lr: 0.0005422405379026137\n",
      "step: 100, acc: 0.875, loss: 0.287 (data_loss: 0.287, reg_loss: 0.000), lr: 0.0005393161471254449\n",
      "step: 200, acc: 0.906, loss: 0.241 (data_loss: 0.241, reg_loss: 0.000), lr: 0.0005364231305653899\n",
      "step: 300, acc: 0.891, loss: 0.269 (data_loss: 0.269, reg_loss: 0.000), lr: 0.0005335609860207021\n",
      "step: 400, acc: 0.922, loss: 0.231 (data_loss: 0.231, reg_loss: 0.000), lr: 0.0005307292219509606\n",
      "step: 468, acc: 0.917, loss: 0.222 (data_loss: 0.222, reg_loss: 0.000), lr: 0.0005288207297726071\n",
      "training, acc: 0.919, loss: 0.210 (data_loss: 0.210, reg_loss: 0.000), lr: 0.0005288207297726071\n",
      "validation, acc: 0.874, loss: 0.371\n",
      "epoch: 20\n",
      "step: 0, acc: 0.875, loss: 0.239 (data_loss: 0.239, reg_loss: 0.000), lr: 0.0005287927661149595\n",
      "step: 100, acc: 0.875, loss: 0.286 (data_loss: 0.286, reg_loss: 0.000), lr: 0.0005260112566408921\n",
      "step: 200, acc: 0.906, loss: 0.248 (data_loss: 0.248, reg_loss: 0.000), lr: 0.0005232588561561404\n",
      "step: 300, acc: 0.906, loss: 0.244 (data_loss: 0.244, reg_loss: 0.000), lr: 0.0005205351100931758\n",
      "step: 400, acc: 0.914, loss: 0.221 (data_loss: 0.221, reg_loss: 0.000), lr: 0.0005178395733001916\n",
      "step: 468, acc: 0.927, loss: 0.235 (data_loss: 0.235, reg_loss: 0.000), lr: 0.0005160224985809382\n",
      "training, acc: 0.922, loss: 0.204 (data_loss: 0.204, reg_loss: 0.000), lr: 0.0005160224985809382\n",
      "validation, acc: 0.872, loss: 0.385\n"
     ]
    }
   ],
   "source": [
    "# Make the best fitted model\n",
    "Train_Model(X=X, y=y, X_test=X_test, y_test=y_test, model_number=\"_best\", epochs=20, number_of_layers=3, number_of_neurons=256, batch_size=128, learning_rate=0.001, dropout_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best fitted model:\n",
      "validation, acc: 0.872, loss: 0.385\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best fitted model\n",
    "print(\"Best fitted model:\")\n",
    "model = Model.load(\"fashion_mnist_best.model\")  \n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: T-shirt/top, Prediction: Shirt --> Wrong\n",
      "Ground truth: Trouser, Prediction: Trouser --> Correct\n",
      "Ground truth: Pullover, Prediction: Coat --> Wrong\n",
      "Ground truth: Dress, Prediction: Dress --> Correct\n",
      "Ground truth: Coat, Prediction: Coat --> Correct\n",
      "Ground truth: Sandal, Prediction: Sandal --> Correct\n",
      "Ground truth: Shirt, Prediction: Shirt --> Correct\n",
      "Ground truth: Sneaker, Prediction: Sneaker --> Correct\n",
      "Ground truth: Bag, Prediction: Bag --> Correct\n",
      "Ground truth: Ankle boot, Prediction: Ankle boot --> Correct\n"
     ]
    }
   ],
   "source": [
    "# Check the best fitted model with some test images\n",
    "model = Model.load(\"fashion_mnist_best.model\")\n",
    "\n",
    "# Label index to label name relation\n",
    "fashion_mnist_labels = {\n",
    "    0: \"T-shirt/top\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle boot\",\n",
    "}\n",
    "\n",
    "paths = [\n",
    "    \"fashion_mnist_images/test/0/0100.png\",\n",
    "    \"fashion_mnist_images/test/1/0100.png\",\n",
    "    \"fashion_mnist_images/test/2/0100.png\",\n",
    "    \"fashion_mnist_images/test/3/0100.png\",\n",
    "    \"fashion_mnist_images/test/4/0100.png\",\n",
    "    \"fashion_mnist_images/test/5/0100.png\",\n",
    "    \"fashion_mnist_images/test/6/0100.png\",\n",
    "    \"fashion_mnist_images/test/7/0100.png\",\n",
    "    \"fashion_mnist_images/test/8/0100.png\",\n",
    "    \"fashion_mnist_images/test/9/0100.png\",\n",
    "]\n",
    "\n",
    "label = 0\n",
    "score = 0\n",
    "for path in paths:\n",
    "    image_data = cv2.imread(\"../\" + path, cv2.IMREAD_GRAYSCALE)\n",
    "    image_data = cv2.resize(image_data, (28, 28))\n",
    "    image_data = (image_data.reshape(1, -1).astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "    model = Model.load(\"fashion_mnist_best.model\")\n",
    "    confidences = model.predict(image_data)\n",
    "    predictions = model.output_layer_activation.predictions(confidences)\n",
    "    prediction = fashion_mnist_labels[predictions[0]]\n",
    "    \n",
    "    if prediction == fashion_mnist_labels[label]:\n",
    "        print(\"Ground truth: \" + fashion_mnist_labels[label] + \", Prediction: \" + prediction + \" --> Correct\")\n",
    "        score += 1\n",
    "    else:\n",
    "        print(\"Ground truth: \" + fashion_mnist_labels[label] + \", Prediction: \" + prediction + \" --> Wrong\")\n",
    "    label += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag\n",
      "T-shirt/top\n",
      "Dress\n"
     ]
    }
   ],
   "source": [
    "# Test best model with user made images\n",
    "\n",
    "paths = [\n",
    "   \"bag.png\",\n",
    "   \"pants.png\",\n",
    "   \"pullover.png\",\n",
    "   \"sneakers.png\",\n",
    "   \"tshirt.png\"\n",
    "]\n",
    "\n",
    "image_data = cv2.imread(\"../predictions/bag.png\", cv2.IMREAD_GRAYSCALE)\n",
    "image_data = cv2.resize(image_data, (28, 28))\n",
    "image_data = 255 - image_data  # this line is needed only for predictions folder\n",
    "image_data = (image_data.reshape(1, -1).astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "model = Model.load(\"fashion_mnist_best.model\")\n",
    "confidences = model.predict(image_data)\n",
    "predictions = model.output_layer_activation.predictions(confidences)\n",
    "prediction = fashion_mnist_labels[predictions[0]]\n",
    "print(prediction)\n",
    "\n",
    "image_data = cv2.imread(\"../predictions/tshirt.png\", cv2.IMREAD_GRAYSCALE)\n",
    "image_data = cv2.resize(image_data, (28, 28))\n",
    "image_data = 255 - image_data  # this line is needed only for predictions folder\n",
    "image_data = (image_data.reshape(1, -1).astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "model = Model.load(\"fashion_mnist_best.model\")\n",
    "confidences = model.predict(image_data)\n",
    "predictions = model.output_layer_activation.predictions(confidences)\n",
    "prediction = fashion_mnist_labels[predictions[0]]\n",
    "print(prediction)\n",
    "\n",
    "image_data = cv2.imread(\"../predictions/pants.png\", cv2.IMREAD_GRAYSCALE)\n",
    "image_data = cv2.resize(image_data, (28, 28))\n",
    "image_data = 255 - image_data  # this line is needed only for predictions folder\n",
    "image_data = (image_data.reshape(1, -1).astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "model = Model.load(\"fashion_mnist_best.model\")\n",
    "confidences = model.predict(image_data)\n",
    "predictions = model.output_layer_activation.predictions(confidences)\n",
    "prediction = fashion_mnist_labels[predictions[0]]\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
